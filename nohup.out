logfile: /home/leeshu/wmm/neuraloperator-main/logs/darcy/20260218_075631__mHC2_L8_421.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'arch': 'fno', 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_small2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [32, 32], 'hidden_channels': 32, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 2, 'n_layers': 8, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 2, 'hc_dynamic': True}, 'opt': {'_config_name': 'darcyoptconfig', 'n_epochs': 300, 'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.005, 'weight_decay': 0.0001, 'hc_weight_decay': 0.0, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 60, 'gamma': 0.5}, 'data': {'_config_name': 'darcydatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data/Darcy_pt', 'batch_size': 8, 'n_train': 1000, 'train_resolution': 421, 'n_tests': [224], 'test_resolutions': [421], 'test_batch_sizes': [8], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'darcy32_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 421 with 224 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-7): 8 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([32, 32, 32, 17]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-7): 8 x Flattened1dConv(
        (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-7): 8 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))
          (1): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-7): 8 x SoftGating()
    )
  )
  (hc_layers): ModuleList(
    (0-7): 8 x HyperConnection(
      (layer_norm): GroupNorm(1, 32, eps=1e-05, affine=True)
      (dynamic_beta_fn): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
      (1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
      (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0001

Parameter Group 1
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7fd76c9c6fd0>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fd76c982070>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fd76c982070>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fd76c9c6f70>}

### Beginning Training...


n_params: 8935737
Training on 800 samples
Testing on [224] samples         on resolutions [421].
/home/leeshu/anaconda3/envs/FNO/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([8, 1, 421, 421])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[0] time=51.52, avg_loss=0.5424, train_err=4.3393
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
Eval: 421_h1=0.2134, 421_l2=0.1040
[1] time=45.38, avg_loss=0.1696, train_err=1.3567
Eval: 421_h1=0.1319, 421_l2=0.0793
[2] time=45.38, avg_loss=0.1293, train_err=1.0342
Eval: 421_h1=0.1190, 421_l2=0.0728
[3] time=45.38, avg_loss=0.1132, train_err=0.9057
Eval: 421_h1=0.1071, 421_l2=0.0653
logfile: /home/leeshu/wmm/neuraloperator-main/logs/darcy/20260218_080053__mHCFNO421.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'arch': 'fno', 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_small2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [32, 32], 'hidden_channels': 32, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 2, 'n_layers': 8, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 0, 'hc_dynamic': True}, 'opt': {'_config_name': 'darcyoptconfig', 'n_epochs': 300, 'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.005, 'weight_decay': 0.0, 'hc_weight_decay': 0.0, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 60, 'gamma': 0.5}, 'data': {'_config_name': 'darcydatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data/Darcy_pt', 'batch_size': 8, 'n_train': 1000, 'train_resolution': 421, 'n_tests': [224], 'test_resolutions': [421], 'test_batch_sizes': [8], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'darcy32_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 421 with 224 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-7): 8 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([32, 32, 32, 17]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-7): 8 x Flattened1dConv(
        (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-7): 8 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))
          (1): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-7): 8 x SoftGating()
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
      (1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
      (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7fc794f08af0>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fc794f08b50>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fc794f08b50>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fc794f08b20>}

### Beginning Training...


n_params: 8934689
Training on 800 samples
Testing on [224] samples         on resolutions [421].
/home/leeshu/anaconda3/envs/FNO/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([8, 1, 421, 421])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[4] time=60.07, avg_loss=0.1027, train_err=0.8214
Eval: 421_h1=0.0959, 421_l2=0.0598
[0] time=84.49, avg_loss=0.3934, train_err=3.1476
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
Eval: 421_h1=0.1744, 421_l2=0.0857
[5] time=104.69, avg_loss=0.0924, train_err=0.7392
Eval: 421_h1=0.0895, 421_l2=0.0545
[1] time=80.89, avg_loss=0.1540, train_err=1.2319
Eval: 421_h1=0.1328, 421_l2=0.0744
