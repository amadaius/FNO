logfile: /home/leeshu/wmm/neuraloperator-main/logs/darcy/20260128_071320__layer10FNO.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'arch': 'fno', 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_small2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [16, 16], 'hidden_channels': 24, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 2, 'n_layers': 10, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 0, 'hc_dynamic': False}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.005, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 60, 'gamma': 0.5, 'n_epochs': 300}, 'data': {'_config_name': 'darcydatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data', 'batch_size': 8, 'n_train': 1000, 'train_resolution': 32, 'n_tests': [200], 'test_resolutions': [32], 'test_batch_sizes': [8], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'darcy32_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 32 with 200 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-9): 10 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([24, 24, 16, 9]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-9): 10 x Flattened1dConv(
        (conv): Conv1d(24, 24, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-9): 10 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(24, 12, kernel_size=(1,), stride=(1,))
          (1): Conv1d(12, 24, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-9): 10 x SoftGating()
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 48, kernel_size=(1,), stride=(1,))
      (1): Conv1d(48, 24, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(24, 48, kernel_size=(1,), stride=(1,))
      (1): Conv1d(48, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7ff584ef6880>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7ff584c690a0>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7ff584c690a0>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7ff584cbefa0>}

### Beginning Training...


n_params: 1673857
Training on 1000 samples
Testing on [200] samples         on resolutions [32].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([8, 1, 32, 32])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[0] time=7.39, avg_loss=0.4554, train_err=3.6431
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
Eval: 32_h1=0.2780, 32_l2=0.1838
[1] time=6.05, avg_loss=0.2264, train_err=1.8114
Eval: 32_h1=0.2088, 32_l2=0.1499
[2] time=5.73, avg_loss=0.1847, train_err=1.4775
Eval: 32_h1=0.1733, 32_l2=0.1253
[3] time=6.13, avg_loss=0.1581, train_err=1.2650
Eval: 32_h1=0.1578, 32_l2=0.1134
[4] time=6.32, avg_loss=0.1430, train_err=1.1442
Eval: 32_h1=0.1515, 32_l2=0.1066
[5] time=5.68, avg_loss=0.1350, train_err=1.0803
Eval: 32_h1=0.1556, 32_l2=0.1073
[6] time=5.30, avg_loss=0.1297, train_err=1.0374
Eval: 32_h1=0.1466, 32_l2=0.0988
[7] time=5.47, avg_loss=0.1137, train_err=0.9094
Eval: 32_h1=0.1424, 32_l2=0.0881
[8] time=4.97, avg_loss=0.1182, train_err=0.9458
Eval: 32_h1=0.1387, 32_l2=0.0861
logfile: /home/leeshu/wmm/neuraloperator-main/logs/darcy/20260128_071426__L10_HC2_MLP.log
[9] time=6.07, avg_loss=0.1066, train_err=0.8525
Eval: 32_h1=0.1395, 32_l2=0.0964
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'arch': 'fno', 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_small2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [16, 16], 'hidden_channels': 24, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 2, 'n_layers': 10, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 2, 'hc_dynamic': True}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.005, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 60, 'gamma': 0.5, 'n_epochs': 300}, 'data': {'_config_name': 'darcydatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data', 'batch_size': 8, 'n_train': 1000, 'train_resolution': 32, 'n_tests': [200], 'test_resolutions': [32], 'test_batch_sizes': [8], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'darcy32_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 32 with 200 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-9): 10 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([24, 24, 16, 9]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-9): 10 x Flattened1dConv(
        (conv): Conv1d(24, 24, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-9): 10 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(24, 12, kernel_size=(1,), stride=(1,))
          (1): Conv1d(12, 24, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-9): 10 x SoftGating()
    )
  )
  (hc_layers): ModuleList(
    (0-9): 10 x HyperConnection(
      (layer_norm): GroupNorm(1, 24, eps=1e-05, affine=True)
      (dynamic_alpha_fn): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (dynamic_beta_fn): Conv2d(24, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 48, kernel_size=(1,), stride=(1,))
      (1): Conv1d(48, 24, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(24, 48, kernel_size=(1,), stride=(1,))
      (1): Conv1d(48, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7fe7e0e7d220>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fe7e0e7d7c0>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fe7e0e7d7c0>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fe7e0e7d7f0>}

### Beginning Training...


n_params: 1676357
Training on 1000 samples
Testing on [200] samples         on resolutions [32].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([8, 1, 32, 32])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[10] time=6.18, avg_loss=0.1036, train_err=0.8286
Eval: 32_h1=0.1373, 32_l2=0.0793
[0] time=8.35, avg_loss=0.4350, train_err=3.4801
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
Eval: 32_h1=0.2870, 32_l2=0.1852
[11] time=5.95, avg_loss=0.0981, train_err=0.7849
Eval: 32_h1=0.1461, 32_l2=0.0939
[1] time=6.95, avg_loss=0.2506, train_err=2.0045
Eval: 32_h1=0.2648, 32_l2=0.1810
[12] time=5.22, avg_loss=0.0892, train_err=0.7140
Eval: 32_h1=0.1315, 32_l2=0.0741
[13] time=5.88, avg_loss=0.0854, train_err=0.6829
[2] time=6.84, avg_loss=0.2091, train_err=1.6724
Eval: 32_h1=0.1332, 32_l2=0.0778
Eval: 32_h1=0.2004, 32_l2=0.1403
[14] time=5.77, avg_loss=0.0812, train_err=0.6500
Eval: 32_h1=0.1354, 32_l2=0.0715
[3] time=7.45, avg_loss=0.1795, train_err=1.4358
Eval: 32_h1=0.1823, 32_l2=0.1164
[15] time=5.97, avg_loss=0.0802, train_err=0.6414
Eval: 32_h1=0.1418, 32_l2=0.0825
[4] time=8.72, avg_loss=0.1548, train_err=1.2388
Eval: 32_h1=0.1654, 32_l2=0.1077
[16] time=5.60, avg_loss=0.0787, train_err=0.6295
Eval: 32_h1=0.1305, 32_l2=0.0678
[17] time=5.57, avg_loss=0.0742, train_err=0.5933
[5] time=7.24, avg_loss=0.1445, train_err=1.1561
Eval: 32_h1=0.1326, 32_l2=0.0708
Eval: 32_h1=0.1647, 32_l2=0.1007
[18] time=5.63, avg_loss=0.0710, train_err=0.5680
Eval: 32_h1=0.1297, 32_l2=0.0648
[6] time=6.95, avg_loss=0.1372, train_err=1.0979
Eval: 32_h1=0.1573, 32_l2=0.0941
[19] time=6.17, avg_loss=0.0708, train_err=0.5665
Eval: 32_h1=0.1291, 32_l2=0.0646
[7] time=7.28, avg_loss=0.1207, train_err=0.9654
Eval: 32_h1=0.1519, 32_l2=0.0862
[20] time=5.94, avg_loss=0.0670, train_err=0.5361
Eval: 32_h1=0.1298, 32_l2=0.0700
[8] time=7.56, avg_loss=0.1089, train_err=0.8708
Eval: 32_h1=0.1529, 32_l2=0.0871
[21] time=4.98, avg_loss=0.0681, train_err=0.5447
Eval: 32_h1=0.1317, 32_l2=0.0662
[22] time=5.61, avg_loss=0.0625, train_err=0.5002
Eval: 32_h1=0.1279, 32_l2=0.0630
[9] time=7.75, avg_loss=0.1072, train_err=0.8575
Eval: 32_h1=0.1484, 32_l2=0.0832
[23] time=6.23, avg_loss=0.0605, train_err=0.4837
Eval: 32_h1=0.1266, 32_l2=0.0584
[10] time=7.23, avg_loss=0.1048, train_err=0.8382
Eval: 32_h1=0.1511, 32_l2=0.0816
[24] time=5.89, avg_loss=0.0602, train_err=0.4819
Eval: 32_h1=0.1296, 32_l2=0.0637
[11] time=7.78, avg_loss=0.0927, train_err=0.7417
Eval: 32_h1=0.1436, 32_l2=0.0767
[25] time=6.21, avg_loss=0.0638, train_err=0.5102
Eval: 32_h1=0.1289, 32_l2=0.0616
[12] time=7.22, avg_loss=0.0906, train_err=0.7248
Eval: 32_h1=0.1460, 32_l2=0.0766
[26] time=5.45, avg_loss=0.0625, train_err=0.4999
Eval: 32_h1=0.1305, 32_l2=0.0712
[27] time=5.50, avg_loss=0.0610, train_err=0.4884
Eval: 32_h1=0.1283, 32_l2=0.0619
[13] time=7.35, avg_loss=0.0864, train_err=0.6914
Eval: 32_h1=0.1520, 32_l2=0.0806
[28] time=5.61, avg_loss=0.0585, train_err=0.4681
Eval: 32_h1=0.1254, 32_l2=0.0588
[14] time=7.05, avg_loss=0.0840, train_err=0.6724
Eval: 32_h1=0.1404, 32_l2=0.0725
[29] time=5.73, avg_loss=0.0564, train_err=0.4509
Eval: 32_h1=0.1287, 32_l2=0.0638
[15] time=7.28, avg_loss=0.0801, train_err=0.6411
Eval: 32_h1=0.1405, 32_l2=0.0762
[30] time=5.59, avg_loss=0.0560, train_err=0.4483
Eval: 32_h1=0.1283, 32_l2=0.0609
[31] time=6.42, avg_loss=0.0578, train_err=0.4622
[16] time=7.83, avg_loss=0.0776, train_err=0.6212
Eval: 32_h1=0.1275, 32_l2=0.0600
Eval: 32_h1=0.1427, 32_l2=0.0701
[32] time=6.51, avg_loss=0.0517, train_err=0.4140
Eval: 32_h1=0.1261, 32_l2=0.0594
[17] time=7.90, avg_loss=0.0757, train_err=0.6052
Eval: 32_h1=0.1393, 32_l2=0.0707
[33] time=6.32, avg_loss=0.0523, train_err=0.4185
Eval: 32_h1=0.1261, 32_l2=0.0580
[18] time=7.82, avg_loss=0.0757, train_err=0.6057
Eval: 32_h1=0.1375, 32_l2=0.0690
[34] time=5.88, avg_loss=0.0517, train_err=0.4134
Eval: 32_h1=0.1269, 32_l2=0.0589
[19] time=7.08, avg_loss=0.0718, train_err=0.5746
Eval: 32_h1=0.1384, 32_l2=0.0740
[35] time=5.96, avg_loss=0.0510, train_err=0.4080
Eval: 32_h1=0.1264, 32_l2=0.0589
[20] time=6.72, avg_loss=0.0714, train_err=0.5710
Eval: 32_h1=0.1407, 32_l2=0.0743
[36] time=5.80, avg_loss=0.0503, train_err=0.4023
Eval: 32_h1=0.1266, 32_l2=0.0632
[21] time=7.00, avg_loss=0.0757, train_err=0.6053
[37] time=6.38, avg_loss=0.0492, train_err=0.3937
Eval: 32_h1=0.1411, 32_l2=0.0700
Eval: 32_h1=0.1272, 32_l2=0.0583
[38] time=6.39, avg_loss=0.0489, train_err=0.3909
Eval: 32_h1=0.1270, 32_l2=0.0586
[22] time=7.96, avg_loss=0.0658, train_err=0.5267
Eval: 32_h1=0.1359, 32_l2=0.0678
[39] time=6.07, avg_loss=0.0516, train_err=0.4130
Eval: 32_h1=0.1264, 32_l2=0.0616
[23] time=7.14, avg_loss=0.0607, train_err=0.4854
Eval: 32_h1=0.1332, 32_l2=0.0640
[40] time=6.27, avg_loss=0.0516, train_err=0.4127
Eval: 32_h1=0.1261, 32_l2=0.0584
[24] time=7.93, avg_loss=0.0657, train_err=0.5257
Eval: 32_h1=0.1420, 32_l2=0.0740
[41] time=7.61, avg_loss=0.0481, train_err=0.3844
Eval: 32_h1=0.1256, 32_l2=0.0572
[25] time=9.13, avg_loss=0.0661, train_err=0.5289
Eval: 32_h1=0.1361, 32_l2=0.0673
[42] time=7.38, avg_loss=0.0487, train_err=0.3894
Eval: 32_h1=0.1249, 32_l2=0.0577
[43] time=7.78, avg_loss=0.0466, train_err=0.3732
Eval: 32_h1=0.1266, 32_l2=0.0595
[26] time=10.07, avg_loss=0.0630, train_err=0.5037
Eval: 32_h1=0.1385, 32_l2=0.0695
[44] time=7.56, avg_loss=0.0465, train_err=0.3717
Eval: 32_h1=0.1245, 32_l2=0.0563
[27] time=10.59, avg_loss=0.0619, train_err=0.4951
Eval: 32_h1=0.1368, 32_l2=0.0691
[45] time=7.11, avg_loss=0.0478, train_err=0.3827
Eval: 32_h1=0.1261, 32_l2=0.0611
[28] time=10.48, avg_loss=0.0603, train_err=0.4823
Eval: 32_h1=0.1407, 32_l2=0.0720
[46] time=7.90, avg_loss=0.0465, train_err=0.3717
Eval: 32_h1=0.1257, 32_l2=0.0582
[47] time=7.63, avg_loss=0.0461, train_err=0.3692
Eval: 32_h1=0.1257, 32_l2=0.0684
[29] time=9.20, avg_loss=0.0592, train_err=0.4738
Eval: 32_h1=0.1325, 32_l2=0.0650
[48] time=7.72, avg_loss=0.0435, train_err=0.3483
Eval: 32_h1=0.1247, 32_l2=0.0565
[30] time=10.92, avg_loss=0.0571, train_err=0.4570
Eval: 32_h1=0.1331, 32_l2=0.0645
[49] time=7.70, avg_loss=0.0437, train_err=0.3495
Eval: 32_h1=0.1267, 32_l2=0.0626
[31] time=9.24, avg_loss=0.0564, train_err=0.4514
Eval: 32_h1=0.1316, 32_l2=0.0627
[50] time=7.58, avg_loss=0.0451, train_err=0.3604
Eval: 32_h1=0.1252, 32_l2=0.0561
[32] time=9.72, avg_loss=0.0603, train_err=0.4826
[51] time=7.68, avg_loss=0.0463, train_err=0.3705
Eval: 32_h1=0.1258, 32_l2=0.0575
Eval: 32_h1=0.1330, 32_l2=0.0642
[52] time=7.11, avg_loss=0.0442, train_err=0.3533
Eval: 32_h1=0.1251, 32_l2=0.0575
[33] time=10.50, avg_loss=0.0537, train_err=0.4292
Eval: 32_h1=0.1325, 32_l2=0.0638
[53] time=7.98, avg_loss=0.0421, train_err=0.3370
Eval: 32_h1=0.1256, 32_l2=0.0594
[34] time=9.71, avg_loss=0.0525, train_err=0.4203
Eval: 32_h1=0.1297, 32_l2=0.0614
[54] time=7.36, avg_loss=0.0455, train_err=0.3639
Eval: 32_h1=0.1248, 32_l2=0.0577
[55] time=7.12, avg_loss=0.0449, train_err=0.3595
Eval: 32_h1=0.1267, 32_l2=0.0601
[35] time=10.29, avg_loss=0.0535, train_err=0.4276
Eval: 32_h1=0.1310, 32_l2=0.0617
[56] time=7.78, avg_loss=0.0437, train_err=0.3496
Eval: 32_h1=0.1248, 32_l2=0.0583
[36] time=10.53, avg_loss=0.0531, train_err=0.4248
Eval: 32_h1=0.1306, 32_l2=0.0623
[57] time=8.19, avg_loss=0.0419, train_err=0.3350
Eval: 32_h1=0.1256, 32_l2=0.0589
[37] time=10.26, avg_loss=0.0522, train_err=0.4174
Eval: 32_h1=0.1327, 32_l2=0.0642
[58] time=7.32, avg_loss=0.0417, train_err=0.3332
Eval: 32_h1=0.1254, 32_l2=0.0626
[59] time=7.22, avg_loss=0.0451, train_err=0.3607
Eval: 32_h1=0.1304, 32_l2=0.0649
[38] time=9.32, avg_loss=0.0565, train_err=0.4521
Eval: 32_h1=0.1335, 32_l2=0.0681
[60] time=7.11, avg_loss=0.0343, train_err=0.2740
Eval: 32_h1=0.1224, 32_l2=0.0543
[39] time=9.25, avg_loss=0.0530, train_err=0.4236
Eval: 32_h1=0.1318, 32_l2=0.0644
[61] time=6.85, avg_loss=0.0284, train_err=0.2272
Eval: 32_h1=0.1225, 32_l2=0.0544
[40] time=10.01, avg_loss=0.0496, train_err=0.3966
Eval: 32_h1=0.1312, 32_l2=0.0622
[62] time=7.59, avg_loss=0.0263, train_err=0.2105
Eval: 32_h1=0.1217, 32_l2=0.0541
[41] time=9.77, avg_loss=0.0506, train_err=0.4048
[63] time=8.41, avg_loss=0.0246, train_err=0.1964
Eval: 32_h1=0.1322, 32_l2=0.0634
Eval: 32_h1=0.1222, 32_l2=0.0546
[64] time=6.84, avg_loss=0.0254, train_err=0.2033
Eval: 32_h1=0.1233, 32_l2=0.0563
[42] time=10.27, avg_loss=0.0520, train_err=0.4159
Eval: 32_h1=0.1355, 32_l2=0.0693
[65] time=7.60, avg_loss=0.0253, train_err=0.2025
Eval: 32_h1=0.1228, 32_l2=0.0562
[43] time=10.91, avg_loss=0.0516, train_err=0.4126
Eval: 32_h1=0.1309, 32_l2=0.0626
[66] time=7.77, avg_loss=0.0250, train_err=0.2003
Eval: 32_h1=0.1226, 32_l2=0.0574
[67] time=7.18, avg_loss=0.0247, train_err=0.1977
Eval: 32_h1=0.1218, 32_l2=0.0538
[44] time=9.26, avg_loss=0.0524, train_err=0.4189
Eval: 32_h1=0.1326, 32_l2=0.0648
[68] time=7.97, avg_loss=0.0243, train_err=0.1944
Eval: 32_h1=0.1218, 32_l2=0.0534
[45] time=9.25, avg_loss=0.0493, train_err=0.3945
Eval: 32_h1=0.1303, 32_l2=0.0630
[69] time=8.05, avg_loss=0.0249, train_err=0.1992
Eval: 32_h1=0.1242, 32_l2=0.0581
[46] time=9.18, avg_loss=0.0490, train_err=0.3918
Eval: 32_h1=0.1307, 32_l2=0.0616
[70] time=7.42, avg_loss=0.0262, train_err=0.2094
Eval: 32_h1=0.1234, 32_l2=0.0589
[47] time=10.74, avg_loss=0.0471, train_err=0.3766
Eval: 32_h1=0.1287, 32_l2=0.0606
[71] time=7.59, avg_loss=0.0255, train_err=0.2041
Eval: 32_h1=0.1225, 32_l2=0.0543
[72] time=7.38, avg_loss=0.0262, train_err=0.2095
Eval: 32_h1=0.1220, 32_l2=0.0537
[48] time=9.54, avg_loss=0.0454, train_err=0.3631
Eval: 32_h1=0.1299, 32_l2=0.0630
[73] time=7.36, avg_loss=0.0259, train_err=0.2069
Eval: 32_h1=0.1223, 32_l2=0.0582
[49] time=10.09, avg_loss=0.0458, train_err=0.3661
Eval: 32_h1=0.1313, 32_l2=0.0634
[74] time=7.54, avg_loss=0.0270, train_err=0.2157
Eval: 32_h1=0.1241, 32_l2=0.0572
[75] time=6.95, avg_loss=0.0254, train_err=0.2033
[50] time=10.63, avg_loss=0.0489, train_err=0.3910
Eval: 32_h1=0.1214, 32_l2=0.0525
Eval: 32_h1=0.1330, 32_l2=0.0652
[76] time=7.62, avg_loss=0.0236, train_err=0.1885
Eval: 32_h1=0.1221, 32_l2=0.0534
[51] time=9.53, avg_loss=0.0480, train_err=0.3841
Eval: 32_h1=0.1280, 32_l2=0.0603
[77] time=7.77, avg_loss=0.0237, train_err=0.1893
Eval: 32_h1=0.1231, 32_l2=0.0576
[52] time=9.91, avg_loss=0.0460, train_err=0.3680
Eval: 32_h1=0.1291, 32_l2=0.0614
[78] time=6.79, avg_loss=0.0247, train_err=0.1980
Eval: 32_h1=0.1221, 32_l2=0.0537
[53] time=9.27, avg_loss=0.0450, train_err=0.3596
Eval: 32_h1=0.1318, 32_l2=0.0683
[79] time=7.59, avg_loss=0.0249, train_err=0.1994
Eval: 32_h1=0.1221, 32_l2=0.0527
[80] time=7.27, avg_loss=0.0254, train_err=0.2036
Eval: 32_h1=0.1223, 32_l2=0.0540
[54] time=10.29, avg_loss=0.0463, train_err=0.3702
Eval: 32_h1=0.1269, 32_l2=0.0584
[81] time=6.02, avg_loss=0.0250, train_err=0.1997
Eval: 32_h1=0.1232, 32_l2=0.0547
[55] time=10.63, avg_loss=0.0429, train_err=0.3435
Eval: 32_h1=0.1280, 32_l2=0.0600
[82] time=7.81, avg_loss=0.0254, train_err=0.2033
Eval: 32_h1=0.1228, 32_l2=0.0558
[83] time=6.93, avg_loss=0.0244, train_err=0.1953
Eval: 32_h1=0.1223, 32_l2=0.0534
[56] time=10.62, avg_loss=0.0445, train_err=0.3557
Eval: 32_h1=0.1284, 32_l2=0.0613
[84] time=6.91, avg_loss=0.0248, train_err=0.1982
Eval: 32_h1=0.1215, 32_l2=0.0517
[57] time=10.01, avg_loss=0.0455, train_err=0.3643
Eval: 32_h1=0.1301, 32_l2=0.0630
[85] time=6.25, avg_loss=0.0250, train_err=0.2000
Eval: 32_h1=0.1227, 32_l2=0.0554
[86] time=6.97, avg_loss=0.0253, train_err=0.2023
Eval: 32_h1=0.1229, 32_l2=0.0561
[58] time=10.16, avg_loss=0.0440, train_err=0.3523
Eval: 32_h1=0.1300, 32_l2=0.0665
[87] time=8.35, avg_loss=0.0233, train_err=0.1866
Eval: 32_h1=0.1220, 32_l2=0.0529
[59] time=10.00, avg_loss=0.0441, train_err=0.3526
Eval: 32_h1=0.1324, 32_l2=0.0671
[88] time=7.64, avg_loss=0.0237, train_err=0.1898
Eval: 32_h1=0.1229, 32_l2=0.0551
[60] time=10.19, avg_loss=0.0355, train_err=0.2838
Eval: 32_h1=0.1256, 32_l2=0.0589
[61] time=10.27, avg_loss=0.0288, train_err=0.2302
Eval: 32_h1=0.1260, 32_l2=0.0581
logfile: /home/leeshu/wmm/neuraloperator-main/logs/darcy/20260128_072454__L10FNO.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'arch': 'fno', 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_small2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [16, 16], 'hidden_channels': 24, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 2, 'n_layers': 10, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 0, 'hc_dynamic': True}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.005, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 60, 'gamma': 0.5, 'n_epochs': 300}, 'data': {'_config_name': 'darcydatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data', 'batch_size': 8, 'n_train': 1000, 'train_resolution': 32, 'n_tests': [200], 'test_resolutions': [32], 'test_batch_sizes': [8], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'darcy32_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 32 with 200 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-9): 10 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([24, 24, 16, 9]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-9): 10 x Flattened1dConv(
        (conv): Conv1d(24, 24, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-9): 10 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(24, 12, kernel_size=(1,), stride=(1,))
          (1): Conv1d(12, 24, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-9): 10 x SoftGating()
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 48, kernel_size=(1,), stride=(1,))
      (1): Conv1d(48, 24, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(24, 48, kernel_size=(1,), stride=(1,))
      (1): Conv1d(48, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7f592a621880>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7f592a5a30a0>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7f592a5a30a0>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7f592a5f9fa0>}

### Beginning Training...


n_params: 1673857
Training on 1000 samples
Testing on [200] samples         on resolutions [32].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([8, 1, 32, 32])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[0] time=5.69, avg_loss=0.4599, train_err=3.6795
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
logfile: /home/leeshu/wmm/neuraloperator-main/logs/darcy/20260128_072526__L10FNO.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'arch': 'fno', 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_small2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [16, 16], 'hidden_channels': 24, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 2, 'n_layers': 10, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 0, 'hc_dynamic': False}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.005, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 60, 'gamma': 0.5, 'n_epochs': 300}, 'data': {'_config_name': 'darcydatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data', 'batch_size': 8, 'n_train': 1000, 'train_resolution': 32, 'n_tests': [200], 'test_resolutions': [32], 'test_batch_sizes': [8], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'darcy32_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 32 with 200 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-9): 10 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([24, 24, 16, 9]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-9): 10 x Flattened1dConv(
        (conv): Conv1d(24, 24, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-9): 10 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(24, 12, kernel_size=(1,), stride=(1,))
          (1): Conv1d(12, 24, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-9): 10 x SoftGating()
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 48, kernel_size=(1,), stride=(1,))
      (1): Conv1d(48, 24, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(24, 48, kernel_size=(1,), stride=(1,))
      (1): Conv1d(48, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7fbd78531880>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fbd784b40a0>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fbd784b40a0>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fbd78508fa0>}

### Beginning Training...


n_params: 1673857
Training on 1000 samples
Testing on [200] samples         on resolutions [32].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([8, 1, 32, 32])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[0] time=6.16, avg_loss=0.4901, train_err=3.9208
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
Eval: 32_h1=0.2555, 32_l2=0.1697
[1] time=5.38, avg_loss=0.2169, train_err=1.7351
Eval: 32_h1=0.1984, 32_l2=0.1399
[2] time=4.79, avg_loss=0.1778, train_err=1.4223
Eval: 32_h1=0.1761, 32_l2=0.1296
[3] time=4.77, avg_loss=0.1608, train_err=1.2865
Eval: 32_h1=0.1734, 32_l2=0.1259
logfile: /home/leeshu/wmm/neuraloperator-main/logs/darcy/20260128_072554__L10_HC2_MLP.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'arch': 'fno', 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_small2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [16, 16], 'hidden_channels': 24, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 2, 'n_layers': 10, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 2, 'hc_dynamic': True}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.005, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 60, 'gamma': 0.5, 'n_epochs': 300}, 'data': {'_config_name': 'darcydatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data', 'batch_size': 8, 'n_train': 1000, 'train_resolution': 32, 'n_tests': [200], 'test_resolutions': [32], 'test_batch_sizes': [8], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'darcy32_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 32 with 200 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-9): 10 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([24, 24, 16, 9]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-9): 10 x Flattened1dConv(
        (conv): Conv1d(24, 24, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-9): 10 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(24, 12, kernel_size=(1,), stride=(1,))
          (1): Conv1d(12, 24, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-9): 10 x SoftGating()
    )
  )
  (hc_layers): ModuleList(
    (0-9): 10 x HyperConnection(
      (layer_norm): GroupNorm(1, 24, eps=1e-05, affine=True)
      (dynamic_alpha_fn): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (dynamic_beta_fn): Conv2d(24, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 48, kernel_size=(1,), stride=(1,))
      (1): Conv1d(48, 24, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(24, 48, kernel_size=(1,), stride=(1,))
      (1): Conv1d(48, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7fe1e2215250>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fe1e22157f0>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fe1e22157f0>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fe1e2215820>}

### Beginning Training...


n_params: 1676357
Training on 1000 samples
Testing on [200] samples         on resolutions [32].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([8, 1, 32, 32])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[4] time=5.52, avg_loss=0.1404, train_err=1.1232
Eval: 32_h1=0.1557, 32_l2=0.1077
[5] time=5.64, avg_loss=0.1259, train_err=1.0076
Eval: 32_h1=0.1457, 32_l2=0.0962
[0] time=8.98, avg_loss=0.3958, train_err=3.1660
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
Eval: 32_h1=0.2829, 32_l2=0.1847
[6] time=6.88, avg_loss=0.1159, train_err=0.9274
Eval: 32_h1=0.1403, 32_l2=0.0861
[1] time=8.63, avg_loss=0.2475, train_err=1.9802
Eval: 32_h1=0.2237, 32_l2=0.1508
[7] time=5.55, avg_loss=0.1142, train_err=0.9135
Eval: 32_h1=0.1354, 32_l2=0.0779
[8] time=5.04, avg_loss=0.1107, train_err=0.8856
Eval: 32_h1=0.1343, 32_l2=0.0773
[2] time=8.56, avg_loss=0.2125, train_err=1.7001
Eval: 32_h1=0.2289, 32_l2=0.1587
[9] time=5.76, avg_loss=0.0973, train_err=0.7785
Eval: 32_h1=0.1371, 32_l2=0.0737
[3] time=7.13, avg_loss=0.1776, train_err=1.4209
Eval: 32_h1=0.1792, 32_l2=0.1175
[10] time=5.72, avg_loss=0.0907, train_err=0.7255
Eval: 32_h1=0.1322, 32_l2=0.0697
[4] time=8.13, avg_loss=0.1644, train_err=1.3154
Eval: 32_h1=0.1725, 32_l2=0.1111
[11] time=6.94, avg_loss=0.0917, train_err=0.7339
Eval: 32_h1=0.1324, 32_l2=0.0722
[12] time=5.29, avg_loss=0.0842, train_err=0.6737
Eval: 32_h1=0.1304, 32_l2=0.0672
[5] time=8.89, avg_loss=0.1435, train_err=1.1477
Eval: 32_h1=0.1721, 32_l2=0.1072
[13] time=5.53, avg_loss=0.0836, train_err=0.6686
Eval: 32_h1=0.1364, 32_l2=0.0754
[6] time=7.94, avg_loss=0.1384, train_err=1.1068
Eval: 32_h1=0.1634, 32_l2=0.0989
[14] time=5.66, avg_loss=0.0803, train_err=0.6425
Eval: 32_h1=0.1325, 32_l2=0.0705
[15] time=6.04, avg_loss=0.0822, train_err=0.6577
Eval: 32_h1=0.1341, 32_l2=0.0742
[7] time=7.85, avg_loss=0.1231, train_err=0.9851
Eval: 32_h1=0.1503, 32_l2=0.0883
[16] time=5.20, avg_loss=0.0745, train_err=0.5963
Eval: 32_h1=0.1284, 32_l2=0.0674
[8] time=8.51, avg_loss=0.1159, train_err=0.9273
Eval: 32_h1=0.1628, 32_l2=0.0947
[17] time=6.67, avg_loss=0.0695, train_err=0.5558
Eval: 32_h1=0.1290, 32_l2=0.0674
[9] time=8.37, avg_loss=0.1092, train_err=0.8734
Eval: 32_h1=0.1513, 32_l2=0.0862
[18] time=6.09, avg_loss=0.0691, train_err=0.5531
Eval: 32_h1=0.1300, 32_l2=0.0654
[19] time=5.65, avg_loss=0.0716, train_err=0.5731
Eval: 32_h1=0.1307, 32_l2=0.0664
[10] time=7.60, avg_loss=0.0987, train_err=0.7898
Eval: 32_h1=0.1460, 32_l2=0.0782
[20] time=6.00, avg_loss=0.0691, train_err=0.5529
Eval: 32_h1=0.1292, 32_l2=0.0655
[11] time=7.99, avg_loss=0.0974, train_err=0.7789
Eval: 32_h1=0.1457, 32_l2=0.0809
[21] time=6.01, avg_loss=0.0644, train_err=0.5153
Eval: 32_h1=0.1274, 32_l2=0.0631
[12] time=8.01, avg_loss=0.0925, train_err=0.7400
Eval: 32_h1=0.1407, 32_l2=0.0750
[22] time=5.74, avg_loss=0.0636, train_err=0.5089
Eval: 32_h1=0.1254, 32_l2=0.0608
[23] time=6.45, avg_loss=0.0623, train_err=0.4985
Eval: 32_h1=0.1279, 32_l2=0.0656
[13] time=7.47, avg_loss=0.0852, train_err=0.6818
Eval: 32_h1=0.1422, 32_l2=0.0752
[24] time=5.21, avg_loss=0.0599, train_err=0.4790
Eval: 32_h1=0.1311, 32_l2=0.0698
[14] time=7.53, avg_loss=0.0866, train_err=0.6930
Eval: 32_h1=0.1437, 32_l2=0.0752
[25] time=5.03, avg_loss=0.0590, train_err=0.4720
Eval: 32_h1=0.1239, 32_l2=0.0576
[26] time=5.86, avg_loss=0.0608, train_err=0.4867
[15] time=8.36, avg_loss=0.0804, train_err=0.6434
Eval: 32_h1=0.1337, 32_l2=0.0731
Eval: 32_h1=0.1404, 32_l2=0.0729
[27] time=5.21, avg_loss=0.0607, train_err=0.4858
Eval: 32_h1=0.1295, 32_l2=0.0654
[16] time=8.36, avg_loss=0.0765, train_err=0.6117
Eval: 32_h1=0.1406, 32_l2=0.0728
[28] time=6.06, avg_loss=0.0577, train_err=0.4615
Eval: 32_h1=0.1307, 32_l2=0.0704
[17] time=7.84, avg_loss=0.0749, train_err=0.5996
Eval: 32_h1=0.1402, 32_l2=0.0720
[29] time=5.86, avg_loss=0.0562, train_err=0.4497
Eval: 32_h1=0.1322, 32_l2=0.0696
[30] time=5.38, avg_loss=0.0553, train_err=0.4422
Eval: 32_h1=0.1261, 32_l2=0.0611
[18] time=8.65, avg_loss=0.0733, train_err=0.5863
Eval: 32_h1=0.1422, 32_l2=0.0721
[31] time=5.62, avg_loss=0.0534, train_err=0.4276
Eval: 32_h1=0.1259, 32_l2=0.0612
[19] time=8.12, avg_loss=0.0741, train_err=0.5929
Eval: 32_h1=0.1439, 32_l2=0.0742
[32] time=5.18, avg_loss=0.0515, train_err=0.4123
Eval: 32_h1=0.1238, 32_l2=0.0572
[33] time=5.97, avg_loss=0.0519, train_err=0.4151
Eval: 32_h1=0.1267, 32_l2=0.0633
[20] time=8.97, avg_loss=0.0702, train_err=0.5618
Eval: 32_h1=0.1381, 32_l2=0.0669
[34] time=5.96, avg_loss=0.0513, train_err=0.4107
Eval: 32_h1=0.1314, 32_l2=0.0759
[21] time=7.12, avg_loss=0.0732, train_err=0.5854
Eval: 32_h1=0.1342, 32_l2=0.0632
[35] time=5.65, avg_loss=0.0504, train_err=0.4035
Eval: 32_h1=0.1230, 32_l2=0.0575
[22] time=7.70, avg_loss=0.0697, train_err=0.5573
Eval: 32_h1=0.1411, 32_l2=0.0725
[36] time=6.01, avg_loss=0.0519, train_err=0.4153
Eval: 32_h1=0.1231, 32_l2=0.0583
[37] time=5.51, avg_loss=0.0498, train_err=0.3982
Eval: 32_h1=0.1260, 32_l2=0.0613
[23] time=6.94, avg_loss=0.0687, train_err=0.5493
Eval: 32_h1=0.1358, 32_l2=0.0655
[38] time=6.13, avg_loss=0.0485, train_err=0.3880
Eval: 32_h1=0.1227, 32_l2=0.0609
[24] time=6.24, avg_loss=0.0712, train_err=0.5700
Eval: 32_h1=0.1378, 32_l2=0.0692
[39] time=5.52, avg_loss=0.0517, train_err=0.4138
Eval: 32_h1=0.1247, 32_l2=0.0610
[25] time=7.46, avg_loss=0.0663, train_err=0.5304
Eval: 32_h1=0.1386, 32_l2=0.0704
[40] time=6.46, avg_loss=0.0485, train_err=0.3881
Eval: 32_h1=0.1235, 32_l2=0.0613
[26] time=8.16, avg_loss=0.0643, train_err=0.5145
Eval: 32_h1=0.1343, 32_l2=0.0648
[41] time=6.35, avg_loss=0.0482, train_err=0.3852
Eval: 32_h1=0.1237, 32_l2=0.0588
[42] time=5.35, avg_loss=0.0492, train_err=0.3933
Eval: 32_h1=0.1273, 32_l2=0.0645
[27] time=8.27, avg_loss=0.0596, train_err=0.4770
Eval: 32_h1=0.1370, 32_l2=0.0673
[43] time=6.28, avg_loss=0.0499, train_err=0.3991
Eval: 32_h1=0.1230, 32_l2=0.0579
[28] time=7.86, avg_loss=0.0602, train_err=0.4812
Eval: 32_h1=0.1341, 32_l2=0.0651
[44] time=7.11, avg_loss=0.0487, train_err=0.3896
Eval: 32_h1=0.1251, 32_l2=0.0620
[29] time=7.40, avg_loss=0.0565, train_err=0.4517
Eval: 32_h1=0.1324, 32_l2=0.0624
[45] time=6.82, avg_loss=0.0509, train_err=0.4070
Eval: 32_h1=0.1285, 32_l2=0.0701
[30] time=7.92, avg_loss=0.0576, train_err=0.4606
Eval: 32_h1=0.1326, 32_l2=0.0630
[46] time=6.02, avg_loss=0.0487, train_err=0.3893
Eval: 32_h1=0.1240, 32_l2=0.0602
[31] time=7.21, avg_loss=0.0551, train_err=0.4407
Eval: 32_h1=0.1318, 32_l2=0.0643
[47] time=5.29, avg_loss=0.0472, train_err=0.3776
Eval: 32_h1=0.1245, 32_l2=0.0618
[48] time=5.77, avg_loss=0.0464, train_err=0.3708
Eval: 32_h1=0.1237, 32_l2=0.0632
[32] time=8.62, avg_loss=0.0544, train_err=0.4355
Eval: 32_h1=0.1359, 32_l2=0.0689
[49] time=5.43, avg_loss=0.0530, train_err=0.4236
Eval: 32_h1=0.1322, 32_l2=0.0724
[33] time=8.12, avg_loss=0.0546, train_err=0.4364
[50] time=4.74, avg_loss=0.0473, train_err=0.3784
Eval: 32_h1=0.1320, 32_l2=0.0651
Eval: 32_h1=0.1265, 32_l2=0.0632
[51] time=5.05, avg_loss=0.0438, train_err=0.3502
Eval: 32_h1=0.1211, 32_l2=0.0548
[34] time=7.61, avg_loss=0.0556, train_err=0.4445
Eval: 32_h1=0.1316, 32_l2=0.0606
[52] time=4.78, avg_loss=0.0450, train_err=0.3602
Eval: 32_h1=0.1252, 32_l2=0.0610
[35] time=7.36, avg_loss=0.0513, train_err=0.4108
Eval: 32_h1=0.1313, 32_l2=0.0610
[53] time=5.69, avg_loss=0.0438, train_err=0.3502
Eval: 32_h1=0.1227, 32_l2=0.0597
[54] time=5.21, avg_loss=0.0420, train_err=0.3364
Eval: 32_h1=0.1221, 32_l2=0.0568
[36] time=7.47, avg_loss=0.0522, train_err=0.4178
Eval: 32_h1=0.1305, 32_l2=0.0615
[55] time=5.82, avg_loss=0.0419, train_err=0.3354
Eval: 32_h1=0.1212, 32_l2=0.0539
[37] time=7.33, avg_loss=0.0507, train_err=0.4054
Eval: 32_h1=0.1313, 32_l2=0.0661
[56] time=4.63, avg_loss=0.0436, train_err=0.3486
Eval: 32_h1=0.1225, 32_l2=0.0602
[57] time=5.28, avg_loss=0.0435, train_err=0.3481
[38] time=7.20, avg_loss=0.0529, train_err=0.4229
Eval: 32_h1=0.1221, 32_l2=0.0572
Eval: 32_h1=0.1330, 32_l2=0.0651
[58] time=5.82, avg_loss=0.0448, train_err=0.3582
Eval: 32_h1=0.1240, 32_l2=0.0641
[39] time=7.36, avg_loss=0.0567, train_err=0.4533
Eval: 32_h1=0.1311, 32_l2=0.0628
[59] time=5.21, avg_loss=0.0437, train_err=0.3494
Eval: 32_h1=0.1224, 32_l2=0.0566
[40] time=7.94, avg_loss=0.0528, train_err=0.4226
Eval: 32_h1=0.1336, 32_l2=0.0636
[60] time=5.77, avg_loss=0.0353, train_err=0.2822
Eval: 32_h1=0.1203, 32_l2=0.0560
[61] time=5.60, avg_loss=0.0278, train_err=0.2223
Eval: 32_h1=0.1203, 32_l2=0.0585
[41] time=7.48, avg_loss=0.0532, train_err=0.4257
Eval: 32_h1=0.1297, 32_l2=0.0611
[62] time=5.38, avg_loss=0.0256, train_err=0.2048
Eval: 32_h1=0.1189, 32_l2=0.0538
[42] time=7.10, avg_loss=0.0503, train_err=0.4025
Eval: 32_h1=0.1294, 32_l2=0.0597
[63] time=5.76, avg_loss=0.0257, train_err=0.2054
Eval: 32_h1=0.1188, 32_l2=0.0535
[43] time=7.42, avg_loss=0.0493, train_err=0.3946
Eval: 32_h1=0.1344, 32_l2=0.0646
[64] time=6.71, avg_loss=0.0250, train_err=0.2003
Eval: 32_h1=0.1188, 32_l2=0.0535
[44] time=7.21, avg_loss=0.0508, train_err=0.4062
Eval: 32_h1=0.1334, 32_l2=0.0642
[65] time=6.16, avg_loss=0.0250, train_err=0.2001
Eval: 32_h1=0.1192, 32_l2=0.0553
[66] time=5.45, avg_loss=0.0244, train_err=0.1955
Eval: 32_h1=0.1199, 32_l2=0.0567
[45] time=7.83, avg_loss=0.0479, train_err=0.3836
Eval: 32_h1=0.1298, 32_l2=0.0621
[67] time=4.51, avg_loss=0.0260, train_err=0.2081
Eval: 32_h1=0.1190, 32_l2=0.0535
[46] time=8.23, avg_loss=0.0499, train_err=0.3990
Eval: 32_h1=0.1288, 32_l2=0.0591
[68] time=6.47, avg_loss=0.0248, train_err=0.1985
Eval: 32_h1=0.1198, 32_l2=0.0542
[47] time=7.18, avg_loss=0.0469, train_err=0.3749
Eval: 32_h1=0.1291, 32_l2=0.0607
[69] time=5.76, avg_loss=0.0252, train_err=0.2012
Eval: 32_h1=0.1196, 32_l2=0.0548
[70] time=5.96, avg_loss=0.0264, train_err=0.2116
Eval: 32_h1=0.1190, 32_l2=0.0552
[48] time=7.84, avg_loss=0.0464, train_err=0.3710
Eval: 32_h1=0.1315, 32_l2=0.0655
[71] time=5.96, avg_loss=0.0264, train_err=0.2113
Eval: 32_h1=0.1198, 32_l2=0.0542
[49] time=8.57, avg_loss=0.0462, train_err=0.3697
Eval: 32_h1=0.1291, 32_l2=0.0605
[72] time=5.10, avg_loss=0.0253, train_err=0.2027
Eval: 32_h1=0.1190, 32_l2=0.0534
[73] time=5.37, avg_loss=0.0277, train_err=0.2216
Eval: 32_h1=0.1207, 32_l2=0.0583
[50] time=7.88, avg_loss=0.0459, train_err=0.3672
Eval: 32_h1=0.1271, 32_l2=0.0585
[74] time=4.44, avg_loss=0.0279, train_err=0.2230
Eval: 32_h1=0.1200, 32_l2=0.0538
[51] time=7.99, avg_loss=0.0466, train_err=0.3731
Eval: 32_h1=0.1293, 32_l2=0.0602
[75] time=4.49, avg_loss=0.0282, train_err=0.2256
Eval: 32_h1=0.1199, 32_l2=0.0548
[76] time=5.80, avg_loss=0.0250, train_err=0.1998
Eval: 32_h1=0.1190, 32_l2=0.0543
[52] time=8.19, avg_loss=0.0451, train_err=0.3605
Eval: 32_h1=0.1300, 32_l2=0.0618
[77] time=5.88, avg_loss=0.0235, train_err=0.1877
Eval: 32_h1=0.1189, 32_l2=0.0541
[53] time=7.65, avg_loss=0.0459, train_err=0.3675
Eval: 32_h1=0.1295, 32_l2=0.0619
[78] time=5.74, avg_loss=0.0232, train_err=0.1855
Eval: 32_h1=0.1196, 32_l2=0.0538
[79] time=4.73, avg_loss=0.0258, train_err=0.2064
Eval: 32_h1=0.1202, 32_l2=0.0549
[54] time=8.18, avg_loss=0.0430, train_err=0.3439
Eval: 32_h1=0.1278, 32_l2=0.0608
[80] time=5.19, avg_loss=0.0257, train_err=0.2053
Eval: 32_h1=0.1198, 32_l2=0.0557
[55] time=7.30, avg_loss=0.0453, train_err=0.3625
Eval: 32_h1=0.1285, 32_l2=0.0590
[81] time=5.90, avg_loss=0.0251, train_err=0.2010
Eval: 32_h1=0.1189, 32_l2=0.0532
[56] time=8.48, avg_loss=0.0508, train_err=0.4068
[82] time=5.90, avg_loss=0.0248, train_err=0.1985
Eval: 32_h1=0.1282, 32_l2=0.0614
Eval: 32_h1=0.1193, 32_l2=0.0557
[83] time=5.66, avg_loss=0.0238, train_err=0.1901
Eval: 32_h1=0.1193, 32_l2=0.0540
[57] time=7.91, avg_loss=0.0453, train_err=0.3621
Eval: 32_h1=0.1268, 32_l2=0.0571
[84] time=5.86, avg_loss=0.0237, train_err=0.1893
Eval: 32_h1=0.1194, 32_l2=0.0538
[58] time=8.33, avg_loss=0.0434, train_err=0.3471
Eval: 32_h1=0.1287, 32_l2=0.0624
[85] time=5.40, avg_loss=0.0241, train_err=0.1925
Eval: 32_h1=0.1187, 32_l2=0.0534
[86] time=5.81, avg_loss=0.0258, train_err=0.2067
Eval: 32_h1=0.1193, 32_l2=0.0553
[59] time=8.42, avg_loss=0.0438, train_err=0.3506
Eval: 32_h1=0.1271, 32_l2=0.0597
[87] time=5.57, avg_loss=0.0254, train_err=0.2032
Eval: 32_h1=0.1190, 32_l2=0.0535
[60] time=8.45, avg_loss=0.0337, train_err=0.2698
Eval: 32_h1=0.1241, 32_l2=0.0572
[88] time=5.53, avg_loss=0.0241, train_err=0.1929
Eval: 32_h1=0.1193, 32_l2=0.0537
[89] time=5.45, avg_loss=0.0248, train_err=0.1986
Eval: 32_h1=0.1191, 32_l2=0.0532
[61] time=9.27, avg_loss=0.0267, train_err=0.2134
Eval: 32_h1=0.1249, 32_l2=0.0576
[90] time=5.22, avg_loss=0.0250, train_err=0.2003
Eval: 32_h1=0.1186, 32_l2=0.0525
[62] time=7.77, avg_loss=0.0258, train_err=0.2064
Eval: 32_h1=0.1241, 32_l2=0.0565
[91] time=6.79, avg_loss=0.0229, train_err=0.1831
Eval: 32_h1=0.1199, 32_l2=0.0553
[92] time=5.88, avg_loss=0.0241, train_err=0.1928
Eval: 32_h1=0.1195, 32_l2=0.0538
[63] time=8.40, avg_loss=0.0252, train_err=0.2016
Eval: 32_h1=0.1245, 32_l2=0.0570
[93] time=5.62, avg_loss=0.0242, train_err=0.1933
Eval: 32_h1=0.1198, 32_l2=0.0548
[64] time=8.10, avg_loss=0.0249, train_err=0.1990
Eval: 32_h1=0.1239, 32_l2=0.0554
[94] time=5.30, avg_loss=0.0239, train_err=0.1912
Eval: 32_h1=0.1192, 32_l2=0.0532
[65] time=7.78, avg_loss=0.0251, train_err=0.2009
Eval: 32_h1=0.1239, 32_l2=0.0559
[95] time=6.57, avg_loss=0.0238, train_err=0.1906
Eval: 32_h1=0.1194, 32_l2=0.0539
[96] time=5.60, avg_loss=0.0244, train_err=0.1954
Eval: 32_h1=0.1214, 32_l2=0.0561
[66] time=8.36, avg_loss=0.0249, train_err=0.1991
Eval: 32_h1=0.1243, 32_l2=0.0561
[97] time=5.16, avg_loss=0.0236, train_err=0.1890
Eval: 32_h1=0.1197, 32_l2=0.0557
[67] time=7.68, avg_loss=0.0262, train_err=0.2098
Eval: 32_h1=0.1247, 32_l2=0.0563
[98] time=6.11, avg_loss=0.0233, train_err=0.1863
Eval: 32_h1=0.1194, 32_l2=0.0562
[99] time=5.28, avg_loss=0.0235, train_err=0.1881
Eval: 32_h1=0.1195, 32_l2=0.0543
[68] time=8.62, avg_loss=0.0258, train_err=0.2061
Eval: 32_h1=0.1244, 32_l2=0.0563
[100] time=4.93, avg_loss=0.0239, train_err=0.1908
Eval: 32_h1=0.1196, 32_l2=0.0550
[69] time=8.18, avg_loss=0.0255, train_err=0.2036
Eval: 32_h1=0.1258, 32_l2=0.0579
[101] time=5.52, avg_loss=0.0240, train_err=0.1918
Eval: 32_h1=0.1191, 32_l2=0.0548
[102] time=5.98, avg_loss=0.0225, train_err=0.1803
Eval: 32_h1=0.1190, 32_l2=0.0531
[70] time=8.07, avg_loss=0.0276, train_err=0.2212
Eval: 32_h1=0.1242, 32_l2=0.0559
[103] time=5.76, avg_loss=0.0229, train_err=0.1835
Eval: 32_h1=0.1197, 32_l2=0.0547
[71] time=8.02, avg_loss=0.0264, train_err=0.2111
Eval: 32_h1=0.1235, 32_l2=0.0556
[104] time=5.02, avg_loss=0.0241, train_err=0.1926
Eval: 32_h1=0.1206, 32_l2=0.0555
[105] time=5.26, avg_loss=0.0236, train_err=0.1888
Eval: 32_h1=0.1192, 32_l2=0.0542
[72] time=7.71, avg_loss=0.0255, train_err=0.2044
Eval: 32_h1=0.1251, 32_l2=0.0564
[106] time=5.26, avg_loss=0.0227, train_err=0.1819
Eval: 32_h1=0.1191, 32_l2=0.0526
[73] time=7.99, avg_loss=0.0252, train_err=0.2018
Eval: 32_h1=0.1241, 32_l2=0.0554
[107] time=6.45, avg_loss=0.0232, train_err=0.1854
Eval: 32_h1=0.1185, 32_l2=0.0527
[74] time=8.19, avg_loss=0.0253, train_err=0.2026
Eval: 32_h1=0.1240, 32_l2=0.0557
[108] time=5.76, avg_loss=0.0229, train_err=0.1833
Eval: 32_h1=0.1199, 32_l2=0.0547
[109] time=5.27, avg_loss=0.0232, train_err=0.1855
Eval: 32_h1=0.1189, 32_l2=0.0525
[75] time=8.59, avg_loss=0.0264, train_err=0.2114
Eval: 32_h1=0.1249, 32_l2=0.0572
[110] time=4.73, avg_loss=0.0226, train_err=0.1808
Eval: 32_h1=0.1185, 32_l2=0.0529
[111] time=5.38, avg_loss=0.0232, train_err=0.1859
Eval: 32_h1=0.1197, 32_l2=0.0564
[76] time=8.94, avg_loss=0.0264, train_err=0.2116
Eval: 32_h1=0.1243, 32_l2=0.0556
[112] time=5.80, avg_loss=0.0228, train_err=0.1825
Eval: 32_h1=0.1196, 32_l2=0.0540
[77] time=7.77, avg_loss=0.0268, train_err=0.2147
Eval: 32_h1=0.1248, 32_l2=0.0574
[113] time=5.82, avg_loss=0.0234, train_err=0.1873
Eval: 32_h1=0.1191, 32_l2=0.0538
[114] time=5.24, avg_loss=0.0233, train_err=0.1864
Eval: 32_h1=0.1192, 32_l2=0.0533
[78] time=8.45, avg_loss=0.0251, train_err=0.2008
Eval: 32_h1=0.1244, 32_l2=0.0553
[115] time=5.09, avg_loss=0.0231, train_err=0.1846
Eval: 32_h1=0.1189, 32_l2=0.0534
[79] time=7.87, avg_loss=0.0244, train_err=0.1950
Eval: 32_h1=0.1259, 32_l2=0.0583
[116] time=5.79, avg_loss=0.0230, train_err=0.1841
Eval: 32_h1=0.1194, 32_l2=0.0536
[80] time=8.11, avg_loss=0.0253, train_err=0.2021
[117] time=5.82, avg_loss=0.0219, train_err=0.1752
Eval: 32_h1=0.1250, 32_l2=0.0571
Eval: 32_h1=0.1194, 32_l2=0.0542
[118] time=5.85, avg_loss=0.0227, train_err=0.1817
Eval: 32_h1=0.1193, 32_l2=0.0538
[81] time=7.69, avg_loss=0.0240, train_err=0.1917
Eval: 32_h1=0.1258, 32_l2=0.0578
[119] time=5.30, avg_loss=0.0227, train_err=0.1814
Eval: 32_h1=0.1193, 32_l2=0.0530
[82] time=7.40, avg_loss=0.0241, train_err=0.1931
Eval: 32_h1=0.1235, 32_l2=0.0561
[120] time=5.93, avg_loss=0.0185, train_err=0.1478
Eval: 32_h1=0.1184, 32_l2=0.0531
[83] time=7.88, avg_loss=0.0251, train_err=0.2010
Eval: 32_h1=0.1244, 32_l2=0.0558
[121] time=5.98, avg_loss=0.0148, train_err=0.1180
Eval: 32_h1=0.1183, 32_l2=0.0524
[122] time=6.32, avg_loss=0.0140, train_err=0.1121
Eval: 32_h1=0.1185, 32_l2=0.0529
[84] time=8.14, avg_loss=0.0251, train_err=0.2004
Eval: 32_h1=0.1244, 32_l2=0.0583
[123] time=5.73, avg_loss=0.0134, train_err=0.1070
Eval: 32_h1=0.1182, 32_l2=0.0526
[85] time=7.32, avg_loss=0.0250, train_err=0.1998
Eval: 32_h1=0.1246, 32_l2=0.0561
[124] time=5.91, avg_loss=0.0132, train_err=0.1052
Eval: 32_h1=0.1187, 32_l2=0.0534
[86] time=7.69, avg_loss=0.0252, train_err=0.2013
Eval: 32_h1=0.1241, 32_l2=0.0560
[125] time=5.47, avg_loss=0.0132, train_err=0.1053
Eval: 32_h1=0.1185, 32_l2=0.0530
[126] time=6.53, avg_loss=0.0139, train_err=0.1116
Eval: 32_h1=0.1183, 32_l2=0.0525
[87] time=8.04, avg_loss=0.0239, train_err=0.1914
Eval: 32_h1=0.1248, 32_l2=0.0560
[127] time=7.14, avg_loss=0.0140, train_err=0.1119
Eval: 32_h1=0.1183, 32_l2=0.0523
[88] time=7.97, avg_loss=0.0241, train_err=0.1931
Eval: 32_h1=0.1237, 32_l2=0.0551
[128] time=5.19, avg_loss=0.0138, train_err=0.1104
Eval: 32_h1=0.1186, 32_l2=0.0536
[89] time=6.84, avg_loss=0.0247, train_err=0.1973
Eval: 32_h1=0.1243, 32_l2=0.0560
[129] time=5.84, avg_loss=0.0136, train_err=0.1092
Eval: 32_h1=0.1181, 32_l2=0.0520
[90] time=7.28, avg_loss=0.0248, train_err=0.1985
Eval: 32_h1=0.1244, 32_l2=0.0570
[130] time=6.17, avg_loss=0.0137, train_err=0.1095
Eval: 32_h1=0.1188, 32_l2=0.0537
[91] time=7.84, avg_loss=0.0244, train_err=0.1955
[131] time=6.64, avg_loss=0.0137, train_err=0.1098
Eval: 32_h1=0.1234, 32_l2=0.0545
Eval: 32_h1=0.1187, 32_l2=0.0537
[132] time=4.93, avg_loss=0.0142, train_err=0.1136
Eval: 32_h1=0.1187, 32_l2=0.0527
[92] time=7.55, avg_loss=0.0242, train_err=0.1933
Eval: 32_h1=0.1253, 32_l2=0.0585
[133] time=4.75, avg_loss=0.0145, train_err=0.1159
Eval: 32_h1=0.1187, 32_l2=0.0526
[93] time=8.22, avg_loss=0.0262, train_err=0.2100
[134] time=6.06, avg_loss=0.0151, train_err=0.1208
Eval: 32_h1=0.1245, 32_l2=0.0569
Eval: 32_h1=0.1186, 32_l2=0.0533
[135] time=5.81, avg_loss=0.0144, train_err=0.1154
Eval: 32_h1=0.1187, 32_l2=0.0524
[94] time=8.15, avg_loss=0.0255, train_err=0.2037
Eval: 32_h1=0.1247, 32_l2=0.0562
[136] time=4.59, avg_loss=0.0144, train_err=0.1149
Eval: 32_h1=0.1184, 32_l2=0.0526
[137] time=4.54, avg_loss=0.0137, train_err=0.1095
Eval: 32_h1=0.1186, 32_l2=0.0527
[95] time=8.27, avg_loss=0.0242, train_err=0.1933
Eval: 32_h1=0.1239, 32_l2=0.0554
[138] time=5.01, avg_loss=0.0139, train_err=0.1110
Eval: 32_h1=0.1186, 32_l2=0.0529
[96] time=7.23, avg_loss=0.0250, train_err=0.1998
Eval: 32_h1=0.1236, 32_l2=0.0551
[139] time=5.74, avg_loss=0.0142, train_err=0.1139
Eval: 32_h1=0.1189, 32_l2=0.0543
[97] time=7.20, avg_loss=0.0224, train_err=0.1792
Eval: 32_h1=0.1236, 32_l2=0.0549
[140] time=5.78, avg_loss=0.0140, train_err=0.1122
Eval: 32_h1=0.1185, 32_l2=0.0523
[141] time=6.38, avg_loss=0.0145, train_err=0.1162
Eval: 32_h1=0.1185, 32_l2=0.0532
[98] time=8.08, avg_loss=0.0248, train_err=0.1981
Eval: 32_h1=0.1242, 32_l2=0.0554
[142] time=5.55, avg_loss=0.0140, train_err=0.1123
Eval: 32_h1=0.1187, 32_l2=0.0530
[99] time=7.83, avg_loss=0.0231, train_err=0.1847
Eval: 32_h1=0.1237, 32_l2=0.0557
[143] time=7.04, avg_loss=0.0135, train_err=0.1080
Eval: 32_h1=0.1183, 32_l2=0.0526
[100] time=7.55, avg_loss=0.0238, train_err=0.1903
Eval: 32_h1=0.1238, 32_l2=0.0549
[144] time=5.40, avg_loss=0.0137, train_err=0.1094
Eval: 32_h1=0.1185, 32_l2=0.0526
[145] time=5.65, avg_loss=0.0139, train_err=0.1111
Eval: 32_h1=0.1186, 32_l2=0.0524
[101] time=8.64, avg_loss=0.0229, train_err=0.1833
Eval: 32_h1=0.1244, 32_l2=0.0560
[146] time=4.90, avg_loss=0.0143, train_err=0.1142
Eval: 32_h1=0.1185, 32_l2=0.0524
[102] time=8.04, avg_loss=0.0230, train_err=0.1837
Eval: 32_h1=0.1234, 32_l2=0.0555
[147] time=5.44, avg_loss=0.0138, train_err=0.1107
Eval: 32_h1=0.1186, 32_l2=0.0528
[148] time=6.58, avg_loss=0.0135, train_err=0.1084
[103] time=8.47, avg_loss=0.0230, train_err=0.1842
Eval: 32_h1=0.1187, 32_l2=0.0529
Eval: 32_h1=0.1246, 32_l2=0.0566
[149] time=6.44, avg_loss=0.0139, train_err=0.1109
Eval: 32_h1=0.1188, 32_l2=0.0532
[104] time=9.29, avg_loss=0.0235, train_err=0.1882
Eval: 32_h1=0.1239, 32_l2=0.0556
[150] time=7.23, avg_loss=0.0140, train_err=0.1119
Eval: 32_h1=0.1187, 32_l2=0.0527
[105] time=10.63, avg_loss=0.0243, train_err=0.1942
Eval: 32_h1=0.1237, 32_l2=0.0555
[151] time=7.80, avg_loss=0.0139, train_err=0.1114
Eval: 32_h1=0.1189, 32_l2=0.0532
[152] time=8.04, avg_loss=0.0138, train_err=0.1102
Eval: 32_h1=0.1188, 32_l2=0.0530
[106] time=10.40, avg_loss=0.0226, train_err=0.1809
Eval: 32_h1=0.1238, 32_l2=0.0558
[153] time=6.63, avg_loss=0.0133, train_err=0.1067
Eval: 32_h1=0.1186, 32_l2=0.0523
[107] time=10.47, avg_loss=0.0244, train_err=0.1950
Eval: 32_h1=0.1247, 32_l2=0.0570
[154] time=7.12, avg_loss=0.0133, train_err=0.1060
Eval: 32_h1=0.1187, 32_l2=0.0532
[108] time=9.00, avg_loss=0.0236, train_err=0.1885
Eval: 32_h1=0.1235, 32_l2=0.0557
[155] time=7.12, avg_loss=0.0131, train_err=0.1052
Eval: 32_h1=0.1189, 32_l2=0.0545
[156] time=7.64, avg_loss=0.0139, train_err=0.1114
Eval: 32_h1=0.1190, 32_l2=0.0541
[109] time=10.55, avg_loss=0.0222, train_err=0.1779
Eval: 32_h1=0.1242, 32_l2=0.0559
[157] time=7.77, avg_loss=0.0141, train_err=0.1130
Eval: 32_h1=0.1186, 32_l2=0.0524
[110] time=9.71, avg_loss=0.0233, train_err=0.1863
Eval: 32_h1=0.1241, 32_l2=0.0563
[158] time=7.73, avg_loss=0.0144, train_err=0.1150
Eval: 32_h1=0.1188, 32_l2=0.0531
[111] time=9.92, avg_loss=0.0232, train_err=0.1853
Eval: 32_h1=0.1250, 32_l2=0.0561
[159] time=7.76, avg_loss=0.0136, train_err=0.1090
Eval: 32_h1=0.1192, 32_l2=0.0546
[160] time=7.30, avg_loss=0.0131, train_err=0.1045
Eval: 32_h1=0.1187, 32_l2=0.0533
[112] time=9.75, avg_loss=0.0237, train_err=0.1897
Eval: 32_h1=0.1243, 32_l2=0.0579
[161] time=7.47, avg_loss=0.0133, train_err=0.1067
Eval: 32_h1=0.1190, 32_l2=0.0529
[113] time=10.25, avg_loss=0.0226, train_err=0.1809
Eval: 32_h1=0.1234, 32_l2=0.0545
[162] time=6.70, avg_loss=0.0133, train_err=0.1062
Eval: 32_h1=0.1189, 32_l2=0.0539
[114] time=9.85, avg_loss=0.0233, train_err=0.1863
[163] time=7.54, avg_loss=0.0134, train_err=0.1069
Eval: 32_h1=0.1233, 32_l2=0.0547
Eval: 32_h1=0.1187, 32_l2=0.0532
[164] time=7.92, avg_loss=0.0130, train_err=0.1037
Eval: 32_h1=0.1185, 32_l2=0.0522
[115] time=9.92, avg_loss=0.0232, train_err=0.1854
Eval: 32_h1=0.1231, 32_l2=0.0554
[165] time=7.65, avg_loss=0.0133, train_err=0.1062
Eval: 32_h1=0.1189, 32_l2=0.0528
[116] time=10.31, avg_loss=0.0228, train_err=0.1821
Eval: 32_h1=0.1235, 32_l2=0.0549
[166] time=7.87, avg_loss=0.0135, train_err=0.1083
Eval: 32_h1=0.1188, 32_l2=0.0536
[117] time=10.10, avg_loss=0.0229, train_err=0.1831
[167] time=7.11, avg_loss=0.0131, train_err=0.1049
Eval: 32_h1=0.1232, 32_l2=0.0545
Eval: 32_h1=0.1187, 32_l2=0.0529
[168] time=7.27, avg_loss=0.0129, train_err=0.1031
Eval: 32_h1=0.1188, 32_l2=0.0530
[118] time=9.36, avg_loss=0.0218, train_err=0.1741
Eval: 32_h1=0.1235, 32_l2=0.0560
[169] time=6.80, avg_loss=0.0128, train_err=0.1022
Eval: 32_h1=0.1185, 32_l2=0.0532
[119] time=9.81, avg_loss=0.0223, train_err=0.1785
Eval: 32_h1=0.1247, 32_l2=0.0577
[170] time=6.87, avg_loss=0.0132, train_err=0.1058
Eval: 32_h1=0.1191, 32_l2=0.0530
[171] time=7.29, avg_loss=0.0138, train_err=0.1106
[120] time=10.19, avg_loss=0.0189, train_err=0.1509
Eval: 32_h1=0.1187, 32_l2=0.0531
Eval: 32_h1=0.1228, 32_l2=0.0547
[172] time=7.50, avg_loss=0.0134, train_err=0.1073
Eval: 32_h1=0.1191, 32_l2=0.0534
[121] time=9.64, avg_loss=0.0143, train_err=0.1146
Eval: 32_h1=0.1229, 32_l2=0.0540
[173] time=6.92, avg_loss=0.0131, train_err=0.1050
Eval: 32_h1=0.1188, 32_l2=0.0529
[122] time=10.10, avg_loss=0.0132, train_err=0.1056
Eval: 32_h1=0.1230, 32_l2=0.0552
[174] time=7.15, avg_loss=0.0131, train_err=0.1050
Eval: 32_h1=0.1188, 32_l2=0.0534
[175] time=7.53, avg_loss=0.0132, train_err=0.1056
Eval: 32_h1=0.1185, 32_l2=0.0530
[123] time=10.43, avg_loss=0.0134, train_err=0.1072
Eval: 32_h1=0.1230, 32_l2=0.0550
[176] time=7.65, avg_loss=0.0130, train_err=0.1039
Eval: 32_h1=0.1189, 32_l2=0.0530
[124] time=8.88, avg_loss=0.0134, train_err=0.1072
Eval: 32_h1=0.1229, 32_l2=0.0543
[177] time=5.63, avg_loss=0.0129, train_err=0.1031
Eval: 32_h1=0.1188, 32_l2=0.0532
[125] time=7.43, avg_loss=0.0137, train_err=0.1095
Eval: 32_h1=0.1229, 32_l2=0.0545
[178] time=5.42, avg_loss=0.0130, train_err=0.1042
Eval: 32_h1=0.1188, 32_l2=0.0531
[179] time=6.28, avg_loss=0.0131, train_err=0.1047
[126] time=8.24, avg_loss=0.0137, train_err=0.1096
Eval: 32_h1=0.1188, 32_l2=0.0530
Eval: 32_h1=0.1234, 32_l2=0.0545
[180] time=5.55, avg_loss=0.0111, train_err=0.0890
Eval: 32_h1=0.1187, 32_l2=0.0532
[127] time=8.29, avg_loss=0.0136, train_err=0.1087
Eval: 32_h1=0.1227, 32_l2=0.0543
[181] time=6.34, avg_loss=0.0093, train_err=0.0747
Eval: 32_h1=0.1186, 32_l2=0.0531
[128] time=8.88, avg_loss=0.0138, train_err=0.1105
[182] time=5.45, avg_loss=0.0089, train_err=0.0709
Eval: 32_h1=0.1187, 32_l2=0.0529
Eval: 32_h1=0.1231, 32_l2=0.0552
[183] time=5.80, avg_loss=0.0088, train_err=0.0706
Eval: 32_h1=0.1186, 32_l2=0.0527
[129] time=8.10, avg_loss=0.0139, train_err=0.1108
Eval: 32_h1=0.1230, 32_l2=0.0548
[184] time=6.18, avg_loss=0.0088, train_err=0.0704
Eval: 32_h1=0.1186, 32_l2=0.0526
[130] time=8.55, avg_loss=0.0137, train_err=0.1098
Eval: 32_h1=0.1227, 32_l2=0.0543
[185] time=5.68, avg_loss=0.0088, train_err=0.0702
Eval: 32_h1=0.1186, 32_l2=0.0528
[186] time=5.49, avg_loss=0.0087, train_err=0.0694
Eval: 32_h1=0.1185, 32_l2=0.0524
[131] time=8.14, avg_loss=0.0144, train_err=0.1151
Eval: 32_h1=0.1230, 32_l2=0.0546
[187] time=5.90, avg_loss=0.0087, train_err=0.0694
Eval: 32_h1=0.1186, 32_l2=0.0523
[132] time=7.28, avg_loss=0.0142, train_err=0.1137
Eval: 32_h1=0.1230, 32_l2=0.0542
[188] time=5.96, avg_loss=0.0089, train_err=0.0710
Eval: 32_h1=0.1186, 32_l2=0.0525
[133] time=8.81, avg_loss=0.0142, train_err=0.1138
[189] time=5.90, avg_loss=0.0089, train_err=0.0714
Eval: 32_h1=0.1235, 32_l2=0.0546
Eval: 32_h1=0.1189, 32_l2=0.0534
[190] time=7.02, avg_loss=0.0091, train_err=0.0726
[134] time=7.34, avg_loss=0.0143, train_err=0.1145
Eval: 32_h1=0.1188, 32_l2=0.0527
Eval: 32_h1=0.1231, 32_l2=0.0547
[191] time=5.63, avg_loss=0.0092, train_err=0.0733
Eval: 32_h1=0.1187, 32_l2=0.0523
[135] time=6.92, avg_loss=0.0138, train_err=0.1103
Eval: 32_h1=0.1233, 32_l2=0.0554
[192] time=6.14, avg_loss=0.0094, train_err=0.0750
Eval: 32_h1=0.1186, 32_l2=0.0527
[136] time=7.37, avg_loss=0.0137, train_err=0.1100
Eval: 32_h1=0.1230, 32_l2=0.0547
[193] time=5.37, avg_loss=0.0091, train_err=0.0731
Eval: 32_h1=0.1189, 32_l2=0.0530
[137] time=7.60, avg_loss=0.0134, train_err=0.1070
Eval: 32_h1=0.1231, 32_l2=0.0544
[194] time=5.53, avg_loss=0.0093, train_err=0.0747
Eval: 32_h1=0.1188, 32_l2=0.0529
[195] time=5.20, avg_loss=0.0094, train_err=0.0748
Eval: 32_h1=0.1187, 32_l2=0.0524
[138] time=7.55, avg_loss=0.0134, train_err=0.1075
Eval: 32_h1=0.1227, 32_l2=0.0539
[196] time=4.79, avg_loss=0.0092, train_err=0.0736
Eval: 32_h1=0.1188, 32_l2=0.0528
[139] time=8.66, avg_loss=0.0139, train_err=0.1108
Eval: 32_h1=0.1229, 32_l2=0.0548
[197] time=6.13, avg_loss=0.0096, train_err=0.0768
Eval: 32_h1=0.1187, 32_l2=0.0526
[198] time=5.03, avg_loss=0.0092, train_err=0.0740
Eval: 32_h1=0.1187, 32_l2=0.0526
[140] time=8.73, avg_loss=0.0132, train_err=0.1059
Eval: 32_h1=0.1230, 32_l2=0.0546
[199] time=5.16, avg_loss=0.0091, train_err=0.0730
Eval: 32_h1=0.1189, 32_l2=0.0529
[200] time=4.97, avg_loss=0.0093, train_err=0.0742
Eval: 32_h1=0.1187, 32_l2=0.0529
[141] time=8.18, avg_loss=0.0133, train_err=0.1062
Eval: 32_h1=0.1229, 32_l2=0.0545
[201] time=6.89, avg_loss=0.0096, train_err=0.0768
Eval: 32_h1=0.1187, 32_l2=0.0525
[142] time=7.49, avg_loss=0.0138, train_err=0.1107
Eval: 32_h1=0.1230, 32_l2=0.0544
[202] time=5.84, avg_loss=0.0095, train_err=0.0758
Eval: 32_h1=0.1187, 32_l2=0.0524
[143] time=8.23, avg_loss=0.0134, train_err=0.1073
Eval: 32_h1=0.1227, 32_l2=0.0542
[203] time=5.46, avg_loss=0.0096, train_err=0.0772
Eval: 32_h1=0.1186, 32_l2=0.0520
[144] time=7.74, avg_loss=0.0137, train_err=0.1095
Eval: 32_h1=0.1227, 32_l2=0.0542
[204] time=6.74, avg_loss=0.0094, train_err=0.0752
Eval: 32_h1=0.1188, 32_l2=0.0528
[205] time=6.09, avg_loss=0.0098, train_err=0.0781
Eval: 32_h1=0.1187, 32_l2=0.0530
[145] time=8.35, avg_loss=0.0138, train_err=0.1103
Eval: 32_h1=0.1232, 32_l2=0.0545
[206] time=6.47, avg_loss=0.0091, train_err=0.0731
Eval: 32_h1=0.1186, 32_l2=0.0522
[146] time=8.29, avg_loss=0.0140, train_err=0.1117
Eval: 32_h1=0.1231, 32_l2=0.0546
[207] time=5.74, avg_loss=0.0087, train_err=0.0699
Eval: 32_h1=0.1186, 32_l2=0.0522
[208] time=6.11, avg_loss=0.0088, train_err=0.0703
Eval: 32_h1=0.1188, 32_l2=0.0526
[147] time=9.20, avg_loss=0.0145, train_err=0.1163
Eval: 32_h1=0.1233, 32_l2=0.0549
[209] time=5.81, avg_loss=0.0086, train_err=0.0691
Eval: 32_h1=0.1187, 32_l2=0.0527
[148] time=8.70, avg_loss=0.0140, train_err=0.1118
Eval: 32_h1=0.1232, 32_l2=0.0552
[210] time=6.11, avg_loss=0.0087, train_err=0.0695
Eval: 32_h1=0.1189, 32_l2=0.0526
[149] time=7.30, avg_loss=0.0136, train_err=0.1084
Eval: 32_h1=0.1227, 32_l2=0.0536
[211] time=5.86, avg_loss=0.0089, train_err=0.0709
Eval: 32_h1=0.1186, 32_l2=0.0524
[212] time=5.90, avg_loss=0.0089, train_err=0.0712
Eval: 32_h1=0.1190, 32_l2=0.0530
[150] time=8.32, avg_loss=0.0129, train_err=0.1031
Eval: 32_h1=0.1226, 32_l2=0.0538
[213] time=4.90, avg_loss=0.0094, train_err=0.0753
Eval: 32_h1=0.1187, 32_l2=0.0525
[214] time=5.06, avg_loss=0.0091, train_err=0.0726
Eval: 32_h1=0.1187, 32_l2=0.0523
[151] time=9.77, avg_loss=0.0128, train_err=0.1024
Eval: 32_h1=0.1230, 32_l2=0.0544
[215] time=6.58, avg_loss=0.0090, train_err=0.0721
Eval: 32_h1=0.1188, 32_l2=0.0528
[152] time=7.25, avg_loss=0.0131, train_err=0.1047
Eval: 32_h1=0.1229, 32_l2=0.0539
[216] time=6.27, avg_loss=0.0089, train_err=0.0711
Eval: 32_h1=0.1187, 32_l2=0.0525
[153] time=9.23, avg_loss=0.0132, train_err=0.1053
Eval: 32_h1=0.1230, 32_l2=0.0541
[217] time=5.63, avg_loss=0.0089, train_err=0.0709
Eval: 32_h1=0.1187, 32_l2=0.0528
[154] time=7.09, avg_loss=0.0131, train_err=0.1052
[218] time=6.11, avg_loss=0.0088, train_err=0.0706
Eval: 32_h1=0.1228, 32_l2=0.0545
Eval: 32_h1=0.1189, 32_l2=0.0527
[219] time=4.61, avg_loss=0.0089, train_err=0.0710
Eval: 32_h1=0.1188, 32_l2=0.0527
[155] time=9.16, avg_loss=0.0136, train_err=0.1085
Eval: 32_h1=0.1229, 32_l2=0.0542
[220] time=6.06, avg_loss=0.0087, train_err=0.0699
Eval: 32_h1=0.1187, 32_l2=0.0522
[156] time=6.33, avg_loss=0.0133, train_err=0.1061
Eval: 32_h1=0.1226, 32_l2=0.0534
[221] time=5.38, avg_loss=0.0088, train_err=0.0705
Eval: 32_h1=0.1188, 32_l2=0.0527
[222] time=5.33, avg_loss=0.0089, train_err=0.0710
Eval: 32_h1=0.1187, 32_l2=0.0529
[157] time=6.78, avg_loss=0.0138, train_err=0.1108
Eval: 32_h1=0.1229, 32_l2=0.0549
[223] time=5.90, avg_loss=0.0088, train_err=0.0706
Eval: 32_h1=0.1188, 32_l2=0.0526
[158] time=7.44, avg_loss=0.0139, train_err=0.1111
Eval: 32_h1=0.1234, 32_l2=0.0547
[224] time=5.34, avg_loss=0.0088, train_err=0.0705
Eval: 32_h1=0.1189, 32_l2=0.0527
[159] time=8.16, avg_loss=0.0144, train_err=0.1154
[225] time=5.15, avg_loss=0.0089, train_err=0.0713
Eval: 32_h1=0.1232, 32_l2=0.0549
Eval: 32_h1=0.1189, 32_l2=0.0529
[226] time=6.04, avg_loss=0.0091, train_err=0.0725
[160] time=6.44, avg_loss=0.0136, train_err=0.1085
Eval: 32_h1=0.1188, 32_l2=0.0526
Eval: 32_h1=0.1229, 32_l2=0.0542
[227] time=5.17, avg_loss=0.0090, train_err=0.0717
Eval: 32_h1=0.1188, 32_l2=0.0524
[161] time=8.35, avg_loss=0.0130, train_err=0.1038
Eval: 32_h1=0.1229, 32_l2=0.0539
[228] time=5.25, avg_loss=0.0089, train_err=0.0713
Eval: 32_h1=0.1188, 32_l2=0.0524
[229] time=4.93, avg_loss=0.0086, train_err=0.0686
Eval: 32_h1=0.1188, 32_l2=0.0523
[162] time=8.21, avg_loss=0.0129, train_err=0.1036
Eval: 32_h1=0.1227, 32_l2=0.0539
[230] time=5.29, avg_loss=0.0084, train_err=0.0676
Eval: 32_h1=0.1188, 32_l2=0.0529
[163] time=7.69, avg_loss=0.0131, train_err=0.1047
Eval: 32_h1=0.1230, 32_l2=0.0549
[231] time=6.60, avg_loss=0.0085, train_err=0.0677
Eval: 32_h1=0.1187, 32_l2=0.0525
[164] time=8.21, avg_loss=0.0129, train_err=0.1029
Eval: 32_h1=0.1230, 32_l2=0.0542
[232] time=5.57, avg_loss=0.0085, train_err=0.0676
Eval: 32_h1=0.1189, 32_l2=0.0528
[233] time=5.51, avg_loss=0.0085, train_err=0.0678
Eval: 32_h1=0.1187, 32_l2=0.0523
[165] time=6.69, avg_loss=0.0126, train_err=0.1009
Eval: 32_h1=0.1233, 32_l2=0.0548
[234] time=6.53, avg_loss=0.0086, train_err=0.0692
Eval: 32_h1=0.1189, 32_l2=0.0525
[166] time=7.63, avg_loss=0.0131, train_err=0.1046
Eval: 32_h1=0.1230, 32_l2=0.0547
[235] time=6.44, avg_loss=0.0086, train_err=0.0689
Eval: 32_h1=0.1189, 32_l2=0.0528
[167] time=8.72, avg_loss=0.0133, train_err=0.1063
Eval: 32_h1=0.1230, 32_l2=0.0540
[236] time=5.20, avg_loss=0.0085, train_err=0.0679
Eval: 32_h1=0.1188, 32_l2=0.0527
[237] time=5.51, avg_loss=0.0083, train_err=0.0666
Eval: 32_h1=0.1188, 32_l2=0.0524
[168] time=7.40, avg_loss=0.0139, train_err=0.1113
Eval: 32_h1=0.1229, 32_l2=0.0537
[238] time=6.08, avg_loss=0.0087, train_err=0.0692
Eval: 32_h1=0.1189, 32_l2=0.0527
[169] time=8.01, avg_loss=0.0137, train_err=0.1093
Eval: 32_h1=0.1231, 32_l2=0.0543
[239] time=6.44, avg_loss=0.0094, train_err=0.0752
Eval: 32_h1=0.1189, 32_l2=0.0522
[170] time=7.99, avg_loss=0.0128, train_err=0.1024
Eval: 32_h1=0.1227, 32_l2=0.0541
[240] time=5.29, avg_loss=0.0080, train_err=0.0637
Eval: 32_h1=0.1188, 32_l2=0.0524
[241] time=5.36, avg_loss=0.0072, train_err=0.0572
Eval: 32_h1=0.1188, 32_l2=0.0524
[171] time=8.36, avg_loss=0.0127, train_err=0.1016
Eval: 32_h1=0.1227, 32_l2=0.0535
[242] time=5.63, avg_loss=0.0069, train_err=0.0551
Eval: 32_h1=0.1188, 32_l2=0.0526
[172] time=7.91, avg_loss=0.0128, train_err=0.1027
Eval: 32_h1=0.1229, 32_l2=0.0539
[243] time=5.59, avg_loss=0.0068, train_err=0.0545
Eval: 32_h1=0.1188, 32_l2=0.0524
[244] time=5.30, avg_loss=0.0068, train_err=0.0541
Eval: 32_h1=0.1188, 32_l2=0.0523
[173] time=8.88, avg_loss=0.0136, train_err=0.1089
Eval: 32_h1=0.1231, 32_l2=0.0542
[245] time=6.16, avg_loss=0.0067, train_err=0.0539
Eval: 32_h1=0.1188, 32_l2=0.0525
[174] time=8.47, avg_loss=0.0132, train_err=0.1054
Eval: 32_h1=0.1230, 32_l2=0.0547
[246] time=6.15, avg_loss=0.0067, train_err=0.0537
Eval: 32_h1=0.1189, 32_l2=0.0525
[175] time=7.83, avg_loss=0.0125, train_err=0.1003
Eval: 32_h1=0.1230, 32_l2=0.0546
[247] time=6.50, avg_loss=0.0067, train_err=0.0537
Eval: 32_h1=0.1188, 32_l2=0.0525
[248] time=5.83, avg_loss=0.0068, train_err=0.0543
Eval: 32_h1=0.1188, 32_l2=0.0525
[176] time=8.14, avg_loss=0.0127, train_err=0.1020
Eval: 32_h1=0.1228, 32_l2=0.0535
[249] time=6.17, avg_loss=0.0068, train_err=0.0543
Eval: 32_h1=0.1189, 32_l2=0.0527
[177] time=7.22, avg_loss=0.0124, train_err=0.0994
Eval: 32_h1=0.1231, 32_l2=0.0544
[250] time=6.45, avg_loss=0.0069, train_err=0.0548
Eval: 32_h1=0.1189, 32_l2=0.0526
[178] time=8.43, avg_loss=0.0126, train_err=0.1011
Eval: 32_h1=0.1231, 32_l2=0.0544
[251] time=6.17, avg_loss=0.0070, train_err=0.0560
Eval: 32_h1=0.1189, 32_l2=0.0526
[252] time=5.26, avg_loss=0.0071, train_err=0.0564
Eval: 32_h1=0.1189, 32_l2=0.0527
[179] time=8.00, avg_loss=0.0125, train_err=0.0999
Eval: 32_h1=0.1230, 32_l2=0.0545
[253] time=6.30, avg_loss=0.0070, train_err=0.0558
Eval: 32_h1=0.1189, 32_l2=0.0524
[180] time=8.41, avg_loss=0.0108, train_err=0.0865
Eval: 32_h1=0.1228, 32_l2=0.0540
[254] time=6.13, avg_loss=0.0071, train_err=0.0570
Eval: 32_h1=0.1189, 32_l2=0.0525
[181] time=7.68, avg_loss=0.0088, train_err=0.0704
Eval: 32_h1=0.1227, 32_l2=0.0538
[255] time=6.60, avg_loss=0.0071, train_err=0.0566
Eval: 32_h1=0.1190, 32_l2=0.0530
[256] time=5.50, avg_loss=0.0072, train_err=0.0573
Eval: 32_h1=0.1190, 32_l2=0.0526
[182] time=9.09, avg_loss=0.0084, train_err=0.0671
Eval: 32_h1=0.1227, 32_l2=0.0538
[257] time=6.33, avg_loss=0.0070, train_err=0.0561
Eval: 32_h1=0.1189, 32_l2=0.0524
[183] time=8.50, avg_loss=0.0083, train_err=0.0661
Eval: 32_h1=0.1227, 32_l2=0.0538
[258] time=6.70, avg_loss=0.0069, train_err=0.0551
Eval: 32_h1=0.1189, 32_l2=0.0526
[184] time=8.44, avg_loss=0.0083, train_err=0.0664
[259] time=5.69, avg_loss=0.0069, train_err=0.0552
Eval: 32_h1=0.1228, 32_l2=0.0539
Eval: 32_h1=0.1189, 32_l2=0.0525
[260] time=6.20, avg_loss=0.0070, train_err=0.0559
Eval: 32_h1=0.1189, 32_l2=0.0524
[185] time=8.64, avg_loss=0.0082, train_err=0.0652
Eval: 32_h1=0.1227, 32_l2=0.0537
[261] time=5.60, avg_loss=0.0070, train_err=0.0558
Eval: 32_h1=0.1188, 32_l2=0.0522
[262] time=4.38, avg_loss=0.0070, train_err=0.0562
[186] time=8.04, avg_loss=0.0081, train_err=0.0648
Eval: 32_h1=0.1189, 32_l2=0.0525
Eval: 32_h1=0.1227, 32_l2=0.0538
[263] time=5.39, avg_loss=0.0070, train_err=0.0559
Eval: 32_h1=0.1188, 32_l2=0.0521
[187] time=7.41, avg_loss=0.0085, train_err=0.0678
Eval: 32_h1=0.1228, 32_l2=0.0539
[264] time=5.11, avg_loss=0.0069, train_err=0.0555
Eval: 32_h1=0.1190, 32_l2=0.0526
[188] time=7.84, avg_loss=0.0085, train_err=0.0677
Eval: 32_h1=0.1229, 32_l2=0.0542
[265] time=5.81, avg_loss=0.0068, train_err=0.0542
Eval: 32_h1=0.1189, 32_l2=0.0522
[266] time=6.45, avg_loss=0.0068, train_err=0.0544
Eval: 32_h1=0.1189, 32_l2=0.0525
[189] time=8.44, avg_loss=0.0084, train_err=0.0676
Eval: 32_h1=0.1228, 32_l2=0.0537
[267] time=6.53, avg_loss=0.0069, train_err=0.0553
Eval: 32_h1=0.1189, 32_l2=0.0525
[190] time=8.05, avg_loss=0.0086, train_err=0.0689
Eval: 32_h1=0.1229, 32_l2=0.0538
[268] time=6.38, avg_loss=0.0069, train_err=0.0554
Eval: 32_h1=0.1189, 32_l2=0.0526
[191] time=8.10, avg_loss=0.0086, train_err=0.0685
Eval: 32_h1=0.1227, 32_l2=0.0538
[269] time=6.72, avg_loss=0.0069, train_err=0.0549
Eval: 32_h1=0.1190, 32_l2=0.0526
[192] time=7.09, avg_loss=0.0088, train_err=0.0702
Eval: 32_h1=0.1228, 32_l2=0.0540
[270] time=6.02, avg_loss=0.0068, train_err=0.0548
Eval: 32_h1=0.1188, 32_l2=0.0522
[271] time=5.68, avg_loss=0.0068, train_err=0.0548
Eval: 32_h1=0.1191, 32_l2=0.0528
[193] time=8.62, avg_loss=0.0090, train_err=0.0719
Eval: 32_h1=0.1229, 32_l2=0.0539
[272] time=5.68, avg_loss=0.0068, train_err=0.0544
Eval: 32_h1=0.1189, 32_l2=0.0522
[194] time=7.47, avg_loss=0.0089, train_err=0.0713
Eval: 32_h1=0.1229, 32_l2=0.0537
[273] time=6.27, avg_loss=0.0068, train_err=0.0545
Eval: 32_h1=0.1188, 32_l2=0.0521
[195] time=7.83, avg_loss=0.0087, train_err=0.0692
Eval: 32_h1=0.1230, 32_l2=0.0540
[274] time=4.93, avg_loss=0.0067, train_err=0.0535
Eval: 32_h1=0.1190, 32_l2=0.0525
[275] time=5.51, avg_loss=0.0067, train_err=0.0535
Eval: 32_h1=0.1190, 32_l2=0.0524
[196] time=8.62, avg_loss=0.0087, train_err=0.0698
Eval: 32_h1=0.1230, 32_l2=0.0544
[276] time=6.30, avg_loss=0.0067, train_err=0.0539
Eval: 32_h1=0.1190, 32_l2=0.0526
[197] time=7.67, avg_loss=0.0091, train_err=0.0729
Eval: 32_h1=0.1230, 32_l2=0.0539
[277] time=5.75, avg_loss=0.0068, train_err=0.0541
Eval: 32_h1=0.1190, 32_l2=0.0524
[198] time=7.16, avg_loss=0.0089, train_err=0.0710
Eval: 32_h1=0.1227, 32_l2=0.0536
[278] time=5.82, avg_loss=0.0067, train_err=0.0539
Eval: 32_h1=0.1190, 32_l2=0.0526
[279] time=5.65, avg_loss=0.0067, train_err=0.0535
Eval: 32_h1=0.1190, 32_l2=0.0524
[199] time=8.38, avg_loss=0.0088, train_err=0.0707
Eval: 32_h1=0.1228, 32_l2=0.0537
[280] time=6.66, avg_loss=0.0068, train_err=0.0541
Eval: 32_h1=0.1189, 32_l2=0.0522
[200] time=7.91, avg_loss=0.0088, train_err=0.0708
Eval: 32_h1=0.1228, 32_l2=0.0539
[281] time=5.71, avg_loss=0.0067, train_err=0.0540
Eval: 32_h1=0.1190, 32_l2=0.0525
[201] time=8.55, avg_loss=0.0084, train_err=0.0675
[282] time=6.08, avg_loss=0.0069, train_err=0.0553
Eval: 32_h1=0.1229, 32_l2=0.0538
Eval: 32_h1=0.1190, 32_l2=0.0525
[283] time=6.22, avg_loss=0.0068, train_err=0.0546
Eval: 32_h1=0.1190, 32_l2=0.0524
[202] time=8.33, avg_loss=0.0085, train_err=0.0677
Eval: 32_h1=0.1228, 32_l2=0.0537
[284] time=5.81, avg_loss=0.0068, train_err=0.0541
Eval: 32_h1=0.1190, 32_l2=0.0524
[203] time=8.09, avg_loss=0.0085, train_err=0.0679
Eval: 32_h1=0.1230, 32_l2=0.0541
[285] time=6.13, avg_loss=0.0067, train_err=0.0533
Eval: 32_h1=0.1190, 32_l2=0.0527
[204] time=8.49, avg_loss=0.0086, train_err=0.0691
[286] time=6.59, avg_loss=0.0067, train_err=0.0536
Eval: 32_h1=0.1229, 32_l2=0.0538
Eval: 32_h1=0.1189, 32_l2=0.0525
[287] time=6.49, avg_loss=0.0068, train_err=0.0540
Eval: 32_h1=0.1190, 32_l2=0.0524
[205] time=8.91, avg_loss=0.0084, train_err=0.0668
Eval: 32_h1=0.1229, 32_l2=0.0538
[288] time=6.62, avg_loss=0.0067, train_err=0.0539
Eval: 32_h1=0.1190, 32_l2=0.0525
[206] time=8.85, avg_loss=0.0085, train_err=0.0679
Eval: 32_h1=0.1229, 32_l2=0.0540
[289] time=5.55, avg_loss=0.0068, train_err=0.0544
Eval: 32_h1=0.1190, 32_l2=0.0524
[290] time=5.78, avg_loss=0.0067, train_err=0.0537
Eval: 32_h1=0.1190, 32_l2=0.0525
[207] time=8.32, avg_loss=0.0085, train_err=0.0677
Eval: 32_h1=0.1229, 32_l2=0.0537
[291] time=5.87, avg_loss=0.0065, train_err=0.0522
Eval: 32_h1=0.1189, 32_l2=0.0524
[208] time=7.00, avg_loss=0.0087, train_err=0.0697
Eval: 32_h1=0.1228, 32_l2=0.0537
[292] time=5.84, avg_loss=0.0064, train_err=0.0515
Eval: 32_h1=0.1190, 32_l2=0.0524
[209] time=8.08, avg_loss=0.0088, train_err=0.0707
Eval: 32_h1=0.1229, 32_l2=0.0538
[293] time=6.11, avg_loss=0.0065, train_err=0.0519
Eval: 32_h1=0.1190, 32_l2=0.0523
[294] time=5.62, avg_loss=0.0066, train_err=0.0525
Eval: 32_h1=0.1190, 32_l2=0.0525
[210] time=9.05, avg_loss=0.0086, train_err=0.0687
Eval: 32_h1=0.1229, 32_l2=0.0538
[295] time=5.78, avg_loss=0.0067, train_err=0.0534
Eval: 32_h1=0.1189, 32_l2=0.0525
[211] time=9.24, avg_loss=0.0083, train_err=0.0664
Eval: 32_h1=0.1228, 32_l2=0.0537
[296] time=6.72, avg_loss=0.0066, train_err=0.0528
Eval: 32_h1=0.1190, 32_l2=0.0523
[297] time=6.45, avg_loss=0.0066, train_err=0.0526
Eval: 32_h1=0.1190, 32_l2=0.0526
[212] time=9.55, avg_loss=0.0083, train_err=0.0668
Eval: 32_h1=0.1230, 32_l2=0.0540
[298] time=6.29, avg_loss=0.0065, train_err=0.0522
Eval: 32_h1=0.1191, 32_l2=0.0523
[213] time=8.47, avg_loss=0.0084, train_err=0.0675
Eval: 32_h1=0.1230, 32_l2=0.0542
[299] time=5.71, avg_loss=0.0066, train_err=0.0529
Eval: 32_h1=0.1190, 32_l2=0.0525
[214] time=7.74, avg_loss=0.0085, train_err=0.0680
Eval: 32_h1=0.1230, 32_l2=0.0542
[215] time=9.23, avg_loss=0.0083, train_err=0.0662
Eval: 32_h1=0.1229, 32_l2=0.0540
[216] time=6.73, avg_loss=0.0083, train_err=0.0665
Eval: 32_h1=0.1229, 32_l2=0.0538
[217] time=8.52, avg_loss=0.0084, train_err=0.0671
Eval: 32_h1=0.1230, 32_l2=0.0540
[218] time=6.80, avg_loss=0.0083, train_err=0.0665
Eval: 32_h1=0.1229, 32_l2=0.0537
[219] time=7.32, avg_loss=0.0082, train_err=0.0656
Eval: 32_h1=0.1229, 32_l2=0.0538
[220] time=7.81, avg_loss=0.0083, train_err=0.0666
Eval: 32_h1=0.1229, 32_l2=0.0537
[221] time=7.51, avg_loss=0.0085, train_err=0.0677
Eval: 32_h1=0.1229, 32_l2=0.0536
[222] time=6.09, avg_loss=0.0086, train_err=0.0689
Eval: 32_h1=0.1229, 32_l2=0.0537
[223] time=6.49, avg_loss=0.0086, train_err=0.0686
Eval: 32_h1=0.1229, 32_l2=0.0540
[224] time=6.88, avg_loss=0.0083, train_err=0.0662
Eval: 32_h1=0.1229, 32_l2=0.0539
[225] time=6.98, avg_loss=0.0086, train_err=0.0685
Eval: 32_h1=0.1231, 32_l2=0.0539
[226] time=8.35, avg_loss=0.0085, train_err=0.0682
Eval: 32_h1=0.1229, 32_l2=0.0538
[227] time=7.11, avg_loss=0.0084, train_err=0.0674
Eval: 32_h1=0.1230, 32_l2=0.0539
[228] time=6.34, avg_loss=0.0083, train_err=0.0662
Eval: 32_h1=0.1230, 32_l2=0.0537
[229] time=7.68, avg_loss=0.0083, train_err=0.0667
Eval: 32_h1=0.1230, 32_l2=0.0536
[230] time=7.55, avg_loss=0.0081, train_err=0.0650
Eval: 32_h1=0.1229, 32_l2=0.0539
[231] time=7.79, avg_loss=0.0079, train_err=0.0632
Eval: 32_h1=0.1231, 32_l2=0.0538
[232] time=7.54, avg_loss=0.0079, train_err=0.0632
Eval: 32_h1=0.1230, 32_l2=0.0538
[233] time=7.64, avg_loss=0.0079, train_err=0.0631
Eval: 32_h1=0.1230, 32_l2=0.0541
[234] time=6.43, avg_loss=0.0081, train_err=0.0649
Eval: 32_h1=0.1228, 32_l2=0.0537
[235] time=6.16, avg_loss=0.0082, train_err=0.0657
Eval: 32_h1=0.1231, 32_l2=0.0544
[236] time=7.22, avg_loss=0.0082, train_err=0.0655
Eval: 32_h1=0.1230, 32_l2=0.0540
[237] time=7.60, avg_loss=0.0086, train_err=0.0687
Eval: 32_h1=0.1230, 32_l2=0.0539
[238] time=6.50, avg_loss=0.0082, train_err=0.0658
Eval: 32_h1=0.1229, 32_l2=0.0536
[239] time=8.69, avg_loss=0.0082, train_err=0.0653
Eval: 32_h1=0.1229, 32_l2=0.0536
[240] time=7.59, avg_loss=0.0073, train_err=0.0583
Eval: 32_h1=0.1230, 32_l2=0.0540
[241] time=7.78, avg_loss=0.0065, train_err=0.0517
Eval: 32_h1=0.1229, 32_l2=0.0536
[242] time=7.84, avg_loss=0.0063, train_err=0.0501
Eval: 32_h1=0.1229, 32_l2=0.0537
[243] time=7.79, avg_loss=0.0062, train_err=0.0492
Eval: 32_h1=0.1229, 32_l2=0.0536
[244] time=7.12, avg_loss=0.0062, train_err=0.0493
Eval: 32_h1=0.1229, 32_l2=0.0537
[245] time=7.55, avg_loss=0.0061, train_err=0.0490
Eval: 32_h1=0.1229, 32_l2=0.0537
[246] time=8.67, avg_loss=0.0061, train_err=0.0487
Eval: 32_h1=0.1230, 32_l2=0.0537
[247] time=6.41, avg_loss=0.0061, train_err=0.0487
Eval: 32_h1=0.1229, 32_l2=0.0536
[248] time=6.60, avg_loss=0.0061, train_err=0.0491
Eval: 32_h1=0.1229, 32_l2=0.0536
[249] time=6.78, avg_loss=0.0062, train_err=0.0495
Eval: 32_h1=0.1229, 32_l2=0.0538
[250] time=7.79, avg_loss=0.0063, train_err=0.0503
Eval: 32_h1=0.1229, 32_l2=0.0536
[251] time=7.98, avg_loss=0.0063, train_err=0.0507
Eval: 32_h1=0.1229, 32_l2=0.0537
[252] time=9.00, avg_loss=0.0064, train_err=0.0511
Eval: 32_h1=0.1230, 32_l2=0.0539
[253] time=8.42, avg_loss=0.0065, train_err=0.0518
Eval: 32_h1=0.1229, 32_l2=0.0537
[254] time=7.78, avg_loss=0.0065, train_err=0.0524
Eval: 32_h1=0.1229, 32_l2=0.0535
[255] time=7.44, avg_loss=0.0069, train_err=0.0554
Eval: 32_h1=0.1230, 32_l2=0.0537
[256] time=7.94, avg_loss=0.0067, train_err=0.0535
Eval: 32_h1=0.1230, 32_l2=0.0539
[257] time=7.58, avg_loss=0.0065, train_err=0.0521
Eval: 32_h1=0.1230, 32_l2=0.0538
[258] time=8.97, avg_loss=0.0064, train_err=0.0512
Eval: 32_h1=0.1230, 32_l2=0.0537
[259] time=7.47, avg_loss=0.0065, train_err=0.0518
Eval: 32_h1=0.1230, 32_l2=0.0538
[260] time=7.51, avg_loss=0.0065, train_err=0.0520
Eval: 32_h1=0.1230, 32_l2=0.0537
[261] time=6.82, avg_loss=0.0063, train_err=0.0508
Eval: 32_h1=0.1229, 32_l2=0.0535
[262] time=7.54, avg_loss=0.0063, train_err=0.0505
Eval: 32_h1=0.1231, 32_l2=0.0539
[263] time=6.75, avg_loss=0.0063, train_err=0.0501
Eval: 32_h1=0.1229, 32_l2=0.0536
[264] time=6.57, avg_loss=0.0062, train_err=0.0497
Eval: 32_h1=0.1230, 32_l2=0.0538
[265] time=6.91, avg_loss=0.0063, train_err=0.0502
Eval: 32_h1=0.1230, 32_l2=0.0537
[266] time=7.70, avg_loss=0.0064, train_err=0.0508
Eval: 32_h1=0.1230, 32_l2=0.0538
[267] time=6.86, avg_loss=0.0063, train_err=0.0504
Eval: 32_h1=0.1230, 32_l2=0.0535
[268] time=8.07, avg_loss=0.0064, train_err=0.0512
Eval: 32_h1=0.1230, 32_l2=0.0537
[269] time=8.31, avg_loss=0.0064, train_err=0.0512
Eval: 32_h1=0.1230, 32_l2=0.0538
[270] time=8.39, avg_loss=0.0062, train_err=0.0498
Eval: 32_h1=0.1230, 32_l2=0.0538
[271] time=7.78, avg_loss=0.0063, train_err=0.0508
Eval: 32_h1=0.1230, 32_l2=0.0537
[272] time=7.05, avg_loss=0.0063, train_err=0.0505
Eval: 32_h1=0.1231, 32_l2=0.0538
[273] time=7.43, avg_loss=0.0064, train_err=0.0510
Eval: 32_h1=0.1230, 32_l2=0.0536
[274] time=7.42, avg_loss=0.0063, train_err=0.0502
Eval: 32_h1=0.1231, 32_l2=0.0538
[275] time=8.03, avg_loss=0.0062, train_err=0.0498
Eval: 32_h1=0.1230, 32_l2=0.0536
[276] time=6.97, avg_loss=0.0063, train_err=0.0506
Eval: 32_h1=0.1230, 32_l2=0.0536
[277] time=7.91, avg_loss=0.0062, train_err=0.0492
Eval: 32_h1=0.1231, 32_l2=0.0537
[278] time=8.17, avg_loss=0.0062, train_err=0.0495
Eval: 32_h1=0.1231, 32_l2=0.0537
[279] time=8.51, avg_loss=0.0062, train_err=0.0500
Eval: 32_h1=0.1231, 32_l2=0.0539
[280] time=7.45, avg_loss=0.0062, train_err=0.0494
Eval: 32_h1=0.1230, 32_l2=0.0536
[281] time=7.67, avg_loss=0.0062, train_err=0.0492
Eval: 32_h1=0.1231, 32_l2=0.0537
[282] time=8.33, avg_loss=0.0063, train_err=0.0505
Eval: 32_h1=0.1231, 32_l2=0.0538
[283] time=8.81, avg_loss=0.0063, train_err=0.0501
Eval: 32_h1=0.1231, 32_l2=0.0536
[284] time=8.29, avg_loss=0.0063, train_err=0.0501
Eval: 32_h1=0.1231, 32_l2=0.0537
[285] time=7.67, avg_loss=0.0062, train_err=0.0495
Eval: 32_h1=0.1230, 32_l2=0.0535
[286] time=6.56, avg_loss=0.0061, train_err=0.0488
Eval: 32_h1=0.1231, 32_l2=0.0538
[287] time=8.15, avg_loss=0.0060, train_err=0.0482
Eval: 32_h1=0.1230, 32_l2=0.0537
[288] time=6.30, avg_loss=0.0060, train_err=0.0480
Eval: 32_h1=0.1231, 32_l2=0.0539
[289] time=7.18, avg_loss=0.0062, train_err=0.0499
Eval: 32_h1=0.1231, 32_l2=0.0538
[290] time=7.81, avg_loss=0.0062, train_err=0.0498
Eval: 32_h1=0.1231, 32_l2=0.0537
[291] time=8.25, avg_loss=0.0061, train_err=0.0492
Eval: 32_h1=0.1231, 32_l2=0.0537
[292] time=8.66, avg_loss=0.0061, train_err=0.0487
Eval: 32_h1=0.1231, 32_l2=0.0537
[293] time=8.04, avg_loss=0.0060, train_err=0.0481
Eval: 32_h1=0.1231, 32_l2=0.0537
[294] time=7.69, avg_loss=0.0060, train_err=0.0477
Eval: 32_h1=0.1230, 32_l2=0.0536
[295] time=6.85, avg_loss=0.0060, train_err=0.0479
Eval: 32_h1=0.1231, 32_l2=0.0536
[296] time=7.25, avg_loss=0.0060, train_err=0.0478
Eval: 32_h1=0.1231, 32_l2=0.0537
[297] time=7.58, avg_loss=0.0061, train_err=0.0488
Eval: 32_h1=0.1231, 32_l2=0.0538
[298] time=6.71, avg_loss=0.0060, train_err=0.0477
Eval: 32_h1=0.1232, 32_l2=0.0537
[299] time=7.83, avg_loss=0.0062, train_err=0.0493
Eval: 32_h1=0.1230, 32_l2=0.0537
logfile: /home/leeshu/wmm/neuraloperator-main/logs/darcy/20260128_131957__L8FNO421.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'arch': 'fno', 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_small2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [64, 64], 'hidden_channels': 64, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 2, 'n_layers': 8, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 0, 'hc_dynamic': False}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.005, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 60, 'gamma': 0.5, 'n_epochs': 300}, 'data': {'_config_name': 'darcydatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data/Darcy_pt', 'batch_size': 8, 'n_train': 1000, 'train_resolution': 421, 'n_tests': [224], 'test_resolutions': [421], 'test_batch_sizes': [8], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'darcy32_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 421 with 224 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-7): 8 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([64, 64, 64, 33]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-7): 8 x Flattened1dConv(
        (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-7): 8 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
          (1): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-7): 8 x SoftGating()
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
      (1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
      (1): Conv1d(128, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7f15772f18e0>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7f15772c5760>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7f15772c5760>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7f15772c5730>}

### Beginning Training...


n_params: 138496577
Training on 800 samples
Testing on [224] samples         on resolutions [421].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([8, 1, 421, 421])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
logfile: /home/leeshu/wmm/neuraloperator-main/logs/navier_stokes/20260128_132012__L8FNOT50.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_medium2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [64, 64], 'hidden_channels': 64, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 4, 'n_layers': 8, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 0, 'hc_dynamic': True}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.0003, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 100, 'gamma': 0.5, 'n_epochs': 600}, 'data': {'_config_name': 'navierstokesdatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data/V1e-3_N5000_T50', 'batch_size': 24, 'n_train': 10000, 'train_resolution': 128, 'n_tests': [2000], 'test_resolutions': [128], 'test_batch_sizes': [24], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'ns128_hc_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 128 with 2000 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-7): 8 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([64, 64, 64, 33]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-7): 8 x Flattened1dConv(
        (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-7): 8 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
          (1): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-7): 8 x SoftGating()
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
      (1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
      (1): Conv1d(256, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.0003
    lr: 0.0003
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7f26fff5c4f0>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7f26fd9fe400>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7f26fd9fe400>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7f26fd9fe3d0>}

### Beginning Training...


n_params: 138505025
Training on 10000 samples
Testing on [2000] samples         on resolutions [128].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([24, 1, 128, 128])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[0] time=78.09, avg_loss=0.4010, train_err=3.2081
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
Eval: 421_h1=0.1438, 421_l2=0.0847
logfile: /home/leeshu/wmm/neuraloperator-main/logs/navier_stokes/20260128_132137__L8HC2T50.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_medium2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [64, 64], 'hidden_channels': 64, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 4, 'n_layers': 8, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 2, 'hc_dynamic': True}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.0003, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 100, 'gamma': 0.5, 'n_epochs': 600}, 'data': {'_config_name': 'navierstokesdatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data/V1e-3_N5000_T50', 'batch_size': 24, 'n_train': 10000, 'train_resolution': 128, 'n_tests': [2000], 'test_resolutions': [128], 'test_batch_sizes': [24], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'ns128_hc_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 128 with 2000 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-7): 8 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([64, 64, 64, 33]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-7): 8 x Flattened1dConv(
        (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-7): 8 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
          (1): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-7): 8 x SoftGating()
    )
  )
  (hc_layers): ModuleList(
    (0-7): 8 x HyperConnection(
      (layer_norm): GroupNorm(1, 64, eps=1e-05, affine=True)
      (dynamic_alpha_fn): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (dynamic_beta_fn): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
      (1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
      (1): Conv1d(256, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.0003
    lr: 0.0003
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7f78ac9cc820>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7f78ac9cc9d0>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7f78ac9cc9d0>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7f78ac9cca00>}

### Beginning Training...


n_params: 138510225
Training on 10000 samples
Testing on [2000] samples         on resolutions [128].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([24, 1, 128, 128])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[0] time=107.49, avg_loss=0.2036, train_err=4.8823
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
Eval: 128_h1=0.0970, 128_l2=0.0775
[1] time=75.21, avg_loss=0.1383, train_err=1.1062
Eval: 421_h1=0.1081, 421_l2=0.0749
logfile: /home/leeshu/wmm/neuraloperator-main/logs/darcy/20260128_132444__L8FNO421.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'arch': 'fno', 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_small2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [32, 32], 'hidden_channels': 32, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 2, 'n_layers': 8, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 0, 'hc_dynamic': False}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.005, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 60, 'gamma': 0.5, 'n_epochs': 300}, 'data': {'_config_name': 'darcydatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data/Darcy_pt', 'batch_size': 8, 'n_train': 1000, 'train_resolution': 421, 'n_tests': [224], 'test_resolutions': [421], 'test_batch_sizes': [8], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'darcy32_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 421 with 224 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-7): 8 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([32, 32, 32, 17]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-7): 8 x Flattened1dConv(
        (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-7): 8 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))
          (1): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-7): 8 x SoftGating()
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
      (1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
      (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7fe5e0f158e0>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fe5e0eea760>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fe5e0eea760>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fe5e0eea730>}

### Beginning Training...


n_params: 8934689
Training on 800 samples
Testing on [224] samples         on resolutions [421].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([8, 1, 421, 421])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[0] time=36.86, avg_loss=0.3977, train_err=3.1817
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
Eval: 421_h1=0.1851, 421_l2=0.0892
logfile: /home/leeshu/wmm/neuraloperator-main/logs/darcy/20260128_132546__L8HC2_421.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'arch': 'fno', 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_small2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [32, 32], 'hidden_channels': 32, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 2, 'n_layers': 8, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 0, 'hc_dynamic': False}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.005, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 60, 'gamma': 0.5, 'n_epochs': 300}, 'data': {'_config_name': 'darcydatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data/Darcy_pt', 'batch_size': 8, 'n_train': 1000, 'train_resolution': 421, 'n_tests': [224], 'test_resolutions': [421], 'test_batch_sizes': [8], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'darcy32_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 421 with 224 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-7): 8 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([32, 32, 32, 17]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-7): 8 x Flattened1dConv(
        (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-7): 8 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))
          (1): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-7): 8 x SoftGating()
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
      (1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
      (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7f15a767b8e0>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7f15a764f760>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7f15a764f760>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7f15a764f730>}

### Beginning Training...


n_params: 8934689
Training on 800 samples
Testing on [224] samples         on resolutions [421].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([8, 1, 421, 421])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[1] time=51.93, avg_loss=0.1478, train_err=1.1824
Eval: 421_h1=0.1382, 421_l2=0.0834
[1] time=256.63, avg_loss=0.0810, train_err=1.9433
Eval: 128_h1=0.0693, 128_l2=0.0528
[0] time=84.72, avg_loss=0.4110, train_err=3.2884
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
Eval: 421_h1=0.1644, 421_l2=0.0883
logfile: /home/leeshu/wmm/neuraloperator-main/logs/navier_stokes/20260128_132738__L8FNOT50.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_medium2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [64, 64], 'hidden_channels': 64, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 4, 'n_layers': 8, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 0, 'hc_dynamic': False}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.0003, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 100, 'gamma': 0.5, 'n_epochs': 600}, 'data': {'_config_name': 'navierstokesdatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data/V1e-3_N5000_T50', 'batch_size': 24, 'n_train': 10000, 'train_resolution': 128, 'n_tests': [2000], 'test_resolutions': [128], 'test_batch_sizes': [24], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'ns128_hc_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 128 with 2000 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-7): 8 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([64, 64, 64, 33]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-7): 8 x Flattened1dConv(
        (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-7): 8 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
          (1): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-7): 8 x SoftGating()
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
      (1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
      (1): Conv1d(256, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.0003
    lr: 0.0003
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7f4a29b5f4f0>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7f4a29616400>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7f4a29616400>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7f4a296163d0>}

### Beginning Training...


n_params: 138505025
Training on 10000 samples
Testing on [2000] samples         on resolutions [128].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([24, 1, 128, 128])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[2] time=81.73, avg_loss=0.1218, train_err=0.9742
logfile: /home/leeshu/wmm/neuraloperator-main/logs/navier_stokes/20260128_132800__L8HC2T50.log
Eval: 421_h1=0.1276, 421_l2=0.0817
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_medium2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [64, 64], 'hidden_channels': 64, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 4, 'n_layers': 8, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 2, 'hc_dynamic': True}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.0003, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 100, 'gamma': 0.5, 'n_epochs': 600}, 'data': {'_config_name': 'navierstokesdatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data/V1e-3_N5000_T50', 'batch_size': 24, 'n_train': 10000, 'train_resolution': 128, 'n_tests': [2000], 'test_resolutions': [128], 'test_batch_sizes': [24], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'ns128_hc_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 128 with 2000 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-7): 8 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([64, 64, 64, 33]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-7): 8 x Flattened1dConv(
        (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-7): 8 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
          (1): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-7): 8 x SoftGating()
    )
  )
  (hc_layers): ModuleList(
    (0-7): 8 x HyperConnection(
      (layer_norm): GroupNorm(1, 64, eps=1e-05, affine=True)
      (dynamic_alpha_fn): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (dynamic_beta_fn): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
      (1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
      (1): Conv1d(256, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.0003
    lr: 0.0003
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7f0b321267c0>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7f0b32126970>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7f0b32126970>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7f0b321269a0>}

### Beginning Training...


n_params: 138510225
Training on 10000 samples
Testing on [2000] samples         on resolutions [128].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([24, 1, 128, 128])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[1] time=81.13, avg_loss=0.1535, train_err=1.2280
Eval: 421_h1=0.1436, 421_l2=0.0837
[3] time=75.49, avg_loss=0.1157, train_err=0.9259
Eval: 421_h1=0.1222, 421_l2=0.0843
logfile: /home/leeshu/wmm/neuraloperator-main/logs/darcy/20260128_132933__L8HC2_421.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'arch': 'fno', 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_small2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [32, 32], 'hidden_channels': 32, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 2, 'n_layers': 8, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 2, 'hc_dynamic': True}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.005, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 60, 'gamma': 0.5, 'n_epochs': 300}, 'data': {'_config_name': 'darcydatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data/Darcy_pt', 'batch_size': 8, 'n_train': 1000, 'train_resolution': 421, 'n_tests': [224], 'test_resolutions': [421], 'test_batch_sizes': [8], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'darcy32_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 421 with 224 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-7): 8 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([32, 32, 32, 17]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-7): 8 x Flattened1dConv(
        (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-7): 8 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))
          (1): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-7): 8 x SoftGating()
    )
  )
  (hc_layers): ModuleList(
    (0-7): 8 x HyperConnection(
      (layer_norm): GroupNorm(1, 32, eps=1e-05, affine=True)
      (dynamic_alpha_fn): Conv2d(32, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (dynamic_beta_fn): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
      (1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
      (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7f5b8ed887f0>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7f5b8ed88d90>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7f5b8ed88d90>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7f5b8ed88dc0>}

### Beginning Training...


n_params: 8937329
Training on 800 samples
Testing on [224] samples         on resolutions [421].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([8, 1, 421, 421])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[4] time=57.54, avg_loss=0.1005, train_err=0.8041
Eval: 421_h1=0.0918, 421_l2=0.0629
[0] time=210.74, avg_loss=0.2015, train_err=4.8328
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
Eval: 128_h1=0.1003, 128_l2=0.0697
[5] time=82.12, avg_loss=0.0919, train_err=0.7350
Eval: 421_h1=0.1068, 421_l2=0.0684
[6] time=82.12, avg_loss=0.0846, train_err=0.6767
Eval: 421_h1=0.0816, 421_l2=0.0582
[0] time=308.22, avg_loss=0.3154, train_err=2.5231
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
[7] time=81.87, avg_loss=0.0844, train_err=0.6756
Eval: 421_h1=0.0838, 421_l2=0.0586
Eval: 421_h1=0.1892, 421_l2=0.0977
[1] time=265.04, avg_loss=0.0819, train_err=1.9649
Eval: 128_h1=0.0690, 128_l2=0.0516
[0] time=488.58, avg_loss=0.1230, train_err=2.9490
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
[8] time=81.87, avg_loss=0.0769, train_err=0.6155
Eval: 421_h1=0.0781, 421_l2=0.0537
Eval: 128_h1=0.0794, 128_l2=0.0544
[9] time=82.10, avg_loss=0.0744, train_err=0.5950
Eval: 421_h1=0.0749, 421_l2=0.0514
[10] time=82.15, avg_loss=0.0752, train_err=0.6013
Eval: 421_h1=0.0858, 421_l2=0.0591
[1] time=304.63, avg_loss=0.1541, train_err=1.2328
[2] time=249.57, avg_loss=0.0620, train_err=1.4876
Eval: 128_h1=0.0564, 128_l2=0.0419
Eval: 421_h1=0.1426, 421_l2=0.0769
[11] time=81.60, avg_loss=0.0719, train_err=0.5751
Eval: 421_h1=0.0900, 421_l2=0.0616
[12] time=82.09, avg_loss=0.0677, train_err=0.5413
Eval: 421_h1=0.0704, 421_l2=0.0470
[13] time=82.09, avg_loss=0.0701, train_err=0.5605
Eval: 421_h1=0.0870, 421_l2=0.0522
[1] time=470.42, avg_loss=0.0698, train_err=1.6731
[3] time=254.58, avg_loss=0.0516, train_err=1.2377
Eval: 128_h1=0.0612, 128_l2=0.0494
Eval: 128_h1=0.0493, 128_l2=0.0373
[14] time=82.12, avg_loss=0.0671, train_err=0.5368
Eval: 421_h1=0.0727, 421_l2=0.0446
[2] time=304.22, avg_loss=0.1283, train_err=1.0263
Eval: 421_h1=0.1282, 421_l2=0.0669
[15] time=81.50, avg_loss=0.0669, train_err=0.5348
Eval: 421_h1=0.0929, 421_l2=0.0534
[16] time=82.12, avg_loss=0.0699, train_err=0.5592
Eval: 421_h1=0.0687, 421_l2=0.0373
[4] time=255.81, avg_loss=0.0450, train_err=1.0796
Eval: 128_h1=0.0446, 128_l2=0.0348
[17] time=82.13, avg_loss=0.0612, train_err=0.4898
Eval: 421_h1=0.0747, 421_l2=0.0438
[18] time=82.14, avg_loss=0.0613, train_err=0.4904
[3] time=304.33, avg_loss=0.1152, train_err=0.9213
Eval: 421_h1=0.0704, 421_l2=0.0355
Eval: 421_h1=0.1100, 421_l2=0.0649
[19] time=81.80, avg_loss=0.0594, train_err=0.4755
[2] time=478.51, avg_loss=0.0583, train_err=1.3972
Eval: 421_h1=0.0666, 421_l2=0.0285
Eval: 128_h1=0.0545, 128_l2=0.0476
[5] time=256.62, avg_loss=0.0402, train_err=0.9649
Eval: 128_h1=0.0409, 128_l2=0.0328
[20] time=82.16, avg_loss=0.0589, train_err=0.4710
Eval: 421_h1=0.0668, 421_l2=0.0270
[21] time=82.10, avg_loss=0.0606, train_err=0.4847
Eval: 421_h1=0.0688, 421_l2=0.0303
[4] time=304.77, avg_loss=0.1073, train_err=0.8587
[22] time=81.87, avg_loss=0.0592, train_err=0.4737
Eval: 421_h1=0.0979, 421_l2=0.0523
Eval: 421_h1=0.0700, 421_l2=0.0337
[6] time=261.37, avg_loss=0.0366, train_err=0.8786
Eval: 128_h1=0.0382, 128_l2=0.0312
[23] time=82.12, avg_loss=0.0678, train_err=0.5421
Eval: 421_h1=0.0733, 421_l2=0.0284
[24] time=82.13, avg_loss=0.0564, train_err=0.4511
Eval: 421_h1=0.0670, 421_l2=0.0223
[3] time=480.65, avg_loss=0.0515, train_err=1.2345
Eval: 128_h1=0.0476, 128_l2=0.0427
[25] time=82.14, avg_loss=0.0600, train_err=0.4797
Eval: 421_h1=0.0662, 421_l2=0.0233
[5] time=304.46, avg_loss=0.0981, train_err=0.7849
[7] time=248.99, avg_loss=0.0338, train_err=0.8108
Eval: 421_h1=0.1041, 421_l2=0.0547
Eval: 128_h1=0.0362, 128_l2=0.0299
[26] time=81.51, avg_loss=0.0569, train_err=0.4553
Eval: 421_h1=0.0709, 421_l2=0.0293
[27] time=82.15, avg_loss=0.0538, train_err=0.4302
Eval: 421_h1=0.0660, 421_l2=0.0204
[28] time=82.10, avg_loss=0.0516, train_err=0.4127
Eval: 421_h1=0.0678, 421_l2=0.0226
[8] time=256.20, avg_loss=0.0315, train_err=0.7549
Eval: 128_h1=0.0343, 128_l2=0.0286
[29] time=82.13, avg_loss=0.0548, train_err=0.4380
[6] time=304.46, avg_loss=0.0915, train_err=0.7319
Eval: 421_h1=0.0665, 421_l2=0.0211
Eval: 421_h1=0.0943, 421_l2=0.0448
[30] time=81.83, avg_loss=0.0517, train_err=0.4137
Eval: 421_h1=0.0677, 421_l2=0.0196
[4] time=481.45, avg_loss=0.0467, train_err=1.1197
Eval: 128_h1=0.0439, 128_l2=0.0405
[31] time=82.14, avg_loss=0.0507, train_err=0.4054
Eval: 421_h1=0.0692, 421_l2=0.0214
[9] time=261.93, avg_loss=0.0295, train_err=0.7074
Eval: 128_h1=0.0323, 128_l2=0.0268
[32] time=82.14, avg_loss=0.0540, train_err=0.4317
Eval: 421_h1=0.0705, 421_l2=0.0225
[7] time=304.67, avg_loss=0.0866, train_err=0.6926
[33] time=81.88, avg_loss=0.0530, train_err=0.4239
Eval: 421_h1=0.0735, 421_l2=0.0341
Eval: 421_h1=0.0876, 421_l2=0.0429
[34] time=81.87, avg_loss=0.0517, train_err=0.4138
Eval: 421_h1=0.0684, 421_l2=0.0260
[10] time=253.53, avg_loss=0.0278, train_err=0.6672
Eval: 128_h1=0.0302, 128_l2=0.0247
[35] time=82.16, avg_loss=0.0540, train_err=0.4317
Eval: 421_h1=0.0660, 421_l2=0.0171
[5] time=476.28, avg_loss=0.0430, train_err=1.0316
[36] time=82.15, avg_loss=0.0531, train_err=0.4245
Eval: 421_h1=0.0715, 421_l2=0.0258
Eval: 128_h1=0.0416, 128_l2=0.0393
[8] time=304.77, avg_loss=0.0847, train_err=0.6779
Eval: 421_h1=0.1077, 421_l2=0.0640
[37] time=81.53, avg_loss=0.0490, train_err=0.3919
Eval: 421_h1=0.0667, 421_l2=0.0176
[11] time=252.39, avg_loss=0.0265, train_err=0.6353
Eval: 128_h1=0.0292, 128_l2=0.0243
[38] time=82.12, avg_loss=0.0483, train_err=0.3865
Eval: 421_h1=0.0676, 421_l2=0.0178
[39] time=82.18, avg_loss=0.0493, train_err=0.3942
Eval: 421_h1=0.0711, 421_l2=0.0265
[40] time=82.08, avg_loss=0.0517, train_err=0.4137
Eval: 421_h1=0.0692, 421_l2=0.0220
[9] time=304.01, avg_loss=0.0866, train_err=0.6926
Eval: 421_h1=0.0905, 421_l2=0.0438
[12] time=257.35, avg_loss=0.0257, train_err=0.6152
Eval: 128_h1=0.0265, 128_l2=0.0231
[41] time=81.55, avg_loss=0.0491, train_err=0.3930
Eval: 421_h1=0.0696, 421_l2=0.0246
[6] time=478.51, avg_loss=0.0398, train_err=0.9548
Eval: 128_h1=0.0409, 128_l2=0.0394
[42] time=82.14, avg_loss=0.0485, train_err=0.3881
Eval: 421_h1=0.0745, 421_l2=0.0294
[43] time=82.06, avg_loss=0.0484, train_err=0.3869
Eval: 421_h1=0.0693, 421_l2=0.0209
[13] time=253.38, avg_loss=0.0244, train_err=0.5850
Eval: 128_h1=0.0248, 128_l2=0.0215
[10] time=304.81, avg_loss=0.0752, train_err=0.6014
[44] time=81.81, avg_loss=0.0489, train_err=0.3915
Eval: 421_h1=0.0908, 421_l2=0.0363
Eval: 421_h1=0.0863, 421_l2=0.0415
[45] time=81.82, avg_loss=0.0518, train_err=0.4141
Eval: 421_h1=0.0690, 421_l2=0.0178
[46] time=82.10, avg_loss=0.0465, train_err=0.3718
Eval: 421_h1=0.0661, 421_l2=0.0144
[14] time=265.05, avg_loss=0.0235, train_err=0.5629
Eval: 128_h1=0.0249, 128_l2=0.0201
[47] time=82.15, avg_loss=0.0444, train_err=0.3554
Eval: 421_h1=0.0654, 421_l2=0.0154
[7] time=490.91, avg_loss=0.0375, train_err=0.9005
Eval: 128_h1=0.0397, 128_l2=0.0383
[11] time=304.71, avg_loss=0.0764, train_err=0.6108
Eval: 421_h1=0.0883, 421_l2=0.0398
[48] time=81.59, avg_loss=0.0448, train_err=0.3588
Eval: 421_h1=0.0659, 421_l2=0.0164
[49] time=82.09, avg_loss=0.0469, train_err=0.3750
Eval: 421_h1=0.0681, 421_l2=0.0165
[15] time=250.52, avg_loss=0.0225, train_err=0.5402
Eval: 128_h1=0.0252, 128_l2=0.0203
[50] time=82.12, avg_loss=0.0484, train_err=0.3875
Eval: 421_h1=0.0753, 421_l2=0.0314
[51] time=82.09, avg_loss=0.0492, train_err=0.3935
Eval: 421_h1=0.0776, 421_l2=0.0303
[12] time=304.14, avg_loss=0.0718, train_err=0.5744
Eval: 421_h1=0.0794, 421_l2=0.0348
[52] time=81.49, avg_loss=0.0460, train_err=0.3680
Eval: 421_h1=0.0675, 421_l2=0.0179
[16] time=256.05, avg_loss=0.0214, train_err=0.5138
[8] time=472.38, avg_loss=0.0357, train_err=0.8551
Eval: 128_h1=0.0225, 128_l2=0.0186
Eval: 128_h1=0.0381, 128_l2=0.0367
[53] time=82.13, avg_loss=0.0431, train_err=0.3448
Eval: 421_h1=0.0653, 421_l2=0.0137
[54] time=82.10, avg_loss=0.0460, train_err=0.3678
Eval: 421_h1=0.0672, 421_l2=0.0166
[13] time=304.76, avg_loss=0.0706, train_err=0.5649
[55] time=81.81, avg_loss=0.0441, train_err=0.3525
Eval: 421_h1=0.0721, 421_l2=0.0220
Eval: 421_h1=0.0798, 421_l2=0.0336
[17] time=262.83, avg_loss=0.0205, train_err=0.4924
Eval: 128_h1=0.0219, 128_l2=0.0184
[56] time=81.85, avg_loss=0.0462, train_err=0.3694
Eval: 421_h1=0.0671, 421_l2=0.0143
[57] time=82.19, avg_loss=0.0427, train_err=0.3414
Eval: 421_h1=0.0667, 421_l2=0.0158
[58] time=82.12, avg_loss=0.0436, train_err=0.3487
Eval: 421_h1=0.0668, 421_l2=0.0161
[9] time=488.80, avg_loss=0.0341, train_err=0.8169
[14] time=304.59, avg_loss=0.0701, train_err=0.5607
Eval: 128_h1=0.0368, 128_l2=0.0354
[18] time=253.17, avg_loss=0.0199, train_err=0.4770
Eval: 128_h1=0.0215, 128_l2=0.0180
Eval: 421_h1=0.0823, 421_l2=0.0368
[59] time=81.52, avg_loss=0.0477, train_err=0.3816
Eval: 421_h1=0.0719, 421_l2=0.0251
[60] time=82.15, avg_loss=0.0390, train_err=0.3116
Eval: 421_h1=0.0645, 421_l2=0.0132
[61] time=82.12, avg_loss=0.0353, train_err=0.2826
Eval: 421_h1=0.0643, 421_l2=0.0112
[19] time=251.22, avg_loss=0.0193, train_err=0.4634
Eval: 128_h1=0.0210, 128_l2=0.0176
[62] time=82.11, avg_loss=0.0351, train_err=0.2810
Eval: 421_h1=0.0651, 421_l2=0.0149
[15] time=304.14, avg_loss=0.0681, train_err=0.5450
Eval: 421_h1=0.0766, 421_l2=0.0287
[63] time=81.52, avg_loss=0.0356, train_err=0.2844
Eval: 421_h1=0.0663, 421_l2=0.0138
[10] time=468.72, avg_loss=0.0329, train_err=0.7887
[64] time=82.08, avg_loss=0.0349, train_err=0.2789
Eval: 421_h1=0.0645, 421_l2=0.0113
Eval: 128_h1=0.0348, 128_l2=0.0332
[20] time=255.49, avg_loss=0.0189, train_err=0.4524
Eval: 128_h1=0.0211, 128_l2=0.0172
[65] time=82.10, avg_loss=0.0348, train_err=0.2787
Eval: 421_h1=0.0647, 421_l2=0.0112
[16] time=304.75, avg_loss=0.0694, train_err=0.5555
[66] time=81.87, avg_loss=0.0345, train_err=0.2758
Eval: 421_h1=0.0650, 421_l2=0.0111
Eval: 421_h1=0.0859, 421_l2=0.0383
[67] time=81.84, avg_loss=0.0340, train_err=0.2718
Eval: 421_h1=0.0647, 421_l2=0.0105
[21] time=259.71, avg_loss=0.0188, train_err=0.4519
Eval: 128_h1=0.0283, 128_l2=0.0234
[68] time=82.08, avg_loss=0.0343, train_err=0.2746
Eval: 421_h1=0.0650, 421_l2=0.0118
[69] time=82.11, avg_loss=0.0345, train_err=0.2760
Eval: 421_h1=0.0657, 421_l2=0.0113
[11] time=482.13, avg_loss=0.0314, train_err=0.7521
[17] time=304.68, avg_loss=0.0651, train_err=0.5212
Eval: 128_h1=0.0337, 128_l2=0.0318
[70] time=81.84, avg_loss=0.0343, train_err=0.2741
Eval: 421_h1=0.0775, 421_l2=0.0273
Eval: 421_h1=0.0647, 421_l2=0.0125
[22] time=249.84, avg_loss=0.0198, train_err=0.4742
Eval: 128_h1=0.0192, 128_l2=0.0158
[71] time=82.14, avg_loss=0.0345, train_err=0.2756
Eval: 421_h1=0.0656, 421_l2=0.0137
[72] time=82.14, avg_loss=0.0342, train_err=0.2732
Eval: 421_h1=0.0656, 421_l2=0.0134
[73] time=82.12, avg_loss=0.0341, train_err=0.2732
Eval: 421_h1=0.0658, 421_l2=0.0132
[18] time=304.43, avg_loss=0.0607, train_err=0.4858
[23] time=253.46, avg_loss=0.0179, train_err=0.4302
Eval: 421_h1=0.0725, 421_l2=0.0271
Eval: 128_h1=0.0190, 128_l2=0.0157
[74] time=81.57, avg_loss=0.0347, train_err=0.2774
Eval: 421_h1=0.0669, 421_l2=0.0161
[75] time=82.08, avg_loss=0.0363, train_err=0.2907
[12] time=473.43, avg_loss=0.0298, train_err=0.7148
Eval: 421_h1=0.0662, 421_l2=0.0119
Eval: 128_h1=0.0318, 128_l2=0.0291
[76] time=82.14, avg_loss=0.0342, train_err=0.2736
Eval: 421_h1=0.0655, 421_l2=0.0115
[24] time=255.25, avg_loss=0.0172, train_err=0.4121
Eval: 128_h1=0.0208, 128_l2=0.0170
[77] time=82.10, avg_loss=0.0342, train_err=0.2738
[19] time=304.44, avg_loss=0.0681, train_err=0.5451
Eval: 421_h1=0.0655, 421_l2=0.0113
Eval: 421_h1=0.0817, 421_l2=0.0356
[78] time=81.80, avg_loss=0.0342, train_err=0.2733
Eval: 421_h1=0.0660, 421_l2=0.0114
[79] time=82.11, avg_loss=0.0352, train_err=0.2817
Eval: 421_h1=0.0673, 421_l2=0.0131
[25] time=263.82, avg_loss=0.0170, train_err=0.4073
Eval: 128_h1=0.0206, 128_l2=0.0168
[80] time=82.10, avg_loss=0.0348, train_err=0.2784
Eval: 421_h1=0.0660, 421_l2=0.0149
[13] time=492.46, avg_loss=0.0286, train_err=0.6862
[20] time=304.71, avg_loss=0.0586, train_err=0.4689
[81] time=81.81, avg_loss=0.0335, train_err=0.2678
Eval: 128_h1=0.0308, 128_l2=0.0277
Eval: 421_h1=0.0739, 421_l2=0.0288
Eval: 421_h1=0.0658, 421_l2=0.0118
[82] time=82.14, avg_loss=0.0333, train_err=0.2664
Eval: 421_h1=0.0665, 421_l2=0.0132
[26] time=251.25, avg_loss=0.0169, train_err=0.4061
Eval: 128_h1=0.0193, 128_l2=0.0158
[83] time=82.14, avg_loss=0.0338, train_err=0.2702
Eval: 421_h1=0.0664, 421_l2=0.0123
[84] time=82.12, avg_loss=0.0341, train_err=0.2729
Eval: 421_h1=0.0679, 421_l2=0.0150
[21] time=304.38, avg_loss=0.0617, train_err=0.4935
Eval: 421_h1=0.0806, 421_l2=0.0322
[85] time=81.51, avg_loss=0.0344, train_err=0.2753
Eval: 421_h1=0.0679, 421_l2=0.0155
[27] time=251.29, avg_loss=0.0165, train_err=0.3967
Eval: 128_h1=0.0187, 128_l2=0.0154
[86] time=82.09, avg_loss=0.0346, train_err=0.2766
Eval: 421_h1=0.0663, 421_l2=0.0107
[14] time=464.19, avg_loss=0.0274, train_err=0.6559
Eval: 128_h1=0.0289, 128_l2=0.0263
[87] time=82.09, avg_loss=0.0343, train_err=0.2747
Eval: 421_h1=0.0670, 421_l2=0.0124
[88] time=82.12, avg_loss=0.0331, train_err=0.2646
Eval: 421_h1=0.0663, 421_l2=0.0111
[22] time=304.18, avg_loss=0.0591, train_err=0.4727
Eval: 421_h1=0.0753, 421_l2=0.0269
[28] time=256.29, avg_loss=0.0163, train_err=0.3916
Eval: 128_h1=0.0176, 128_l2=0.0145
[89] time=81.52, avg_loss=0.0327, train_err=0.2615
Eval: 421_h1=0.0660, 421_l2=0.0107
[90] time=82.10, avg_loss=0.0349, train_err=0.2793
Eval: 421_h1=0.0688, 421_l2=0.0173
[91] time=82.10, avg_loss=0.0336, train_err=0.2689
Eval: 421_h1=0.0669, 421_l2=0.0141
[29] time=261.22, avg_loss=0.0163, train_err=0.3913
Eval: 128_h1=0.0189, 128_l2=0.0160
[23] time=304.68, avg_loss=0.0603, train_err=0.4826
[15] time=490.60, avg_loss=0.0264, train_err=0.6335
[92] time=81.82, avg_loss=0.0325, train_err=0.2599
Eval: 421_h1=0.0660, 421_l2=0.0111
Eval: 421_h1=0.0734, 421_l2=0.0231
Eval: 128_h1=0.0284, 128_l2=0.0253
[93] time=81.83, avg_loss=0.0325, train_err=0.2601
Eval: 421_h1=0.0668, 421_l2=0.0112
[94] time=82.15, avg_loss=0.0323, train_err=0.2585
Eval: 421_h1=0.0667, 421_l2=0.0139
[30] time=250.57, avg_loss=0.0165, train_err=0.3954
Eval: 128_h1=0.0216, 128_l2=0.0191
[95] time=82.15, avg_loss=0.0329, train_err=0.2633
Eval: 421_h1=0.0669, 421_l2=0.0121
[24] time=304.71, avg_loss=0.0647, train_err=0.5175
Eval: 421_h1=0.0796, 421_l2=0.0347
[96] time=81.53, avg_loss=0.0329, train_err=0.2628
Eval: 421_h1=0.0659, 421_l2=0.0108
[97] time=82.10, avg_loss=0.0335, train_err=0.2681
Eval: 421_h1=0.0700, 421_l2=0.0193
[16] time=467.79, avg_loss=0.0257, train_err=0.6156
[31] time=252.13, avg_loss=0.0151, train_err=0.3612
Eval: 128_h1=0.0276, 128_l2=0.0243
Eval: 128_h1=0.0162, 128_l2=0.0135
[98] time=82.12, avg_loss=0.0329, train_err=0.2636
Eval: 421_h1=0.0665, 421_l2=0.0116
[99] time=82.13, avg_loss=0.0316, train_err=0.2531
Eval: 421_h1=0.0667, 421_l2=0.0108
[25] time=304.08, avg_loss=0.0565, train_err=0.4521
Eval: 421_h1=0.0713, 421_l2=0.0233
[100] time=81.55, avg_loss=0.0313, train_err=0.2505
Eval: 421_h1=0.0670, 421_l2=0.0139
[32] time=262.78, avg_loss=0.0148, train_err=0.3541
Eval: 128_h1=0.0160, 128_l2=0.0133
[101] time=82.08, avg_loss=0.0317, train_err=0.2538
Eval: 421_h1=0.0680, 421_l2=0.0143
[102] time=82.13, avg_loss=0.0328, train_err=0.2622
Eval: 421_h1=0.0672, 421_l2=0.0139
[26] time=304.69, avg_loss=0.0566, train_err=0.4527
[103] time=81.84, avg_loss=0.0319, train_err=0.2555
[17] time=487.45, avg_loss=0.0251, train_err=0.6029
Eval: 421_h1=0.0675, 421_l2=0.0114
Eval: 421_h1=0.0725, 421_l2=0.0207
Eval: 128_h1=0.0268, 128_l2=0.0234
[33] time=252.17, avg_loss=0.0145, train_err=0.3482
Eval: 128_h1=0.0158, 128_l2=0.0134
[104] time=81.86, avg_loss=0.0338, train_err=0.2706
Eval: 421_h1=0.0674, 421_l2=0.0125
[105] time=82.11, avg_loss=0.0332, train_err=0.2654
Eval: 421_h1=0.0668, 421_l2=0.0117
[106] time=82.11, avg_loss=0.0325, train_err=0.2597
Eval: 421_h1=0.0674, 421_l2=0.0121
[34] time=255.14, avg_loss=0.0141, train_err=0.3384
[27] time=304.61, avg_loss=0.0574, train_err=0.4593
Eval: 128_h1=0.0159, 128_l2=0.0135
Eval: 421_h1=0.0742, 421_l2=0.0228
[107] time=81.57, avg_loss=0.0307, train_err=0.2452
Eval: 421_h1=0.0672, 421_l2=0.0115
[108] time=82.13, avg_loss=0.0310, train_err=0.2483
Eval: 421_h1=0.0671, 421_l2=0.0112
[18] time=472.95, avg_loss=0.0246, train_err=0.5902
Eval: 128_h1=0.0266, 128_l2=0.0228
[109] time=82.14, avg_loss=0.0306, train_err=0.2446
Eval: 421_h1=0.0677, 421_l2=0.0132
[35] time=253.41, avg_loss=0.0140, train_err=0.3361
Eval: 128_h1=0.0156, 128_l2=0.0128
[110] time=82.16, avg_loss=0.0324, train_err=0.2592
Eval: 421_h1=0.0674, 421_l2=0.0127
[28] time=304.04, avg_loss=0.0575, train_err=0.4604
Eval: 421_h1=0.0712, 421_l2=0.0213
[111] time=81.59, avg_loss=0.0314, train_err=0.2515
Eval: 421_h1=0.0669, 421_l2=0.0114
[112] time=82.16, avg_loss=0.0312, train_err=0.2497
Eval: 421_h1=0.0669, 421_l2=0.0109
[36] time=263.39, avg_loss=0.0142, train_err=0.3412
Eval: 128_h1=0.0161, 128_l2=0.0133
[113] time=82.14, avg_loss=0.0315, train_err=0.2522
Eval: 421_h1=0.0679, 421_l2=0.0120
[29] time=304.53, avg_loss=0.0524, train_err=0.4189
[114] time=81.92, avg_loss=0.0318, train_err=0.2542
Eval: 421_h1=0.0670, 421_l2=0.0113
[19] time=484.45, avg_loss=0.0235, train_err=0.5630
Eval: 421_h1=0.0694, 421_l2=0.0191
Eval: 128_h1=0.0259, 128_l2=0.0222
[115] time=81.83, avg_loss=0.0307, train_err=0.2459
Eval: 421_h1=0.0674, 421_l2=0.0110
[37] time=246.90, avg_loss=0.0152, train_err=0.3646
Eval: 128_h1=0.0160, 128_l2=0.0132
[116] time=82.14, avg_loss=0.0302, train_err=0.2419
Eval: 421_h1=0.0689, 421_l2=0.0130
[117] time=82.13, avg_loss=0.0321, train_err=0.2565
Eval: 421_h1=0.0672, 421_l2=0.0116
[30] time=304.61, avg_loss=0.0531, train_err=0.4245
Eval: 421_h1=0.0713, 421_l2=0.0211
[118] time=81.62, avg_loss=0.0318, train_err=0.2547
Eval: 421_h1=0.0686, 421_l2=0.0170
[38] time=252.64, avg_loss=0.0147, train_err=0.3528
Eval: 128_h1=0.0150, 128_l2=0.0121
[119] time=82.15, avg_loss=0.0308, train_err=0.2463
Eval: 421_h1=0.0670, 421_l2=0.0113
[20] time=468.33, avg_loss=0.0229, train_err=0.5486
Eval: 128_h1=0.0255, 128_l2=0.0216
[120] time=82.16, avg_loss=0.0283, train_err=0.2267
Eval: 421_h1=0.0665, 421_l2=0.0097
[121] time=82.13, avg_loss=0.0269, train_err=0.2153
Eval: 421_h1=0.0668, 421_l2=0.0097
[31] time=303.97, avg_loss=0.0537, train_err=0.4298
[39] time=257.57, avg_loss=0.0142, train_err=0.3401
Eval: 128_h1=0.0149, 128_l2=0.0120
Eval: 421_h1=0.0704, 421_l2=0.0199
[122] time=81.55, avg_loss=0.0265, train_err=0.2123
Eval: 421_h1=0.0672, 421_l2=0.0103
[123] time=82.11, avg_loss=0.0265, train_err=0.2119
Eval: 421_h1=0.0672, 421_l2=0.0095
[124] time=82.15, avg_loss=0.0262, train_err=0.2100
Eval: 421_h1=0.0672, 421_l2=0.0097
[40] time=257.69, avg_loss=0.0137, train_err=0.3290
Eval: 128_h1=0.0146, 128_l2=0.0118
[125] time=82.14, avg_loss=0.0261, train_err=0.2090
[32] time=304.39, avg_loss=0.0507, train_err=0.4056
Eval: 421_h1=0.0673, 421_l2=0.0095
[21] time=482.67, avg_loss=0.0224, train_err=0.5377
Eval: 421_h1=0.0705, 421_l2=0.0187
Eval: 128_h1=0.0246, 128_l2=0.0207
[126] time=81.86, avg_loss=0.0261, train_err=0.2086
Eval: 421_h1=0.0675, 421_l2=0.0093
[127] time=82.14, avg_loss=0.0263, train_err=0.2102
Eval: 421_h1=0.0677, 421_l2=0.0095
[41] time=242.53, avg_loss=0.0132, train_err=0.3167
Eval: 128_h1=0.0146, 128_l2=0.0117
[128] time=82.12, avg_loss=0.0262, train_err=0.2094
Eval: 421_h1=0.0677, 421_l2=0.0094
[33] time=304.60, avg_loss=0.0567, train_err=0.4538
[129] time=81.83, avg_loss=0.0262, train_err=0.2096
Eval: 421_h1=0.0877, 421_l2=0.0416
Eval: 421_h1=0.0677, 421_l2=0.0095
[130] time=82.14, avg_loss=0.0261, train_err=0.2092
Eval: 421_h1=0.0677, 421_l2=0.0095
[42] time=251.78, avg_loss=0.0130, train_err=0.3114
Eval: 128_h1=0.0142, 128_l2=0.0116
[22] time=460.24, avg_loss=0.0220, train_err=0.5285
Eval: 128_h1=0.0239, 128_l2=0.0200
[131] time=82.11, avg_loss=0.0262, train_err=0.2099
Eval: 421_h1=0.0680, 421_l2=0.0114
[132] time=82.10, avg_loss=0.0264, train_err=0.2115
Eval: 421_h1=0.0674, 421_l2=0.0095
[34] time=304.30, avg_loss=0.0607, train_err=0.4855
Eval: 421_h1=0.0706, 421_l2=0.0199
[133] time=81.60, avg_loss=0.0268, train_err=0.2141
Eval: 421_h1=0.0682, 421_l2=0.0115
[43] time=254.16, avg_loss=0.0125, train_err=0.2993
Eval: 128_h1=0.0139, 128_l2=0.0115
[134] time=82.16, avg_loss=0.0273, train_err=0.2180
Eval: 421_h1=0.0685, 421_l2=0.0114
[135] time=82.11, avg_loss=0.0273, train_err=0.2186
Eval: 421_h1=0.0680, 421_l2=0.0111
[136] time=82.12, avg_loss=0.0271, train_err=0.2168
[35] time=304.35, avg_loss=0.0534, train_err=0.4275
Eval: 421_h1=0.0682, 421_l2=0.0097
[44] time=261.81, avg_loss=0.0121, train_err=0.2910
[23] time=484.49, avg_loss=0.0221, train_err=0.5294
Eval: 421_h1=0.0731, 421_l2=0.0240
Eval: 128_h1=0.0136, 128_l2=0.0112
Eval: 128_h1=0.0234, 128_l2=0.0196
[137] time=81.81, avg_loss=0.0265, train_err=0.2121
Eval: 421_h1=0.0679, 421_l2=0.0097
[138] time=82.12, avg_loss=0.0263, train_err=0.2101
Eval: 421_h1=0.0679, 421_l2=0.0095
[139] time=82.16, avg_loss=0.0260, train_err=0.2082
Eval: 421_h1=0.0680, 421_l2=0.0098
[45] time=249.85, avg_loss=0.0122, train_err=0.2921
Eval: 128_h1=0.0137, 128_l2=0.0111
[36] time=304.67, avg_loss=0.0512, train_err=0.4100
[140] time=81.80, avg_loss=0.0260, train_err=0.2082
Eval: 421_h1=0.0681, 421_l2=0.0097
Eval: 421_h1=0.0798, 421_l2=0.0309
[141] time=82.16, avg_loss=0.0261, train_err=0.2090
Eval: 421_h1=0.0679, 421_l2=0.0097
[24] time=461.83, avg_loss=0.0218, train_err=0.5224
[142] time=82.14, avg_loss=0.0264, train_err=0.2116
Eval: 128_h1=0.0231, 128_l2=0.0187
Eval: 421_h1=0.0688, 421_l2=0.0115
[46] time=249.52, avg_loss=0.0123, train_err=0.2949
Eval: 128_h1=0.0137, 128_l2=0.0110
[143] time=82.18, avg_loss=0.0261, train_err=0.2089
Eval: 421_h1=0.0682, 421_l2=0.0093
[37] time=304.44, avg_loss=0.0502, train_err=0.4019
Eval: 421_h1=0.0694, 421_l2=0.0171
[144] time=81.56, avg_loss=0.0260, train_err=0.2083
Eval: 421_h1=0.0682, 421_l2=0.0100
[145] time=82.15, avg_loss=0.0262, train_err=0.2093
Eval: 421_h1=0.0686, 421_l2=0.0097
[47] time=256.00, avg_loss=0.0124, train_err=0.2971
Eval: 128_h1=0.0141, 128_l2=0.0116
[146] time=82.17, avg_loss=0.0279, train_err=0.2231
Eval: 421_h1=0.0687, 421_l2=0.0100
[147] time=82.13, avg_loss=0.0261, train_err=0.2088
Eval: 421_h1=0.0688, 421_l2=0.0108
[38] time=304.03, avg_loss=0.0502, train_err=0.4019
[25] time=490.01, avg_loss=0.0209, train_err=0.5005
Eval: 421_h1=0.0829, 421_l2=0.0344
Eval: 128_h1=0.0253, 128_l2=0.0194
[148] time=81.53, avg_loss=0.0259, train_err=0.2073
Eval: 421_h1=0.0687, 421_l2=0.0097
[48] time=264.51, avg_loss=0.0127, train_err=0.3040
Eval: 128_h1=0.0143, 128_l2=0.0111
[149] time=82.11, avg_loss=0.0261, train_err=0.2087
Eval: 421_h1=0.0683, 421_l2=0.0098
[150] time=82.14, avg_loss=0.0255, train_err=0.2038
Eval: 421_h1=0.0685, 421_l2=0.0096
[39] time=304.69, avg_loss=0.0503, train_err=0.4024
[151] time=81.87, avg_loss=0.0254, train_err=0.2028
Eval: 421_h1=0.0689, 421_l2=0.0102
[49] time=246.22, avg_loss=0.0123, train_err=0.2942
Eval: 421_h1=0.0691, 421_l2=0.0169
Eval: 128_h1=0.0140, 128_l2=0.0112
[152] time=81.85, avg_loss=0.0255, train_err=0.2041
Eval: 421_h1=0.0686, 421_l2=0.0092
[26] time=461.54, avg_loss=0.0200, train_err=0.4799
[153] time=82.16, avg_loss=0.0256, train_err=0.2047
Eval: 421_h1=0.0684, 421_l2=0.0106
Eval: 128_h1=0.0225, 128_l2=0.0174
[154] time=82.18, avg_loss=0.0259, train_err=0.2076
[50] time=248.86, avg_loss=0.0125, train_err=0.2999
Eval: 421_h1=0.0691, 421_l2=0.0104
Eval: 128_h1=0.0174, 128_l2=0.0138
[40] time=304.64, avg_loss=0.0495, train_err=0.3960
Eval: 421_h1=0.0694, 421_l2=0.0188
[155] time=81.58, avg_loss=0.0271, train_err=0.2168
Eval: 421_h1=0.0688, 421_l2=0.0099
[156] time=82.13, avg_loss=0.0258, train_err=0.2066
Eval: 421_h1=0.0686, 421_l2=0.0104
[157] time=82.21, avg_loss=0.0256, train_err=0.2046
[51] time=259.44, avg_loss=0.0131, train_err=0.3140
Eval: 421_h1=0.0686, 421_l2=0.0096
Eval: 128_h1=0.0141, 128_l2=0.0107
[158] time=82.12, avg_loss=0.0252, train_err=0.2016
Eval: 421_h1=0.0687, 421_l2=0.0096
[41] time=303.93, avg_loss=0.0471, train_err=0.3772
Eval: 421_h1=0.0717, 421_l2=0.0217
[27] time=488.36, avg_loss=0.0197, train_err=0.4725
Eval: 128_h1=0.0221, 128_l2=0.0169
[159] time=81.57, avg_loss=0.0251, train_err=0.2010
Eval: 421_h1=0.0689, 421_l2=0.0094
[160] time=82.15, avg_loss=0.0252, train_err=0.2013
Eval: 421_h1=0.0688, 421_l2=0.0102
[52] time=257.16, avg_loss=0.0125, train_err=0.2995
Eval: 128_h1=0.0140, 128_l2=0.0106
[161] time=82.14, avg_loss=0.0254, train_err=0.2032
Eval: 421_h1=0.0689, 421_l2=0.0105
[42] time=304.64, avg_loss=0.0516, train_err=0.4131
[162] time=81.89, avg_loss=0.0254, train_err=0.2032
Eval: 421_h1=0.0687, 421_l2=0.0094
Eval: 421_h1=0.0701, 421_l2=0.0163
[53] time=245.70, avg_loss=0.0122, train_err=0.2933
[163] time=81.79, avg_loss=0.0253, train_err=0.2025
Eval: 421_h1=0.0691, 421_l2=0.0095
Eval: 128_h1=0.0137, 128_l2=0.0104
[28] time=459.10, avg_loss=0.0193, train_err=0.4625
[164] time=82.12, avg_loss=0.0256, train_err=0.2047
Eval: 421_h1=0.0691, 421_l2=0.0103
Eval: 128_h1=0.0213, 128_l2=0.0163
[165] time=82.14, avg_loss=0.0253, train_err=0.2023
Eval: 421_h1=0.0693, 421_l2=0.0094
[43] time=304.66, avg_loss=0.0490, train_err=0.3921
Eval: 421_h1=0.0696, 421_l2=0.0213
[54] time=252.78, avg_loss=0.0128, train_err=0.3068
[166] time=81.56, avg_loss=0.0250, train_err=0.1999
Eval: 421_h1=0.0694, 421_l2=0.0102
Eval: 128_h1=0.0134, 128_l2=0.0102
[167] time=82.13, avg_loss=0.0259, train_err=0.2075
Eval: 421_h1=0.0691, 421_l2=0.0100
[168] time=82.11, avg_loss=0.0254, train_err=0.2034
Eval: 421_h1=0.0694, 421_l2=0.0101
[169] time=82.13, avg_loss=0.0247, train_err=0.1980
[55] time=264.87, avg_loss=0.0117, train_err=0.2816
Eval: 421_h1=0.0692, 421_l2=0.0098
Eval: 128_h1=0.0131, 128_l2=0.0101
[44] time=304.08, avg_loss=0.0457, train_err=0.3656
Eval: 421_h1=0.0686, 421_l2=0.0170
[29] time=491.82, avg_loss=0.0188, train_err=0.4520
Eval: 128_h1=0.0210, 128_l2=0.0159
[170] time=81.55, avg_loss=0.0246, train_err=0.1968
Eval: 421_h1=0.0694, 421_l2=0.0100
[171] time=82.13, avg_loss=0.0245, train_err=0.1958
Eval: 421_h1=0.0694, 421_l2=0.0098
[56] time=251.31, avg_loss=0.0112, train_err=0.2679
[172] time=82.14, avg_loss=0.0249, train_err=0.1992
Eval: 421_h1=0.0694, 421_l2=0.0096
Eval: 128_h1=0.0130, 128_l2=0.0100
[45] time=304.56, avg_loss=0.0451, train_err=0.3605
[173] time=81.86, avg_loss=0.0247, train_err=0.1973
Eval: 421_h1=0.0692, 421_l2=0.0096
Eval: 421_h1=0.0692, 421_l2=0.0190
[174] time=81.84, avg_loss=0.0251, train_err=0.2008
Eval: 421_h1=0.0695, 421_l2=0.0097
[57] time=247.58, avg_loss=0.0108, train_err=0.2598
[175] time=82.17, avg_loss=0.0250, train_err=0.2000
[30] time=460.27, avg_loss=0.0187, train_err=0.4486
Eval: 128_h1=0.0131, 128_l2=0.0099
Eval: 421_h1=0.0699, 421_l2=0.0094
Eval: 128_h1=0.0242, 128_l2=0.0174
[176] time=82.17, avg_loss=0.0246, train_err=0.1971
Eval: 421_h1=0.0693, 421_l2=0.0093
[46] time=304.62, avg_loss=0.0474, train_err=0.3793
Eval: 421_h1=0.0752, 421_l2=0.0275
[177] time=81.53, avg_loss=0.0245, train_err=0.1957
Eval: 421_h1=0.0704, 421_l2=0.0112
[58] time=257.48, avg_loss=0.0109, train_err=0.2620
[178] time=82.14, avg_loss=0.0250, train_err=0.2003
Eval: 128_h1=0.0129, 128_l2=0.0100
Eval: 421_h1=0.0703, 421_l2=0.0104
[179] time=82.12, avg_loss=0.0253, train_err=0.2025
Eval: 421_h1=0.0698, 421_l2=0.0112
[180] time=82.14, avg_loss=0.0239, train_err=0.1908
Eval: 421_h1=0.0692, 421_l2=0.0093
[47] time=304.01, avg_loss=0.0478, train_err=0.3827
Eval: 421_h1=0.0693, 421_l2=0.0183
[31] time=491.15, avg_loss=0.0190, train_err=0.4559
[59] time=260.89, avg_loss=0.0107, train_err=0.2573
[181] time=81.58, avg_loss=0.0230, train_err=0.1842
Eval: 128_h1=0.0228, 128_l2=0.0180
Eval: 421_h1=0.0696, 421_l2=0.0094
Eval: 128_h1=0.0131, 128_l2=0.0101
[182] time=82.08, avg_loss=0.0228, train_err=0.1828
Eval: 421_h1=0.0697, 421_l2=0.0092
[183] time=82.12, avg_loss=0.0228, train_err=0.1824
Eval: 421_h1=0.0698, 421_l2=0.0092
[60] time=253.43, avg_loss=0.0107, train_err=0.2558
[184] time=82.15, avg_loss=0.0227, train_err=0.1818
[48] time=304.43, avg_loss=0.0501, train_err=0.4007
Eval: 421_h1=0.0701, 421_l2=0.0092
Eval: 128_h1=0.0129, 128_l2=0.0102
Eval: 421_h1=0.0701, 421_l2=0.0192
[185] time=81.85, avg_loss=0.0227, train_err=0.1816
Eval: 421_h1=0.0701, 421_l2=0.0092
[186] time=82.17, avg_loss=0.0227, train_err=0.1813
Eval: 421_h1=0.0701, 421_l2=0.0091
[32] time=465.51, avg_loss=0.0207, train_err=0.4973
Eval: 128_h1=0.0279, 128_l2=0.0191
[61] time=247.15, avg_loss=0.0112, train_err=0.2692
[187] time=82.15, avg_loss=0.0226, train_err=0.1811
Eval: 128_h1=0.0135, 128_l2=0.0108
Eval: 421_h1=0.0703, 421_l2=0.0092
[49] time=304.59, avg_loss=0.0439, train_err=0.3509
[188] time=81.81, avg_loss=0.0226, train_err=0.1811
Eval: 421_h1=0.0688, 421_l2=0.0170
Eval: 421_h1=0.0703, 421_l2=0.0093
[189] time=82.14, avg_loss=0.0226, train_err=0.1806
Eval: 421_h1=0.0704, 421_l2=0.0092
[62] time=263.05, avg_loss=0.0111, train_err=0.2655
[190] time=82.15, avg_loss=0.0226, train_err=0.1807
Eval: 421_h1=0.0704, 421_l2=0.0094
Eval: 128_h1=0.0123, 128_l2=0.0095
[191] time=82.13, avg_loss=0.0226, train_err=0.1811
Eval: 421_h1=0.0705, 421_l2=0.0093
[50] time=304.37, avg_loss=0.0470, train_err=0.3762
Eval: 421_h1=0.0704, 421_l2=0.0185
[33] time=489.17, avg_loss=0.0195, train_err=0.4674
[192] time=81.56, avg_loss=0.0226, train_err=0.1811
Eval: 421_h1=0.0705, 421_l2=0.0092
Eval: 128_h1=0.0253, 128_l2=0.0173
[63] time=251.18, avg_loss=0.0108, train_err=0.2579
[193] time=82.10, avg_loss=0.0227, train_err=0.1815
Eval: 128_h1=0.0121, 128_l2=0.0093
Eval: 421_h1=0.0705, 421_l2=0.0097
[194] time=82.14, avg_loss=0.0231, train_err=0.1848
Eval: 421_h1=0.0709, 421_l2=0.0099
[195] time=82.09, avg_loss=0.0232, train_err=0.1855
[51] time=304.41, avg_loss=0.0485, train_err=0.3879
Eval: 421_h1=0.0708, 421_l2=0.0093
Eval: 421_h1=0.0714, 421_l2=0.0200
[64] time=254.49, avg_loss=0.0113, train_err=0.2701
[196] time=81.80, avg_loss=0.0228, train_err=0.1822
Eval: 128_h1=0.0126, 128_l2=0.0097
Eval: 421_h1=0.0705, 421_l2=0.0094
[197] time=82.11, avg_loss=0.0227, train_err=0.1820
Eval: 421_h1=0.0707, 421_l2=0.0095
[34] time=466.32, avg_loss=0.0191, train_err=0.4575
Eval: 128_h1=0.0232, 128_l2=0.0158
[198] time=82.17, avg_loss=0.0228, train_err=0.1821
Eval: 421_h1=0.0706, 421_l2=0.0095
[52] time=304.69, avg_loss=0.0465, train_err=0.3717
[65] time=250.50, avg_loss=0.0120, train_err=0.2875
Eval: 128_h1=0.0124, 128_l2=0.0101
[199] time=81.82, avg_loss=0.0226, train_err=0.1810
Eval: 421_h1=0.0706, 421_l2=0.0093
Eval: 421_h1=0.0706, 421_l2=0.0200
[200] time=81.89, avg_loss=0.0227, train_err=0.1819
Eval: 421_h1=0.0708, 421_l2=0.0100
[201] time=82.09, avg_loss=0.0226, train_err=0.1807
Eval: 421_h1=0.0709, 421_l2=0.0093
[66] time=264.36, avg_loss=0.0111, train_err=0.2658
[202] time=82.13, avg_loss=0.0225, train_err=0.1802
Eval: 128_h1=0.0128, 128_l2=0.0104
Eval: 421_h1=0.0710, 421_l2=0.0093
[53] time=304.61, avg_loss=0.0452, train_err=0.3616
Eval: 421_h1=0.0702, 421_l2=0.0183
[203] time=81.59, avg_loss=0.0225, train_err=0.1800
[35] time=491.84, avg_loss=0.0183, train_err=0.4384
Eval: 421_h1=0.0711, 421_l2=0.0097
Eval: 128_h1=0.0222, 128_l2=0.0159
[204] time=82.17, avg_loss=0.0225, train_err=0.1801
Eval: 421_h1=0.0709, 421_l2=0.0095
[67] time=250.92, avg_loss=0.0106, train_err=0.2533
Eval: 128_h1=0.0132, 128_l2=0.0110
[205] time=82.24, avg_loss=0.0226, train_err=0.1808
Eval: 421_h1=0.0712, 421_l2=0.0096
[206] time=82.17, avg_loss=0.0227, train_err=0.1814
Eval: 421_h1=0.0712, 421_l2=0.0096
[54] time=303.82, avg_loss=0.0551, train_err=0.4405
Eval: 421_h1=0.0736, 421_l2=0.0231
[207] time=81.56, avg_loss=0.0224, train_err=0.1796
Eval: 421_h1=0.0714, 421_l2=0.0092
[68] time=254.56, avg_loss=0.0104, train_err=0.2487
Eval: 128_h1=0.0120, 128_l2=0.0093
[208] time=82.15, avg_loss=0.0223, train_err=0.1787
Eval: 421_h1=0.0711, 421_l2=0.0092
[36] time=471.30, avg_loss=0.0179, train_err=0.4299
Eval: 128_h1=0.0183, 128_l2=0.0137
[209] time=82.16, avg_loss=0.0223, train_err=0.1787
Eval: 421_h1=0.0713, 421_l2=0.0091
[55] time=304.55, avg_loss=0.0485, train_err=0.3880
[210] time=81.89, avg_loss=0.0224, train_err=0.1793
Eval: 421_h1=0.0714, 421_l2=0.0095
Eval: 421_h1=0.0687, 421_l2=0.0181
[69] time=256.26, avg_loss=0.0106, train_err=0.2549
Eval: 128_h1=0.0119, 128_l2=0.0093
[211] time=81.86, avg_loss=0.0225, train_err=0.1802
Eval: 421_h1=0.0714, 421_l2=0.0092
[212] time=82.15, avg_loss=0.0223, train_err=0.1788
Eval: 421_h1=0.0715, 421_l2=0.0093
[213] time=82.19, avg_loss=0.0223, train_err=0.1786
Eval: 421_h1=0.0713, 421_l2=0.0096
[56] time=304.60, avg_loss=0.0451, train_err=0.3612
Eval: 421_h1=0.0683, 421_l2=0.0194
[70] time=260.61, avg_loss=0.0105, train_err=0.2528
[214] time=81.64, avg_loss=0.0224, train_err=0.1794
Eval: 128_h1=0.0120, 128_l2=0.0095
Eval: 421_h1=0.0714, 421_l2=0.0092
[37] time=488.43, avg_loss=0.0169, train_err=0.4052
Eval: 128_h1=0.0182, 128_l2=0.0130
[215] time=82.19, avg_loss=0.0222, train_err=0.1775
Eval: 421_h1=0.0714, 421_l2=0.0094
[216] time=82.17, avg_loss=0.0223, train_err=0.1780
Eval: 421_h1=0.0714, 421_l2=0.0094
[71] time=246.63, avg_loss=0.0103, train_err=0.2480
Eval: 128_h1=0.0119, 128_l2=0.0092
[217] time=82.11, avg_loss=0.0223, train_err=0.1781
Eval: 421_h1=0.0714, 421_l2=0.0094
[57] time=303.84, avg_loss=0.0420, train_err=0.3358
Eval: 421_h1=0.0682, 421_l2=0.0161
[218] time=81.57, avg_loss=0.0221, train_err=0.1770
Eval: 421_h1=0.0718, 421_l2=0.0094
[219] time=82.17, avg_loss=0.0223, train_err=0.1780
Eval: 421_h1=0.0717, 421_l2=0.0093
[38] time=469.27, avg_loss=0.0170, train_err=0.4072
[72] time=256.08, avg_loss=0.0103, train_err=0.2480
Eval: 128_h1=0.0119, 128_l2=0.0096
[220] time=82.16, avg_loss=0.0221, train_err=0.1771
Eval: 128_h1=0.0181, 128_l2=0.0133
Eval: 421_h1=0.0717, 421_l2=0.0094
[58] time=304.60, avg_loss=0.0415, train_err=0.3321
[221] time=81.87, avg_loss=0.0221, train_err=0.1766
Eval: 421_h1=0.0719, 421_l2=0.0101
Eval: 421_h1=0.0716, 421_l2=0.0218
[222] time=81.86, avg_loss=0.0222, train_err=0.1773
Eval: 421_h1=0.0716, 421_l2=0.0094
[73] time=259.31, avg_loss=0.0102, train_err=0.2442
Eval: 128_h1=0.0119, 128_l2=0.0092
[223] time=82.14, avg_loss=0.0221, train_err=0.1767
Eval: 421_h1=0.0720, 421_l2=0.0095
[224] time=82.15, avg_loss=0.0220, train_err=0.1759
Eval: 421_h1=0.0720, 421_l2=0.0099
[59] time=304.57, avg_loss=0.0465, train_err=0.3718
Eval: 421_h1=0.0692, 421_l2=0.0183
[225] time=81.52, avg_loss=0.0219, train_err=0.1753
Eval: 421_h1=0.0719, 421_l2=0.0095
[39] time=482.87, avg_loss=0.0175, train_err=0.4208
Eval: 128_h1=0.0219, 128_l2=0.0151
[74] time=253.13, avg_loss=0.0099, train_err=0.2380
Eval: 128_h1=0.0115, 128_l2=0.0090
[226] time=82.18, avg_loss=0.0219, train_err=0.1754
Eval: 421_h1=0.0716, 421_l2=0.0093
[227] time=82.18, avg_loss=0.0220, train_err=0.1756
Eval: 421_h1=0.0720, 421_l2=0.0095
[228] time=82.16, avg_loss=0.0219, train_err=0.1753
Eval: 421_h1=0.0719, 421_l2=0.0092
[60] time=303.99, avg_loss=0.0379, train_err=0.3030
Eval: 421_h1=0.0657, 421_l2=0.0136
[75] time=250.29, avg_loss=0.0097, train_err=0.2336
Eval: 128_h1=0.0112, 128_l2=0.0082
[229] time=81.57, avg_loss=0.0220, train_err=0.1764
Eval: 421_h1=0.0724, 421_l2=0.0095
[230] time=82.16, avg_loss=0.0220, train_err=0.1759
Eval: 421_h1=0.0719, 421_l2=0.0097
[40] time=468.01, avg_loss=0.0168, train_err=0.4041
[231] time=82.14, avg_loss=0.0219, train_err=0.1756
Eval: 421_h1=0.0721, 421_l2=0.0093
Eval: 128_h1=0.0175, 128_l2=0.0123
[76] time=256.32, avg_loss=0.0098, train_err=0.2340
Eval: 128_h1=0.0114, 128_l2=0.0081
[61] time=304.57, avg_loss=0.0350, train_err=0.2799
[232] time=81.83, avg_loss=0.0219, train_err=0.1750
Eval: 421_h1=0.0719, 421_l2=0.0095
Eval: 421_h1=0.0662, 421_l2=0.0139
[233] time=81.84, avg_loss=0.0219, train_err=0.1751
Eval: 421_h1=0.0726, 421_l2=0.0127
[234] time=82.15, avg_loss=0.0221, train_err=0.1772
Eval: 421_h1=0.0721, 421_l2=0.0093
[77] time=259.50, avg_loss=0.0098, train_err=0.2361
Eval: 128_h1=0.0115, 128_l2=0.0083
[235] time=82.15, avg_loss=0.0218, train_err=0.1743
Eval: 421_h1=0.0720, 421_l2=0.0096
[62] time=304.66, avg_loss=0.0344, train_err=0.2755
Eval: 421_h1=0.0670, 421_l2=0.0143
[236] time=81.56, avg_loss=0.0216, train_err=0.1731
Eval: 421_h1=0.0723, 421_l2=0.0094
[41] time=480.48, avg_loss=0.0161, train_err=0.3870
Eval: 128_h1=0.0173, 128_l2=0.0125
[237] time=82.16, avg_loss=0.0216, train_err=0.1727
Eval: 421_h1=0.0721, 421_l2=0.0093
[78] time=246.71, avg_loss=0.0098, train_err=0.2358
Eval: 128_h1=0.0115, 128_l2=0.0086
[238] time=82.15, avg_loss=0.0216, train_err=0.1728
Eval: 421_h1=0.0724, 421_l2=0.0094
[239] time=82.13, avg_loss=0.0216, train_err=0.1730
Eval: 421_h1=0.0721, 421_l2=0.0093
[63] time=303.90, avg_loss=0.0340, train_err=0.2718
Eval: 421_h1=0.0665, 421_l2=0.0130
[240] time=81.65, avg_loss=0.0214, train_err=0.1712
Eval: 421_h1=0.0722, 421_l2=0.0092
[79] time=251.14, avg_loss=0.0107, train_err=0.2574
Eval: 128_h1=0.0119, 128_l2=0.0082
[241] time=82.16, avg_loss=0.0211, train_err=0.1684
Eval: 421_h1=0.0725, 421_l2=0.0092
[42] time=468.15, avg_loss=0.0160, train_err=0.3837
[242] time=82.22, avg_loss=0.0210, train_err=0.1679
Eval: 421_h1=0.0725, 421_l2=0.0092
Eval: 128_h1=0.0171, 128_l2=0.0120
[243] time=82.17, avg_loss=0.0210, train_err=0.1677
[64] time=304.23, avg_loss=0.0341, train_err=0.2726
Eval: 421_h1=0.0726, 421_l2=0.0093
Eval: 421_h1=0.0664, 421_l2=0.0127
[80] time=261.30, avg_loss=0.0104, train_err=0.2491
Eval: 128_h1=0.0113, 128_l2=0.0082
[244] time=81.80, avg_loss=0.0209, train_err=0.1675
Eval: 421_h1=0.0727, 421_l2=0.0093
[245] time=82.18, avg_loss=0.0209, train_err=0.1674
Eval: 421_h1=0.0727, 421_l2=0.0093
[246] time=82.21, avg_loss=0.0209, train_err=0.1673
Eval: 421_h1=0.0728, 421_l2=0.0093
[81] time=253.74, avg_loss=0.0096, train_err=0.2297
[65] time=304.59, avg_loss=0.0339, train_err=0.2709
Eval: 128_h1=0.0112, 128_l2=0.0079
[247] time=81.90, avg_loss=0.0209, train_err=0.1672
Eval: 421_h1=0.0666, 421_l2=0.0124
Eval: 421_h1=0.0728, 421_l2=0.0092
[43] time=480.61, avg_loss=0.0160, train_err=0.3834
Eval: 128_h1=0.0171, 128_l2=0.0123
[248] time=82.22, avg_loss=0.0209, train_err=0.1671
Eval: 421_h1=0.0729, 421_l2=0.0093
[249] time=82.14, avg_loss=0.0209, train_err=0.1669
Eval: 421_h1=0.0729, 421_l2=0.0092
[82] time=250.93, avg_loss=0.0092, train_err=0.2200
Eval: 128_h1=0.0107, 128_l2=0.0076
[250] time=82.19, avg_loss=0.0209, train_err=0.1668
Eval: 421_h1=0.0730, 421_l2=0.0092
[66] time=304.14, avg_loss=0.0337, train_err=0.2697
Eval: 421_h1=0.0666, 421_l2=0.0128
[251] time=81.58, avg_loss=0.0208, train_err=0.1668
Eval: 421_h1=0.0730, 421_l2=0.0092
[252] time=82.21, avg_loss=0.0208, train_err=0.1667
Eval: 421_h1=0.0731, 421_l2=0.0093
[83] time=250.90, avg_loss=0.0087, train_err=0.2090
Eval: 128_h1=0.0105, 128_l2=0.0075
[44] time=470.34, avg_loss=0.0160, train_err=0.3842
[253] time=82.19, avg_loss=0.0209, train_err=0.1670
Eval: 421_h1=0.0732, 421_l2=0.0098
Eval: 128_h1=0.0184, 128_l2=0.0142
[254] time=82.15, avg_loss=0.0210, train_err=0.1679
[67] time=304.38, avg_loss=0.0336, train_err=0.2687
Eval: 421_h1=0.0734, 421_l2=0.0094
Eval: 421_h1=0.0666, 421_l2=0.0122
[255] time=81.80, avg_loss=0.0210, train_err=0.1679
Eval: 421_h1=0.0735, 421_l2=0.0093
[84] time=264.38, avg_loss=0.0085, train_err=0.2039
Eval: 128_h1=0.0100, 128_l2=0.0074
[256] time=82.13, avg_loss=0.0209, train_err=0.1670
Eval: 421_h1=0.0733, 421_l2=0.0092
[257] time=82.19, avg_loss=0.0208, train_err=0.1666
Eval: 421_h1=0.0733, 421_l2=0.0093
[68] time=304.50, avg_loss=0.0341, train_err=0.2731
[258] time=81.95, avg_loss=0.0208, train_err=0.1665
Eval: 421_h1=0.0669, 421_l2=0.0128
Eval: 421_h1=0.0732, 421_l2=0.0093
[85] time=247.53, avg_loss=0.0086, train_err=0.2056
[45] time=479.89, avg_loss=0.0157, train_err=0.3758
Eval: 128_h1=0.0101, 128_l2=0.0075
Eval: 128_h1=0.0167, 128_l2=0.0119
[259] time=82.16, avg_loss=0.0208, train_err=0.1665
Eval: 421_h1=0.0732, 421_l2=0.0093
[260] time=82.18, avg_loss=0.0208, train_err=0.1664
Eval: 421_h1=0.0732, 421_l2=0.0092
[261] time=82.19, avg_loss=0.0208, train_err=0.1665
Eval: 421_h1=0.0733, 421_l2=0.0093
[86] time=251.62, avg_loss=0.0087, train_err=0.2097
[69] time=304.25, avg_loss=0.0347, train_err=0.2778
Eval: 128_h1=0.0108, 128_l2=0.0077
Eval: 421_h1=0.0668, 421_l2=0.0128
[262] time=81.51, avg_loss=0.0208, train_err=0.1663
Eval: 421_h1=0.0734, 421_l2=0.0093
[263] time=82.16, avg_loss=0.0208, train_err=0.1662
Eval: 421_h1=0.0734, 421_l2=0.0093
[264] time=82.21, avg_loss=0.0208, train_err=0.1662
[46] time=469.71, avg_loss=0.0151, train_err=0.3624
Eval: 421_h1=0.0736, 421_l2=0.0093
Eval: 128_h1=0.0170, 128_l2=0.0122
[87] time=252.86, avg_loss=0.0089, train_err=0.2123
Eval: 128_h1=0.0108, 128_l2=0.0073
[265] time=82.17, avg_loss=0.0208, train_err=0.1660
Eval: 421_h1=0.0735, 421_l2=0.0096
[70] time=304.00, avg_loss=0.0344, train_err=0.2748
Eval: 421_h1=0.0669, 421_l2=0.0126
[266] time=81.54, avg_loss=0.0208, train_err=0.1661
Eval: 421_h1=0.0735, 421_l2=0.0098
[267] time=82.13, avg_loss=0.0208, train_err=0.1663
Eval: 421_h1=0.0737, 421_l2=0.0093
[88] time=259.68, avg_loss=0.0082, train_err=0.1965
Eval: 128_h1=0.0104, 128_l2=0.0073
[268] time=82.17, avg_loss=0.0207, train_err=0.1656
Eval: 421_h1=0.0736, 421_l2=0.0092
[71] time=304.57, avg_loss=0.0348, train_err=0.2783
[269] time=81.83, avg_loss=0.0207, train_err=0.1655
Eval: 421_h1=0.0737, 421_l2=0.0092
Eval: 421_h1=0.0672, 421_l2=0.0133
[47] time=483.92, avg_loss=0.0160, train_err=0.3827
[270] time=81.84, avg_loss=0.0206, train_err=0.1651
Eval: 128_h1=0.0168, 128_l2=0.0116
Eval: 421_h1=0.0737, 421_l2=0.0093
[89] time=255.79, avg_loss=0.0080, train_err=0.1917
Eval: 128_h1=0.0104, 128_l2=0.0070
[271] time=82.18, avg_loss=0.0206, train_err=0.1648
Eval: 421_h1=0.0737, 421_l2=0.0092
[272] time=82.18, avg_loss=0.0206, train_err=0.1647
Eval: 421_h1=0.0737, 421_l2=0.0094
[72] time=304.63, avg_loss=0.0349, train_err=0.2796
Eval: 421_h1=0.0672, 421_l2=0.0129
[273] time=81.53, avg_loss=0.0206, train_err=0.1647
Eval: 421_h1=0.0738, 421_l2=0.0095
[90] time=252.93, avg_loss=0.0085, train_err=0.2029
Eval: 128_h1=0.0104, 128_l2=0.0072
[274] time=82.20, avg_loss=0.0206, train_err=0.1646
Eval: 421_h1=0.0740, 421_l2=0.0093
[275] time=82.16, avg_loss=0.0206, train_err=0.1645
Eval: 421_h1=0.0740, 421_l2=0.0093
[48] time=469.99, avg_loss=0.0151, train_err=0.3614
Eval: 128_h1=0.0163, 128_l2=0.0110
[276] time=82.23, avg_loss=0.0205, train_err=0.1643
Eval: 421_h1=0.0740, 421_l2=0.0092
[73] time=303.78, avg_loss=0.0339, train_err=0.2713
[91] time=254.06, avg_loss=0.0083, train_err=0.1998
Eval: 421_h1=0.0674, 421_l2=0.0125
Eval: 128_h1=0.0103, 128_l2=0.0070
[277] time=81.48, avg_loss=0.0206, train_err=0.1647
Eval: 421_h1=0.0739, 421_l2=0.0093
[278] time=82.17, avg_loss=0.0206, train_err=0.1650
Eval: 421_h1=0.0741, 421_l2=0.0095
[279] time=82.20, avg_loss=0.0205, train_err=0.1640
Eval: 421_h1=0.0741, 421_l2=0.0093
[92] time=261.41, avg_loss=0.0083, train_err=0.1984
Eval: 128_h1=0.0102, 128_l2=0.0070
[74] time=304.57, avg_loss=0.0342, train_err=0.2738
[280] time=81.92, avg_loss=0.0205, train_err=0.1638
Eval: 421_h1=0.0741, 421_l2=0.0093
Eval: 421_h1=0.0673, 421_l2=0.0125
[49] time=480.22, avg_loss=0.0142, train_err=0.3394
[281] time=81.84, avg_loss=0.0205, train_err=0.1637
Eval: 421_h1=0.0742, 421_l2=0.0093
Eval: 128_h1=0.0162, 128_l2=0.0109
[282] time=82.23, avg_loss=0.0204, train_err=0.1635
Eval: 421_h1=0.0742, 421_l2=0.0092
[93] time=242.90, avg_loss=0.0091, train_err=0.2176
Eval: 128_h1=0.0110, 128_l2=0.0106
[283] time=82.16, avg_loss=0.0205, train_err=0.1636
Eval: 421_h1=0.0744, 421_l2=0.0093
[75] time=304.57, avg_loss=0.0344, train_err=0.2750
Eval: 421_h1=0.0681, 421_l2=0.0143
[284] time=81.49, avg_loss=0.0205, train_err=0.1638
Eval: 421_h1=0.0741, 421_l2=0.0093
[285] time=82.20, avg_loss=0.0204, train_err=0.1636
Eval: 421_h1=0.0744, 421_l2=0.0092
[94] time=252.27, avg_loss=0.0094, train_err=0.2251
Eval: 128_h1=0.0111, 128_l2=0.0130
[286] time=82.19, avg_loss=0.0204, train_err=0.1633
Eval: 421_h1=0.0742, 421_l2=0.0092
[50] time=468.04, avg_loss=0.0140, train_err=0.3352
Eval: 128_h1=0.0161, 128_l2=0.0109
[287] time=82.16, avg_loss=0.0204, train_err=0.1633
Eval: 421_h1=0.0742, 421_l2=0.0093
[76] time=303.91, avg_loss=0.0349, train_err=0.2789
Eval: 421_h1=0.0675, 421_l2=0.0129
[288] time=81.54, avg_loss=0.0204, train_err=0.1631
Eval: 421_h1=0.0743, 421_l2=0.0094
[95] time=254.21, avg_loss=0.0088, train_err=0.2106
Eval: 128_h1=0.0103, 128_l2=0.0091
[289] time=82.18, avg_loss=0.0203, train_err=0.1627
Eval: 421_h1=0.0745, 421_l2=0.0093
[290] time=82.18, avg_loss=0.0203, train_err=0.1626
Eval: 421_h1=0.0745, 421_l2=0.0094
[77] time=304.54, avg_loss=0.0375, train_err=0.2999
[291] time=81.89, avg_loss=0.0203, train_err=0.1628
Eval: 421_h1=0.0745, 421_l2=0.0093
[96] time=257.16, avg_loss=0.0079, train_err=0.1891
Eval: 421_h1=0.0682, 421_l2=0.0144
Eval: 128_h1=0.0101, 128_l2=0.0079
[51] time=475.79, avg_loss=0.0139, train_err=0.3343
[292] time=81.78, avg_loss=0.0203, train_err=0.1626
Eval: 421_h1=0.0745, 421_l2=0.0093
Eval: 128_h1=0.0160, 128_l2=0.0110
[293] time=82.18, avg_loss=0.0203, train_err=0.1625
Eval: 421_h1=0.0745, 421_l2=0.0093
[294] time=82.14, avg_loss=0.0203, train_err=0.1623
Eval: 421_h1=0.0747, 421_l2=0.0093
[97] time=251.72, avg_loss=0.0079, train_err=0.1890
Eval: 128_h1=0.0102, 128_l2=0.0085
[78] time=304.56, avg_loss=0.0348, train_err=0.2787
Eval: 421_h1=0.0676, 421_l2=0.0144
[295] time=81.54, avg_loss=0.0203, train_err=0.1623
Eval: 421_h1=0.0745, 421_l2=0.0094
[296] time=82.20, avg_loss=0.0203, train_err=0.1620
Eval: 421_h1=0.0747, 421_l2=0.0093
[297] time=82.16, avg_loss=0.0202, train_err=0.1618
Eval: 421_h1=0.0748, 421_l2=0.0095
[98] time=253.00, avg_loss=0.0097, train_err=0.2319
Eval: 128_h1=0.0111, 128_l2=0.0147
[52] time=471.62, avg_loss=0.0140, train_err=0.3362
Eval: 128_h1=0.0162, 128_l2=0.0108
[298] time=82.17, avg_loss=0.0202, train_err=0.1617
Eval: 421_h1=0.0746, 421_l2=0.0094
[79] time=303.85, avg_loss=0.0337, train_err=0.2695
Eval: 421_h1=0.0679, 421_l2=0.0135
[299] time=81.55, avg_loss=0.0202, train_err=0.1619
Eval: 421_h1=0.0748, 421_l2=0.0093
[99] time=252.40, avg_loss=0.0097, train_err=0.2322
Eval: 128_h1=0.0112, 128_l2=0.0122
[80] time=153.78, avg_loss=0.0339, train_err=0.2712
Eval: 421_h1=0.0679, 421_l2=0.0123
[81] time=130.35, avg_loss=0.0338, train_err=0.2701
Eval: 421_h1=0.0681, 421_l2=0.0145
[53] time=486.24, avg_loss=0.0135, train_err=0.3244
[100] time=262.12, avg_loss=0.0080, train_err=0.1914
Eval: 128_h1=0.0163, 128_l2=0.0105
Eval: 128_h1=0.0093, 128_l2=0.0067
[82] time=130.36, avg_loss=0.0330, train_err=0.2639
Eval: 421_h1=0.0676, 421_l2=0.0128
[83] time=130.34, avg_loss=0.0338, train_err=0.2701
Eval: 421_h1=0.0686, 421_l2=0.0155
[101] time=243.56, avg_loss=0.0076, train_err=0.1815
Eval: 128_h1=0.0090, 128_l2=0.0066
[84] time=130.37, avg_loss=0.0339, train_err=0.2713
Eval: 421_h1=0.0680, 421_l2=0.0140
[85] time=130.33, avg_loss=0.0339, train_err=0.2710
[54] time=453.90, avg_loss=0.0149, train_err=0.3575
Eval: 421_h1=0.0683, 421_l2=0.0130
Eval: 128_h1=0.0182, 128_l2=0.0127
[102] time=247.60, avg_loss=0.0073, train_err=0.1756
Eval: 128_h1=0.0089, 128_l2=0.0068
[86] time=130.34, avg_loss=0.0344, train_err=0.2755
Eval: 421_h1=0.0681, 421_l2=0.0128
[87] time=130.38, avg_loss=0.0338, train_err=0.2702
Eval: 421_h1=0.0685, 421_l2=0.0134
[103] time=254.42, avg_loss=0.0072, train_err=0.1720
Eval: 128_h1=0.0088, 128_l2=0.0068
[88] time=130.33, avg_loss=0.0359, train_err=0.2873
Eval: 421_h1=0.0679, 421_l2=0.0124
[55] time=483.75, avg_loss=0.0160, train_err=0.3848
Eval: 128_h1=0.0234, 128_l2=0.0167
[89] time=130.34, avg_loss=0.0336, train_err=0.2685
Eval: 421_h1=0.0702, 421_l2=0.0182
[104] time=258.53, avg_loss=0.0071, train_err=0.1697
Eval: 128_h1=0.0087, 128_l2=0.0070
[90] time=130.36, avg_loss=0.0330, train_err=0.2637
Eval: 421_h1=0.0681, 421_l2=0.0140
[91] time=130.34, avg_loss=0.0324, train_err=0.2591
[105] time=241.74, avg_loss=0.0070, train_err=0.1677
Eval: 421_h1=0.0680, 421_l2=0.0131
Eval: 128_h1=0.0087, 128_l2=0.0071
[56] time=451.23, avg_loss=0.0162, train_err=0.3877
[92] time=130.36, avg_loss=0.0350, train_err=0.2799
Eval: 421_h1=0.0695, 421_l2=0.0175
Eval: 128_h1=0.0187, 128_l2=0.0121
[106] time=248.95, avg_loss=0.0069, train_err=0.1658
Eval: 128_h1=0.0087, 128_l2=0.0072
[93] time=130.39, avg_loss=0.0354, train_err=0.2835
Eval: 421_h1=0.0683, 421_l2=0.0126
[94] time=130.36, avg_loss=0.0327, train_err=0.2613
Eval: 421_h1=0.0702, 421_l2=0.0175
[107] time=263.85, avg_loss=0.0068, train_err=0.1643
Eval: 128_h1=0.0086, 128_l2=0.0072
[95] time=130.33, avg_loss=0.0329, train_err=0.2635
Eval: 421_h1=0.0685, 421_l2=0.0131
[57] time=484.97, avg_loss=0.0140, train_err=0.3347
Eval: 128_h1=0.0156, 128_l2=0.0101
[96] time=130.36, avg_loss=0.0315, train_err=0.2518
Eval: 421_h1=0.0681, 421_l2=0.0124
[108] time=247.13, avg_loss=0.0068, train_err=0.1635
Eval: 128_h1=0.0086, 128_l2=0.0070
[97] time=130.34, avg_loss=0.0319, train_err=0.2555
Eval: 421_h1=0.0685, 421_l2=0.0147
[98] time=130.34, avg_loss=0.0326, train_err=0.2612
Eval: 421_h1=0.0683, 421_l2=0.0122
[109] time=243.45, avg_loss=0.0068, train_err=0.1638
Eval: 128_h1=0.0085, 128_l2=0.0065
[58] time=449.97, avg_loss=0.0140, train_err=0.3349
[99] time=130.34, avg_loss=0.0318, train_err=0.2548
Eval: 128_h1=0.0212, 128_l2=0.0146
Eval: 421_h1=0.0690, 421_l2=0.0150
[100] time=130.36, avg_loss=0.0343, train_err=0.2745
Eval: 421_h1=0.0700, 421_l2=0.0159
[110] time=253.58, avg_loss=0.0067, train_err=0.1602
Eval: 128_h1=0.0085, 128_l2=0.0067
[101] time=130.33, avg_loss=0.0328, train_err=0.2628
Eval: 421_h1=0.0680, 421_l2=0.0129
[102] time=130.34, avg_loss=0.0321, train_err=0.2568
Eval: 421_h1=0.0684, 421_l2=0.0122
[111] time=261.64, avg_loss=0.0066, train_err=0.1577
Eval: 128_h1=0.0085, 128_l2=0.0069
[59] time=488.73, avg_loss=0.0137, train_err=0.3280
Eval: 128_h1=0.0152, 128_l2=0.0097
[103] time=130.34, avg_loss=0.0338, train_err=0.2702
Eval: 421_h1=0.0688, 421_l2=0.0148
[104] time=130.35, avg_loss=0.0325, train_err=0.2604
[112] time=244.60, avg_loss=0.0065, train_err=0.1560
Eval: 421_h1=0.0679, 421_l2=0.0122
Eval: 128_h1=0.0084, 128_l2=0.0071
[105] time=130.33, avg_loss=0.0329, train_err=0.2632
Eval: 421_h1=0.0685, 421_l2=0.0125
[60] time=449.95, avg_loss=0.0133, train_err=0.3196
[113] time=243.96, avg_loss=0.0064, train_err=0.1536
Eval: 128_h1=0.0155, 128_l2=0.0098
Eval: 128_h1=0.0084, 128_l2=0.0072
[106] time=130.35, avg_loss=0.0320, train_err=0.2556
Eval: 421_h1=0.0683, 421_l2=0.0125
[107] time=130.35, avg_loss=0.0317, train_err=0.2538
Eval: 421_h1=0.0699, 421_l2=0.0168
[114] time=258.53, avg_loss=0.0063, train_err=0.1508
Eval: 128_h1=0.0084, 128_l2=0.0072
[108] time=130.37, avg_loss=0.0315, train_err=0.2518
Eval: 421_h1=0.0686, 421_l2=0.0127
[109] time=130.33, avg_loss=0.0312, train_err=0.2497
Eval: 421_h1=0.0685, 421_l2=0.0136
[61] time=484.10, avg_loss=0.0133, train_err=0.3197
Eval: 128_h1=0.0145, 128_l2=0.0092
[115] time=253.56, avg_loss=0.0062, train_err=0.1487
Eval: 128_h1=0.0083, 128_l2=0.0071
[110] time=130.34, avg_loss=0.0309, train_err=0.2473
Eval: 421_h1=0.0687, 421_l2=0.0124
[111] time=130.39, avg_loss=0.0312, train_err=0.2494
Eval: 421_h1=0.0685, 421_l2=0.0117
[116] time=245.63, avg_loss=0.0061, train_err=0.1466
Eval: 128_h1=0.0082, 128_l2=0.0068
[112] time=130.34, avg_loss=0.0319, train_err=0.2552
Eval: 421_h1=0.0683, 421_l2=0.0118
[62] time=449.50, avg_loss=0.0125, train_err=0.3005
Eval: 128_h1=0.0147, 128_l2=0.0091
[113] time=130.32, avg_loss=0.0315, train_err=0.2519
Eval: 421_h1=0.0688, 421_l2=0.0146
[117] time=243.57, avg_loss=0.0060, train_err=0.1447
Eval: 128_h1=0.0082, 128_l2=0.0067
[114] time=130.37, avg_loss=0.0315, train_err=0.2520
Eval: 421_h1=0.0687, 421_l2=0.0142
[115] time=130.33, avg_loss=0.0312, train_err=0.2497
Eval: 421_h1=0.0680, 421_l2=0.0123
[118] time=262.92, avg_loss=0.0060, train_err=0.1430
Eval: 128_h1=0.0081, 128_l2=0.0066
[116] time=130.34, avg_loss=0.0325, train_err=0.2600
Eval: 421_h1=0.0684, 421_l2=0.0131
[63] time=482.69, avg_loss=0.0124, train_err=0.2964
Eval: 128_h1=0.0143, 128_l2=0.0089
[117] time=130.34, avg_loss=0.0316, train_err=0.2525
[119] time=246.20, avg_loss=0.0059, train_err=0.1413
Eval: 421_h1=0.0689, 421_l2=0.0122
Eval: 128_h1=0.0081, 128_l2=0.0065
[118] time=130.37, avg_loss=0.0338, train_err=0.2702
Eval: 421_h1=0.0680, 421_l2=0.0123
[120] time=247.72, avg_loss=0.0058, train_err=0.1401
[119] time=130.34, avg_loss=0.0305, train_err=0.2442
Eval: 128_h1=0.0081, 128_l2=0.0063
Eval: 421_h1=0.0684, 421_l2=0.0122
[64] time=452.19, avg_loss=0.0127, train_err=0.3037
Eval: 128_h1=0.0149, 128_l2=0.0095
[120] time=130.35, avg_loss=0.0283, train_err=0.2261
Eval: 421_h1=0.0678, 421_l2=0.0119
[121] time=248.33, avg_loss=0.0061, train_err=0.1456
Eval: 128_h1=0.0082, 128_l2=0.0063
[121] time=130.33, avg_loss=0.0267, train_err=0.2137
Eval: 421_h1=0.0681, 421_l2=0.0118
[122] time=130.35, avg_loss=0.0262, train_err=0.2095
Eval: 421_h1=0.0683, 421_l2=0.0114
[122] time=261.14, avg_loss=0.0068, train_err=0.1628
Eval: 128_h1=0.0087, 128_l2=0.0066
[123] time=130.33, avg_loss=0.0260, train_err=0.2077
Eval: 421_h1=0.0684, 421_l2=0.0114
[65] time=482.40, avg_loss=0.0128, train_err=0.3079
Eval: 128_h1=0.0144, 128_l2=0.0093
[124] time=130.36, avg_loss=0.0259, train_err=0.2070
Eval: 421_h1=0.0686, 421_l2=0.0114
[123] time=244.18, avg_loss=0.0067, train_err=0.1611
Eval: 128_h1=0.0088, 128_l2=0.0062
[125] time=130.37, avg_loss=0.0258, train_err=0.2063
Eval: 421_h1=0.0687, 421_l2=0.0113
[126] time=130.36, avg_loss=0.0258, train_err=0.2066
Eval: 421_h1=0.0689, 421_l2=0.0113
[124] time=247.93, avg_loss=0.0064, train_err=0.1527
Eval: 128_h1=0.0085, 128_l2=0.0061
[66] time=454.78, avg_loss=0.0124, train_err=0.2979
Eval: 128_h1=0.0143, 128_l2=0.0096
[127] time=130.38, avg_loss=0.0259, train_err=0.2074
Eval: 421_h1=0.0689, 421_l2=0.0113
[128] time=130.39, avg_loss=0.0260, train_err=0.2083
Eval: 421_h1=0.0689, 421_l2=0.0112
[125] time=253.95, avg_loss=0.0062, train_err=0.1495
Eval: 128_h1=0.0083, 128_l2=0.0060
[129] time=130.37, avg_loss=0.0261, train_err=0.2091
Eval: 421_h1=0.0689, 421_l2=0.0114
[67] time=480.99, avg_loss=0.0140, train_err=0.3356
[130] time=130.36, avg_loss=0.0262, train_err=0.2098
Eval: 421_h1=0.0691, 421_l2=0.0111
[126] time=254.76, avg_loss=0.0062, train_err=0.1487
Eval: 128_h1=0.0145, 128_l2=0.0089
Eval: 128_h1=0.0085, 128_l2=0.0059
[131] time=130.35, avg_loss=0.0263, train_err=0.2107
Eval: 421_h1=0.0692, 421_l2=0.0113
[127] time=245.86, avg_loss=0.0063, train_err=0.1506
[132] time=130.36, avg_loss=0.0263, train_err=0.2107
Eval: 128_h1=0.0098, 128_l2=0.0068
Eval: 421_h1=0.0690, 421_l2=0.0113
[133] time=130.35, avg_loss=0.0265, train_err=0.2121
Eval: 421_h1=0.0691, 421_l2=0.0114
[68] time=453.93, avg_loss=0.0123, train_err=0.2953
Eval: 128_h1=0.0140, 128_l2=0.0087
[128] time=246.83, avg_loss=0.0065, train_err=0.1559
Eval: 128_h1=0.0098, 128_l2=0.0069
[134] time=130.36, avg_loss=0.0262, train_err=0.2096
Eval: 421_h1=0.0692, 421_l2=0.0115
[135] time=130.35, avg_loss=0.0267, train_err=0.2140
Eval: 421_h1=0.0696, 421_l2=0.0132
[129] time=257.93, avg_loss=0.0067, train_err=0.1605
Eval: 128_h1=0.0087, 128_l2=0.0061
[136] time=130.36, avg_loss=0.0265, train_err=0.2122
Eval: 421_h1=0.0694, 421_l2=0.0117
[69] time=476.43, avg_loss=0.0118, train_err=0.2823
[137] time=130.36, avg_loss=0.0265, train_err=0.2121
Eval: 128_h1=0.0141, 128_l2=0.0087
Eval: 421_h1=0.0695, 421_l2=0.0119
[130] time=247.08, avg_loss=0.0064, train_err=0.1535
Eval: 128_h1=0.0086, 128_l2=0.0059
[138] time=130.37, avg_loss=0.0274, train_err=0.2192
Eval: 421_h1=0.0694, 421_l2=0.0116
[139] time=130.33, avg_loss=0.0266, train_err=0.2131
Eval: 421_h1=0.0696, 421_l2=0.0120
[131] time=245.51, avg_loss=0.0062, train_err=0.1486
Eval: 128_h1=0.0088, 128_l2=0.0060
[140] time=130.34, avg_loss=0.0263, train_err=0.2107
Eval: 421_h1=0.0696, 421_l2=0.0118
[70] time=453.24, avg_loss=0.0120, train_err=0.2870
Eval: 128_h1=0.0141, 128_l2=0.0085
[141] time=130.34, avg_loss=0.0259, train_err=0.2068
Eval: 421_h1=0.0693, 421_l2=0.0113
[132] time=250.97, avg_loss=0.0061, train_err=0.1464
Eval: 128_h1=0.0089, 128_l2=0.0061
[142] time=130.34, avg_loss=0.0256, train_err=0.2052
Eval: 421_h1=0.0697, 421_l2=0.0113
[143] time=130.33, avg_loss=0.0265, train_err=0.2120
Eval: 421_h1=0.0699, 421_l2=0.0123
[133] time=260.52, avg_loss=0.0060, train_err=0.1446
Eval: 128_h1=0.0089, 128_l2=0.0060
[71] time=481.69, avg_loss=0.0133, train_err=0.3185
Eval: 128_h1=0.0209, 128_l2=0.0140
[144] time=130.35, avg_loss=0.0264, train_err=0.2115
Eval: 421_h1=0.0695, 421_l2=0.0113
[134] time=245.43, avg_loss=0.0059, train_err=0.1425
[145] time=130.35, avg_loss=0.0257, train_err=0.2057
Eval: 128_h1=0.0089, 128_l2=0.0060
Eval: 421_h1=0.0695, 421_l2=0.0118
[146] time=130.35, avg_loss=0.0258, train_err=0.2060
Eval: 421_h1=0.0694, 421_l2=0.0116
[135] time=248.05, avg_loss=0.0060, train_err=0.1437
Eval: 128_h1=0.0081, 128_l2=0.0052
[147] time=130.37, avg_loss=0.0258, train_err=0.2063
Eval: 421_h1=0.0698, 421_l2=0.0124
[72] time=455.73, avg_loss=0.0148, train_err=0.3544
Eval: 128_h1=0.0175, 128_l2=0.0112
[148] time=130.34, avg_loss=0.0258, train_err=0.2065
Eval: 421_h1=0.0693, 421_l2=0.0115
[136] time=254.21, avg_loss=0.0062, train_err=0.1475
Eval: 128_h1=0.0080, 128_l2=0.0051
[149] time=130.33, avg_loss=0.0261, train_err=0.2086
Eval: 421_h1=0.0696, 421_l2=0.0118
[150] time=130.35, avg_loss=0.0258, train_err=0.2064
Eval: 421_h1=0.0707, 421_l2=0.0141
[137] time=255.43, avg_loss=0.0057, train_err=0.1372
Eval: 128_h1=0.0083, 128_l2=0.0054
[73] time=478.68, avg_loss=0.0138, train_err=0.3305
Eval: 128_h1=0.0166, 128_l2=0.0108
[151] time=130.36, avg_loss=0.0260, train_err=0.2077
Eval: 421_h1=0.0696, 421_l2=0.0115
[152] time=130.37, avg_loss=0.0254, train_err=0.2034
Eval: 421_h1=0.0698, 421_l2=0.0113
[138] time=244.92, avg_loss=0.0056, train_err=0.1340
Eval: 128_h1=0.0077, 128_l2=0.0048
[153] time=130.36, avg_loss=0.0252, train_err=0.2018
Eval: 421_h1=0.0696, 421_l2=0.0119
[74] time=454.33, avg_loss=0.0125, train_err=0.2987
[154] time=130.37, avg_loss=0.0260, train_err=0.2084
Eval: 421_h1=0.0699, 421_l2=0.0116
[139] time=247.16, avg_loss=0.0056, train_err=0.1339
Eval: 128_h1=0.0162, 128_l2=0.0107
Eval: 128_h1=0.0077, 128_l2=0.0049
[155] time=130.34, avg_loss=0.0253, train_err=0.2027
Eval: 421_h1=0.0697, 421_l2=0.0113
[156] time=130.45, avg_loss=0.0255, train_err=0.2043
Eval: 421_h1=0.0699, 421_l2=0.0113
[140] time=260.78, avg_loss=0.0056, train_err=0.1332
Eval: 128_h1=0.0077, 128_l2=0.0051
[157] time=130.40, avg_loss=0.0254, train_err=0.2030
Eval: 421_h1=0.0699, 421_l2=0.0114
[75] time=479.44, avg_loss=0.0120, train_err=0.2881
Eval: 128_h1=0.0162, 128_l2=0.0108
[141] time=246.96, avg_loss=0.0054, train_err=0.1307
[158] time=130.42, avg_loss=0.0251, train_err=0.2008
Eval: 128_h1=0.0082, 128_l2=0.0054
Eval: 421_h1=0.0701, 421_l2=0.0118
[159] time=130.40, avg_loss=0.0250, train_err=0.1997
Eval: 421_h1=0.0701, 421_l2=0.0114
[142] time=246.16, avg_loss=0.0054, train_err=0.1304
Eval: 128_h1=0.0081, 128_l2=0.0053
[160] time=130.43, avg_loss=0.0256, train_err=0.2051
Eval: 421_h1=0.0697, 421_l2=0.0115
[76] time=454.79, avg_loss=0.0115, train_err=0.2764
[161] time=130.40, avg_loss=0.0263, train_err=0.2107
Eval: 128_h1=0.0160, 128_l2=0.0106
Eval: 421_h1=0.0701, 421_l2=0.0121
[143] time=248.17, avg_loss=0.0056, train_err=0.1348
Eval: 128_h1=0.0078, 128_l2=0.0059
[162] time=130.42, avg_loss=0.0267, train_err=0.2136
Eval: 421_h1=0.0703, 421_l2=0.0114
[163] time=130.40, avg_loss=0.0252, train_err=0.2019
Eval: 421_h1=0.0700, 421_l2=0.0114
[144] time=253.34, avg_loss=0.0060, train_err=0.1451
Eval: 128_h1=0.0077, 128_l2=0.0058
[164] time=130.44, avg_loss=0.0249, train_err=0.1995
Eval: 421_h1=0.0699, 421_l2=0.0116
[77] time=465.57, avg_loss=0.0114, train_err=0.2734
Eval: 128_h1=0.0158, 128_l2=0.0101
[165] time=130.40, avg_loss=0.0249, train_err=0.1988
Eval: 421_h1=0.0701, 421_l2=0.0114
[145] time=245.57, avg_loss=0.0056, train_err=0.1334
Eval: 128_h1=0.0077, 128_l2=0.0059
[166] time=130.41, avg_loss=0.0247, train_err=0.1978
Eval: 421_h1=0.0700, 421_l2=0.0114
[167] time=130.40, avg_loss=0.0250, train_err=0.2002
Eval: 421_h1=0.0702, 421_l2=0.0112
[146] time=258.33, avg_loss=0.0055, train_err=0.1316
Eval: 128_h1=0.0075, 128_l2=0.0054
[78] time=477.67, avg_loss=0.0115, train_err=0.2756
[168] time=130.42, avg_loss=0.0250, train_err=0.2004
Eval: 128_h1=0.0140, 128_l2=0.0083
Eval: 421_h1=0.0700, 421_l2=0.0115
[169] time=130.40, avg_loss=0.0250, train_err=0.1996
[147] time=251.62, avg_loss=0.0057, train_err=0.1365
Eval: 421_h1=0.0703, 421_l2=0.0116
Eval: 128_h1=0.0075, 128_l2=0.0049
[170] time=130.43, avg_loss=0.0251, train_err=0.2011
Eval: 421_h1=0.0703, 421_l2=0.0114
[148] time=247.55, avg_loss=0.0057, train_err=0.1357
[171] time=130.41, avg_loss=0.0250, train_err=0.2002
Eval: 128_h1=0.0075, 128_l2=0.0047
Eval: 421_h1=0.0702, 421_l2=0.0116
[79] time=458.50, avg_loss=0.0127, train_err=0.3040
Eval: 128_h1=0.0148, 128_l2=0.0087
[172] time=130.43, avg_loss=0.0258, train_err=0.2061
Eval: 421_h1=0.0705, 421_l2=0.0118
[149] time=244.93, avg_loss=0.0056, train_err=0.1354
Eval: 128_h1=0.0086, 128_l2=0.0057
[173] time=130.42, avg_loss=0.0254, train_err=0.2032
Eval: 421_h1=0.0706, 421_l2=0.0121
[174] time=130.42, avg_loss=0.0253, train_err=0.2026
Eval: 421_h1=0.0701, 421_l2=0.0117
[150] time=259.69, avg_loss=0.0060, train_err=0.1440
[80] time=474.19, avg_loss=0.0133, train_err=0.3189
Eval: 128_h1=0.0081, 128_l2=0.0053
Eval: 128_h1=0.0160, 128_l2=0.0102
[175] time=130.41, avg_loss=0.0248, train_err=0.1985
Eval: 421_h1=0.0701, 421_l2=0.0115
[176] time=130.43, avg_loss=0.0244, train_err=0.1952
Eval: 421_h1=0.0704, 421_l2=0.0115
[151] time=250.58, avg_loss=0.0060, train_err=0.1439
Eval: 128_h1=0.0080, 128_l2=0.0053
[177] time=130.41, avg_loss=0.0247, train_err=0.1974
Eval: 421_h1=0.0706, 421_l2=0.0115
[178] time=130.44, avg_loss=0.0244, train_err=0.1955
Eval: 421_h1=0.0701, 421_l2=0.0118
[81] time=462.03, avg_loss=0.0127, train_err=0.3036
Eval: 128_h1=0.0167, 128_l2=0.0109
[152] time=248.75, avg_loss=0.0059, train_err=0.1421
Eval: 128_h1=0.0079, 128_l2=0.0050
[179] time=130.41, avg_loss=0.0252, train_err=0.2016
Eval: 421_h1=0.0708, 421_l2=0.0122
[180] time=130.43, avg_loss=0.0239, train_err=0.1910
Eval: 421_h1=0.0702, 421_l2=0.0112
[153] time=246.32, avg_loss=0.0057, train_err=0.1375
Eval: 128_h1=0.0077, 128_l2=0.0051
[181] time=130.41, avg_loss=0.0227, train_err=0.1815
Eval: 421_h1=0.0705, 421_l2=0.0112
[82] time=470.11, avg_loss=0.0129, train_err=0.3101
Eval: 128_h1=0.0147, 128_l2=0.0093
[182] time=130.42, avg_loss=0.0224, train_err=0.1794
Eval: 421_h1=0.0708, 421_l2=0.0111
[154] time=260.32, avg_loss=0.0056, train_err=0.1345
Eval: 128_h1=0.0077, 128_l2=0.0050
[183] time=130.39, avg_loss=0.0223, train_err=0.1785
Eval: 421_h1=0.0709, 421_l2=0.0113
[155] time=251.00, avg_loss=0.0056, train_err=0.1355
[184] time=130.43, avg_loss=0.0223, train_err=0.1780
Eval: 128_h1=0.0078, 128_l2=0.0052
Eval: 421_h1=0.0710, 421_l2=0.0112
[185] time=130.42, avg_loss=0.0222, train_err=0.1776
[83] time=467.14, avg_loss=0.0128, train_err=0.3069
Eval: 421_h1=0.0712, 421_l2=0.0110
Eval: 128_h1=0.0141, 128_l2=0.0090
[156] time=249.46, avg_loss=0.0053, train_err=0.1272
Eval: 128_h1=0.0075, 128_l2=0.0053
[186] time=130.43, avg_loss=0.0222, train_err=0.1776
Eval: 421_h1=0.0713, 421_l2=0.0111
[187] time=130.40, avg_loss=0.0222, train_err=0.1772
Eval: 421_h1=0.0713, 421_l2=0.0111
[157] time=247.62, avg_loss=0.0051, train_err=0.1233
Eval: 128_h1=0.0073, 128_l2=0.0051
[188] time=130.42, avg_loss=0.0221, train_err=0.1771
Eval: 421_h1=0.0714, 421_l2=0.0113
[84] time=468.01, avg_loss=0.0113, train_err=0.2710
Eval: 128_h1=0.0137, 128_l2=0.0092
[189] time=130.40, avg_loss=0.0222, train_err=0.1773
Eval: 421_h1=0.0715, 421_l2=0.0112
[158] time=260.68, avg_loss=0.0051, train_err=0.1219
Eval: 128_h1=0.0073, 128_l2=0.0054
[190] time=130.41, avg_loss=0.0222, train_err=0.1778
Eval: 421_h1=0.0713, 421_l2=0.0112
[191] time=130.42, avg_loss=0.0224, train_err=0.1790
Eval: 421_h1=0.0718, 421_l2=0.0111
[159] time=249.29, avg_loss=0.0051, train_err=0.1235
Eval: 128_h1=0.0074, 128_l2=0.0052
[192] time=130.45, avg_loss=0.0225, train_err=0.1802
[85] time=468.97, avg_loss=0.0107, train_err=0.2558
Eval: 421_h1=0.0717, 421_l2=0.0112
Eval: 128_h1=0.0144, 128_l2=0.0101
[193] time=130.41, avg_loss=0.0225, train_err=0.1801
Eval: 421_h1=0.0716, 421_l2=0.0113
[160] time=251.34, avg_loss=0.0053, train_err=0.1277
Eval: 128_h1=0.0075, 128_l2=0.0045
[194] time=130.48, avg_loss=0.0224, train_err=0.1793
Eval: 421_h1=0.0714, 421_l2=0.0113
[195] time=130.41, avg_loss=0.0226, train_err=0.1805
Eval: 421_h1=0.0716, 421_l2=0.0113
[161] time=251.12, avg_loss=0.0053, train_err=0.1264
Eval: 128_h1=0.0073, 128_l2=0.0048
[86] time=469.85, avg_loss=0.0105, train_err=0.2518
Eval: 128_h1=0.0143, 128_l2=0.0097
[196] time=130.42, avg_loss=0.0224, train_err=0.1794
Eval: 421_h1=0.0719, 421_l2=0.0114
[197] time=130.39, avg_loss=0.0224, train_err=0.1788
[162] time=258.03, avg_loss=0.0052, train_err=0.1244
Eval: 421_h1=0.0714, 421_l2=0.0111
Eval: 128_h1=0.0079, 128_l2=0.0049
[198] time=130.41, avg_loss=0.0223, train_err=0.1785
Eval: 421_h1=0.0714, 421_l2=0.0113
[163] time=248.93, avg_loss=0.0053, train_err=0.1276
[87] time=470.40, avg_loss=0.0104, train_err=0.2494
[199] time=130.42, avg_loss=0.0226, train_err=0.1805
Eval: 128_h1=0.0078, 128_l2=0.0053
Eval: 421_h1=0.0716, 421_l2=0.0112
Eval: 128_h1=0.0138, 128_l2=0.0091
[200] time=130.42, avg_loss=0.0223, train_err=0.1784
Eval: 421_h1=0.0717, 421_l2=0.0113
[164] time=249.90, avg_loss=0.0053, train_err=0.1273
Eval: 128_h1=0.0074, 128_l2=0.0050
[201] time=130.41, avg_loss=0.0224, train_err=0.1794
Eval: 421_h1=0.0719, 421_l2=0.0112
[202] time=130.42, avg_loss=0.0222, train_err=0.1778
Eval: 421_h1=0.0716, 421_l2=0.0112
[88] time=458.97, avg_loss=0.0112, train_err=0.2689
Eval: 128_h1=0.0138, 128_l2=0.0080
[165] time=247.68, avg_loss=0.0056, train_err=0.1338
Eval: 128_h1=0.0078, 128_l2=0.0049
[203] time=130.41, avg_loss=0.0223, train_err=0.1783
Eval: 421_h1=0.0720, 421_l2=0.0114
[204] time=130.41, avg_loss=0.0225, train_err=0.1797
Eval: 421_h1=0.0719, 421_l2=0.0110
[166] time=249.23, avg_loss=0.0061, train_err=0.1470
Eval: 128_h1=0.0077, 128_l2=0.0051
[205] time=130.42, avg_loss=0.0222, train_err=0.1773
Eval: 421_h1=0.0718, 421_l2=0.0112
[89] time=449.14, avg_loss=0.0113, train_err=0.2709
Eval: 128_h1=0.0134, 128_l2=0.0082
[206] time=130.42, avg_loss=0.0222, train_err=0.1773
Eval: 421_h1=0.0721, 421_l2=0.0112
[167] time=234.47, avg_loss=0.0060, train_err=0.1435
Eval: 128_h1=0.0080, 128_l2=0.0053
[207] time=130.42, avg_loss=0.0221, train_err=0.1765
Eval: 421_h1=0.0720, 421_l2=0.0111
[168] time=235.40, avg_loss=0.0059, train_err=0.1427
[208] time=130.45, avg_loss=0.0221, train_err=0.1771
Eval: 128_h1=0.0079, 128_l2=0.0055
Eval: 421_h1=0.0721, 421_l2=0.0112
[90] time=432.26, avg_loss=0.0106, train_err=0.2535
[209] time=130.42, avg_loss=0.0221, train_err=0.1766
Eval: 128_h1=0.0131, 128_l2=0.0083
Eval: 421_h1=0.0720, 421_l2=0.0114
[169] time=247.00, avg_loss=0.0058, train_err=0.1401
Eval: 128_h1=0.0079, 128_l2=0.0057
[210] time=130.41, avg_loss=0.0221, train_err=0.1765
Eval: 421_h1=0.0721, 421_l2=0.0114
[211] time=130.40, avg_loss=0.0220, train_err=0.1764
Eval: 421_h1=0.0721, 421_l2=0.0114
[170] time=248.48, avg_loss=0.0057, train_err=0.1378
Eval: 128_h1=0.0079, 128_l2=0.0057
[212] time=130.42, avg_loss=0.0220, train_err=0.1762
Eval: 421_h1=0.0722, 421_l2=0.0112
[91] time=457.08, avg_loss=0.0103, train_err=0.2462
Eval: 128_h1=0.0129, 128_l2=0.0082
[213] time=130.40, avg_loss=0.0218, train_err=0.1748
Eval: 421_h1=0.0723, 421_l2=0.0112
[171] time=233.79, avg_loss=0.0056, train_err=0.1346
Eval: 128_h1=0.0078, 128_l2=0.0060
[214] time=130.41, avg_loss=0.0219, train_err=0.1753
Eval: 421_h1=0.0725, 421_l2=0.0113
[215] time=130.42, avg_loss=0.0219, train_err=0.1754
[172] time=235.41, avg_loss=0.0055, train_err=0.1309
Eval: 421_h1=0.0725, 421_l2=0.0114
Eval: 128_h1=0.0077, 128_l2=0.0059
[92] time=429.90, avg_loss=0.0100, train_err=0.2393
Eval: 128_h1=0.0130, 128_l2=0.0081
[216] time=130.42, avg_loss=0.0220, train_err=0.1760
Eval: 421_h1=0.0724, 421_l2=0.0112
[173] time=248.68, avg_loss=0.0053, train_err=0.1280
Eval: 128_h1=0.0075, 128_l2=0.0058
[217] time=130.42, avg_loss=0.0219, train_err=0.1754
Eval: 421_h1=0.0725, 421_l2=0.0115
[218] time=130.43, avg_loss=0.0219, train_err=0.1750
Eval: 421_h1=0.0725, 421_l2=0.0112
[174] time=256.54, avg_loss=0.0052, train_err=0.1259
Eval: 128_h1=0.0075, 128_l2=0.0059
[93] time=479.58, avg_loss=0.0098, train_err=0.2361
[219] time=130.40, avg_loss=0.0218, train_err=0.1745
Eval: 421_h1=0.0723, 421_l2=0.0113
Eval: 128_h1=0.0136, 128_l2=0.0088
[220] time=130.40, avg_loss=0.0218, train_err=0.1743
Eval: 421_h1=0.0723, 421_l2=0.0112
[175] time=260.75, avg_loss=0.0052, train_err=0.1255
Eval: 128_h1=0.0075, 128_l2=0.0060
[221] time=130.38, avg_loss=0.0218, train_err=0.1743
Eval: 421_h1=0.0724, 421_l2=0.0112
[222] time=130.43, avg_loss=0.0218, train_err=0.1743
Eval: 421_h1=0.0727, 421_l2=0.0113
[94] time=491.51, avg_loss=0.0101, train_err=0.2413
[176] time=259.94, avg_loss=0.0052, train_err=0.1240
Eval: 128_h1=0.0075, 128_l2=0.0059
Eval: 128_h1=0.0131, 128_l2=0.0080
[223] time=130.41, avg_loss=0.0218, train_err=0.1740
Eval: 421_h1=0.0723, 421_l2=0.0115
[224] time=130.42, avg_loss=0.0218, train_err=0.1746
Eval: 421_h1=0.0728, 421_l2=0.0114
[177] time=261.74, avg_loss=0.0051, train_err=0.1225
Eval: 128_h1=0.0075, 128_l2=0.0063
[225] time=130.40, avg_loss=0.0217, train_err=0.1738
Eval: 421_h1=0.0729, 421_l2=0.0113
[226] time=130.42, avg_loss=0.0219, train_err=0.1755
Eval: 421_h1=0.0728, 421_l2=0.0113
[95] time=515.72, avg_loss=0.0102, train_err=0.2452
Eval: 128_h1=0.0140, 128_l2=0.0084
[178] time=273.21, avg_loss=0.0054, train_err=0.1296
Eval: 128_h1=0.0081, 128_l2=0.0062
[227] time=130.38, avg_loss=0.0216, train_err=0.1731
Eval: 421_h1=0.0727, 421_l2=0.0114
[228] time=130.42, avg_loss=0.0216, train_err=0.1731
Eval: 421_h1=0.0726, 421_l2=0.0112
[179] time=250.71, avg_loss=0.0058, train_err=0.1387
Eval: 128_h1=0.0077, 128_l2=0.0052
[229] time=130.39, avg_loss=0.0215, train_err=0.1719
Eval: 421_h1=0.0728, 421_l2=0.0113
[96] time=468.46, avg_loss=0.0116, train_err=0.2778
[230] time=130.41, avg_loss=0.0215, train_err=0.1720
Eval: 128_h1=0.0175, 128_l2=0.0108
Eval: 421_h1=0.0730, 421_l2=0.0112
[180] time=251.85, avg_loss=0.0059, train_err=0.1406
Eval: 128_h1=0.0075, 128_l2=0.0048
[231] time=130.38, avg_loss=0.0216, train_err=0.1730
Eval: 421_h1=0.0727, 421_l2=0.0111
[232] time=130.39, avg_loss=0.0215, train_err=0.1719
Eval: 421_h1=0.0728, 421_l2=0.0111
[181] time=258.97, avg_loss=0.0058, train_err=0.1386
Eval: 128_h1=0.0075, 128_l2=0.0046
[233] time=130.37, avg_loss=0.0214, train_err=0.1710
Eval: 421_h1=0.0728, 421_l2=0.0114
[97] time=498.60, avg_loss=0.0119, train_err=0.2865
Eval: 128_h1=0.0167, 128_l2=0.0107
[234] time=130.39, avg_loss=0.0214, train_err=0.1715
Eval: 421_h1=0.0727, 421_l2=0.0115
[182] time=266.57, avg_loss=0.0059, train_err=0.1425
Eval: 128_h1=0.0077, 128_l2=0.0045
[235] time=130.38, avg_loss=0.0215, train_err=0.1718
Eval: 421_h1=0.0729, 421_l2=0.0111
[236] time=130.39, avg_loss=0.0213, train_err=0.1706
Eval: 421_h1=0.0725, 421_l2=0.0112
[183] time=255.74, avg_loss=0.0059, train_err=0.1416
Eval: 128_h1=0.0076, 128_l2=0.0046
[237] time=130.38, avg_loss=0.0213, train_err=0.1704
[98] time=480.82, avg_loss=0.0124, train_err=0.2964
Eval: 421_h1=0.0731, 421_l2=0.0113
Eval: 128_h1=0.0141, 128_l2=0.0090
[238] time=130.40, avg_loss=0.0213, train_err=0.1708
[184] time=256.72, avg_loss=0.0056, train_err=0.1346
Eval: 421_h1=0.0728, 421_l2=0.0114
Eval: 128_h1=0.0075, 128_l2=0.0045
[239] time=130.39, avg_loss=0.0215, train_err=0.1718
Eval: 421_h1=0.0728, 421_l2=0.0113
[240] time=130.40, avg_loss=0.0211, train_err=0.1685
[185] time=272.41, avg_loss=0.0053, train_err=0.1281
Eval: 421_h1=0.0728, 421_l2=0.0112
Eval: 128_h1=0.0073, 128_l2=0.0045
[99] time=518.65, avg_loss=0.0118, train_err=0.2831
[241] time=130.41, avg_loss=0.0205, train_err=0.1640
Eval: 421_h1=0.0733, 421_l2=0.0112
Eval: 128_h1=0.0129, 128_l2=0.0076
[186] time=261.74, avg_loss=0.0051, train_err=0.1227
[242] time=130.40, avg_loss=0.0204, train_err=0.1631
Eval: 128_h1=0.0077, 128_l2=0.0054
Eval: 421_h1=0.0734, 421_l2=0.0112
[243] time=130.42, avg_loss=0.0203, train_err=0.1628
Eval: 421_h1=0.0734, 421_l2=0.0112
[187] time=257.63, avg_loss=0.0050, train_err=0.1201
Eval: 128_h1=0.0071, 128_l2=0.0052
[244] time=130.40, avg_loss=0.0203, train_err=0.1625
Eval: 421_h1=0.0735, 421_l2=0.0112
[100] time=481.45, avg_loss=0.0100, train_err=0.2407
Eval: 128_h1=0.0120, 128_l2=0.0068
[245] time=130.37, avg_loss=0.0203, train_err=0.1624
Eval: 421_h1=0.0736, 421_l2=0.0112
[188] time=258.88, avg_loss=0.0049, train_err=0.1181
Eval: 128_h1=0.0071, 128_l2=0.0053
[246] time=130.39, avg_loss=0.0203, train_err=0.1622
Eval: 421_h1=0.0737, 421_l2=0.0112
[247] time=130.39, avg_loss=0.0203, train_err=0.1620
Eval: 421_h1=0.0737, 421_l2=0.0111
[189] time=269.48, avg_loss=0.0049, train_err=0.1171
Eval: 128_h1=0.0071, 128_l2=0.0053
[248] time=130.38, avg_loss=0.0202, train_err=0.1619
Eval: 421_h1=0.0738, 421_l2=0.0112
[101] time=507.50, avg_loss=0.0095, train_err=0.2290
Eval: 128_h1=0.0116, 128_l2=0.0065
[249] time=130.38, avg_loss=0.0202, train_err=0.1618
Eval: 421_h1=0.0738, 421_l2=0.0111
[190] time=249.28, avg_loss=0.0049, train_err=0.1163
Eval: 128_h1=0.0071, 128_l2=0.0056
[250] time=130.41, avg_loss=0.0202, train_err=0.1617
Eval: 421_h1=0.0739, 421_l2=0.0112
[251] time=130.41, avg_loss=0.0202, train_err=0.1616
Eval: 421_h1=0.0739, 421_l2=0.0112
[191] time=254.46, avg_loss=0.0050, train_err=0.1194
Eval: 128_h1=0.0073, 128_l2=0.0048
[102] time=468.76, avg_loss=0.0093, train_err=0.2228
Eval: 128_h1=0.0114, 128_l2=0.0063
[252] time=130.41, avg_loss=0.0202, train_err=0.1616
Eval: 421_h1=0.0739, 421_l2=0.0111
[253] time=130.43, avg_loss=0.0202, train_err=0.1615
Eval: 421_h1=0.0741, 421_l2=0.0113
[192] time=265.55, avg_loss=0.0050, train_err=0.1193
Eval: 128_h1=0.0072, 128_l2=0.0045
[254] time=130.40, avg_loss=0.0202, train_err=0.1617
Eval: 421_h1=0.0741, 421_l2=0.0112
[255] time=130.40, avg_loss=0.0202, train_err=0.1619
Eval: 421_h1=0.0742, 421_l2=0.0112
[103] time=518.00, avg_loss=0.0091, train_err=0.2171
[193] time=269.56, avg_loss=0.0051, train_err=0.1228
Eval: 128_h1=0.0077, 128_l2=0.0050
Eval: 128_h1=0.0112, 128_l2=0.0062
[256] time=130.45, avg_loss=0.0203, train_err=0.1627
Eval: 421_h1=0.0744, 421_l2=0.0113
[257] time=130.39, avg_loss=0.0203, train_err=0.1628
Eval: 421_h1=0.0743, 421_l2=0.0111
[194] time=250.31, avg_loss=0.0055, train_err=0.1308
Eval: 128_h1=0.0078, 128_l2=0.0050
[258] time=130.42, avg_loss=0.0203, train_err=0.1627
Eval: 421_h1=0.0740, 421_l2=0.0112
[104] time=470.28, avg_loss=0.0089, train_err=0.2126
[259] time=130.40, avg_loss=0.0204, train_err=0.1629
Eval: 421_h1=0.0744, 421_l2=0.0112
Eval: 128_h1=0.0111, 128_l2=0.0061
[195] time=255.59, avg_loss=0.0053, train_err=0.1262
Eval: 128_h1=0.0073, 128_l2=0.0045
[260] time=130.41, avg_loss=0.0203, train_err=0.1625
Eval: 421_h1=0.0741, 421_l2=0.0112
[261] time=130.39, avg_loss=0.0203, train_err=0.1623
Eval: 421_h1=0.0744, 421_l2=0.0112
[196] time=259.25, avg_loss=0.0051, train_err=0.1234
Eval: 128_h1=0.0077, 128_l2=0.0050
[262] time=130.39, avg_loss=0.0203, train_err=0.1620
Eval: 421_h1=0.0743, 421_l2=0.0113
[105] time=483.18, avg_loss=0.0087, train_err=0.2093
Eval: 128_h1=0.0110, 128_l2=0.0059
[263] time=130.38, avg_loss=0.0202, train_err=0.1619
[197] time=253.88, avg_loss=0.0053, train_err=0.1275
Eval: 421_h1=0.0744, 421_l2=0.0112
Eval: 128_h1=0.0080, 128_l2=0.0052
[264] time=130.39, avg_loss=0.0202, train_err=0.1617
Eval: 421_h1=0.0743, 421_l2=0.0112
[198] time=254.23, avg_loss=0.0050, train_err=0.1210
[265] time=130.38, avg_loss=0.0201, train_err=0.1609
Eval: 128_h1=0.0076, 128_l2=0.0050
Eval: 421_h1=0.0745, 421_l2=0.0112
[266] time=130.39, avg_loss=0.0201, train_err=0.1612
[106] time=473.67, avg_loss=0.0086, train_err=0.2060
Eval: 421_h1=0.0747, 421_l2=0.0113
Eval: 128_h1=0.0109, 128_l2=0.0058
[199] time=253.84, avg_loss=0.0051, train_err=0.1225
Eval: 128_h1=0.0076, 128_l2=0.0048
[267] time=130.38, avg_loss=0.0202, train_err=0.1616
Eval: 421_h1=0.0747, 421_l2=0.0112
[268] time=130.42, avg_loss=0.0201, train_err=0.1605
Eval: 421_h1=0.0745, 421_l2=0.0112
[200] time=257.70, avg_loss=0.0055, train_err=0.1330
Eval: 128_h1=0.0072, 128_l2=0.0047
[269] time=130.39, avg_loss=0.0200, train_err=0.1603
Eval: 421_h1=0.0745, 421_l2=0.0112
[107] time=481.51, avg_loss=0.0085, train_err=0.2029
Eval: 128_h1=0.0107, 128_l2=0.0057
[270] time=130.39, avg_loss=0.0200, train_err=0.1601
Eval: 421_h1=0.0747, 421_l2=0.0112
[201] time=256.62, avg_loss=0.0053, train_err=0.1261
Eval: 128_h1=0.0070, 128_l2=0.0047
[271] time=130.38, avg_loss=0.0201, train_err=0.1605
Eval: 421_h1=0.0747, 421_l2=0.0111
[272] time=130.39, avg_loss=0.0200, train_err=0.1602
Eval: 421_h1=0.0748, 421_l2=0.0111
[202] time=268.27, avg_loss=0.0051, train_err=0.1225
Eval: 128_h1=0.0069, 128_l2=0.0047
[273] time=130.39, avg_loss=0.0200, train_err=0.1601
Eval: 421_h1=0.0749, 421_l2=0.0112
[108] time=501.24, avg_loss=0.0083, train_err=0.2000
Eval: 128_h1=0.0106, 128_l2=0.0056
[274] time=130.46, avg_loss=0.0200, train_err=0.1598
Eval: 421_h1=0.0747, 421_l2=0.0112
[203] time=249.76, avg_loss=0.0050, train_err=0.1200
Eval: 128_h1=0.0069, 128_l2=0.0048
[275] time=130.42, avg_loss=0.0200, train_err=0.1596
Eval: 421_h1=0.0747, 421_l2=0.0112
[276] time=130.42, avg_loss=0.0199, train_err=0.1595
Eval: 421_h1=0.0748, 421_l2=0.0112
[204] time=257.76, avg_loss=0.0049, train_err=0.1180
Eval: 128_h1=0.0068, 128_l2=0.0048
[109] time=476.68, avg_loss=0.0082, train_err=0.1976
Eval: 128_h1=0.0105, 128_l2=0.0056
[277] time=130.40, avg_loss=0.0199, train_err=0.1595
Eval: 421_h1=0.0748, 421_l2=0.0112
[278] time=130.43, avg_loss=0.0200, train_err=0.1597
Eval: 421_h1=0.0749, 421_l2=0.0113
[205] time=264.35, avg_loss=0.0049, train_err=0.1164
Eval: 128_h1=0.0068, 128_l2=0.0049
[279] time=130.42, avg_loss=0.0199, train_err=0.1593
Eval: 421_h1=0.0748, 421_l2=0.0112
[280] time=130.38, avg_loss=0.0199, train_err=0.1594
Eval: 421_h1=0.0751, 421_l2=0.0112
[206] time=260.30, avg_loss=0.0048, train_err=0.1149
Eval: 128_h1=0.0068, 128_l2=0.0049
[110] time=496.76, avg_loss=0.0083, train_err=0.1988
Eval: 128_h1=0.0104, 128_l2=0.0055
[281] time=130.39, avg_loss=0.0199, train_err=0.1593
Eval: 421_h1=0.0750, 421_l2=0.0112
[282] time=130.44, avg_loss=0.0198, train_err=0.1588
Eval: 421_h1=0.0750, 421_l2=0.0112
[207] time=257.86, avg_loss=0.0047, train_err=0.1137
Eval: 128_h1=0.0068, 128_l2=0.0050
[283] time=130.39, avg_loss=0.0198, train_err=0.1586
Eval: 421_h1=0.0750, 421_l2=0.0112
[284] time=130.39, avg_loss=0.0198, train_err=0.1585
[111] time=486.27, avg_loss=0.0083, train_err=0.1981
[208] time=258.75, avg_loss=0.0047, train_err=0.1126
Eval: 421_h1=0.0751, 421_l2=0.0112
Eval: 128_h1=0.0068, 128_l2=0.0050
Eval: 128_h1=0.0104, 128_l2=0.0056
[285] time=130.39, avg_loss=0.0198, train_err=0.1584
Eval: 421_h1=0.0752, 421_l2=0.0112
[286] time=130.38, avg_loss=0.0198, train_err=0.1584
[209] time=264.39, avg_loss=0.0047, train_err=0.1115
Eval: 421_h1=0.0750, 421_l2=0.0112
Eval: 128_h1=0.0068, 128_l2=0.0051
[287] time=130.38, avg_loss=0.0198, train_err=0.1584
Eval: 421_h1=0.0755, 421_l2=0.0112
[112] time=506.34, avg_loss=0.0083, train_err=0.1982
Eval: 128_h1=0.0102, 128_l2=0.0055
[210] time=264.78, avg_loss=0.0046, train_err=0.1106
[288] time=130.38, avg_loss=0.0198, train_err=0.1583
Eval: 421_h1=0.0755, 421_l2=0.0112
Eval: 128_h1=0.0068, 128_l2=0.0051
[289] time=130.37, avg_loss=0.0197, train_err=0.1579
Eval: 421_h1=0.0750, 421_l2=0.0113
[211] time=264.77, avg_loss=0.0046, train_err=0.1097
[290] time=130.39, avg_loss=0.0197, train_err=0.1579
Eval: 128_h1=0.0069, 128_l2=0.0052
Eval: 421_h1=0.0753, 421_l2=0.0113
[291] time=130.38, avg_loss=0.0198, train_err=0.1580
Eval: 421_h1=0.0752, 421_l2=0.0112
[113] time=502.70, avg_loss=0.0079, train_err=0.1890
Eval: 128_h1=0.0101, 128_l2=0.0054
[212] time=263.43, avg_loss=0.0045, train_err=0.1089
[292] time=130.41, avg_loss=0.0197, train_err=0.1576
Eval: 128_h1=0.0069, 128_l2=0.0053
Eval: 421_h1=0.0755, 421_l2=0.0113
[293] time=130.38, avg_loss=0.0197, train_err=0.1574
Eval: 421_h1=0.0753, 421_l2=0.0112
[213] time=267.86, avg_loss=0.0045, train_err=0.1083
[294] time=130.42, avg_loss=0.0197, train_err=0.1574
Eval: 128_h1=0.0069, 128_l2=0.0053
Eval: 421_h1=0.0754, 421_l2=0.0112
[295] time=130.41, avg_loss=0.0197, train_err=0.1579
Eval: 421_h1=0.0757, 421_l2=0.0112
[114] time=506.85, avg_loss=0.0074, train_err=0.1781
Eval: 128_h1=0.0101, 128_l2=0.0055
[214] time=260.37, avg_loss=0.0045, train_err=0.1077
Eval: 128_h1=0.0070, 128_l2=0.0054
[296] time=130.41, avg_loss=0.0197, train_err=0.1573
Eval: 421_h1=0.0753, 421_l2=0.0114
[297] time=130.38, avg_loss=0.0196, train_err=0.1568
Eval: 421_h1=0.0756, 421_l2=0.0112
[215] time=257.90, avg_loss=0.0045, train_err=0.1072
Eval: 128_h1=0.0069, 128_l2=0.0054
[298] time=130.40, avg_loss=0.0196, train_err=0.1565
Eval: 421_h1=0.0754, 421_l2=0.0112
[115] time=476.93, avg_loss=0.0073, train_err=0.1761
Eval: 128_h1=0.0100, 128_l2=0.0055
[299] time=130.41, avg_loss=0.0196, train_err=0.1565
Eval: 421_h1=0.0758, 421_l2=0.0112
[216] time=249.89, avg_loss=0.0044, train_err=0.1066
Eval: 128_h1=0.0069, 128_l2=0.0053
[217] time=302.84, avg_loss=0.0044, train_err=0.1062
Eval: 128_h1=0.0069, 128_l2=0.0053
[116] time=555.92, avg_loss=0.0072, train_err=0.1732
Eval: 128_h1=0.0099, 128_l2=0.0052
[218] time=284.00, avg_loss=0.0044, train_err=0.1059
Eval: 128_h1=0.0068, 128_l2=0.0052
[219] time=281.72, avg_loss=0.0044, train_err=0.1063
Eval: 128_h1=0.0067, 128_l2=0.0051
[117] time=546.84, avg_loss=0.0071, train_err=0.1702
logfile: /home/leeshu/wmm/neuraloperator-main/logs/darcy/20260129_060014__L8HC2_421.log
Eval: 128_h1=0.0100, 128_l2=0.0053
logfile: /home/leeshu/wmm/neuraloperator-main/logs/darcy/20260129_060051__L8HC2_sta421.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'arch': 'fno', 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_small2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [32, 32], 'hidden_channels': 32, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 2, 'n_layers': 8, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 2, 'hc_dynamic': False}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.005, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 60, 'gamma': 0.5, 'n_epochs': 300}, 'data': {'_config_name': 'darcydatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data/Darcy_pt', 'batch_size': 8, 'n_train': 1000, 'train_resolution': 421, 'n_tests': [224], 'test_resolutions': [421], 'test_batch_sizes': [8], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'darcy32_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 421 with 224 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-7): 8 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([32, 32, 32, 17]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-7): 8 x Flattened1dConv(
        (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-7): 8 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))
          (1): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-7): 8 x SoftGating()
    )
  )
  (hc_layers): ModuleList(
    (0-7): 8 x HyperConnection(
      (layer_norm): GroupNorm(1, 32, eps=1e-05, affine=True)
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
      (1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
      (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.005
    lr: 0.005
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7f2ba14bb4c0>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7f2ba14bba60>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7f2ba14bba60>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7f2ba14bba90>}

### Beginning Training...


n_params: 8935281
Training on 800 samples
Testing on [224] samples         on resolutions [421].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([8, 1, 421, 421])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
logfile: /home/leeshu/wmm/neuraloperator-main/logs/navier_stokes/20260129_060151__L8HC2staNS.log
logfile: /home/leeshu/wmm/neuraloperator-main/logs/navier_stokes/20260129_060203__L8HC2staNS.log
##### CONFIG #####

{'_config_name': 'default', 'n_params_baseline': None, 'verbose': True, 'distributed': {'use_distributed': False, 'model_parallel_size': 1, 'seed': None}, 'model': {'_config_name': 'fno_medium2d', 'arch': None, 'data_channels': 1, 'out_channels': 1, 'model_arch': 'fno', 'n_modes': [64, 64], 'hidden_channels': 64, 'lifting_channel_ratio': 2, 'projection_channel_ratio': 4, 'n_layers': 8, 'domain_padding': 0.0, 'norm': None, 'fno_skip': 'linear', 'implementation': 'factorized', 'use_channel_mlp': True, 'channel_mlp_expansion': 0.5, 'channel_mlp_dropout': 0, 'separable': False, 'factorization': None, 'rank': 1.0, 'fixed_rank_modes': False, 'stabilizer': None, 'hc_rate': 2, 'hc_dynamic': False}, 'opt': {'training_loss': 'h1', 'testing_loss': 'l2', 'learning_rate': 0.0003, 'weight_decay': 0.0001, 'eval_interval': 1, 'mixed_precision': False, 'scheduler': 'StepLR', 'scheduler_T_max': 500, 'scheduler_patience': 50, 'step_size': 100, 'gamma': 0.5, 'n_epochs': 600}, 'data': {'_config_name': 'navierstokesdatasetconfig', 'folder': '/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/data/V1e-3_N5000_T50', 'batch_size': 24, 'n_train': 10000, 'train_resolution': 128, 'n_tests': [2000], 'test_resolutions': [128], 'test_batch_sizes': [24], 'encode_input': True, 'encode_output': True, 'download': False}, 'patching': {'levels': 0, 'padding': 0, 'stitching': False}, 'wandb': {'_config_name': 'wandbconfig', 'log': False, 'entity': None, 'project': 'your_project', 'name': 'ns128_hc_run', 'group': None, 'sweep': False, 'log_output': True}}
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(
Loading test db for resolution 128 with 2000 samples 
/home/leeshu/wmm/neuraloperator-main/neuralop/data/datasets/pt_dataset.py:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())

### MODEL ###
 FNO(
  (positional_embedding): GridEmbeddingND()
  (fno_blocks): FNOBlocks(
    (convs): ModuleList(
      (0-7): 8 x SpectralConv(
        (weight): DenseTensor(shape=torch.Size([64, 64, 64, 33]), rank=None)
      )
    )
    (fno_skips): ModuleList(
      (0-7): 8 x Flattened1dConv(
        (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
      )
    )
    (channel_mlp): ModuleList(
      (0-7): 8 x ChannelMLP(
        (fcs): ModuleList(
          (0): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
          (1): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (channel_mlp_skips): ModuleList(
      (0-7): 8 x SoftGating()
    )
  )
  (hc_layers): ModuleList(
    (0-7): 8 x HyperConnection(
      (layer_norm): GroupNorm(1, 64, eps=1e-05, affine=True)
    )
  )
  (lifting): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
      (1): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
    )
  )
  (projection): ChannelMLP(
    (fcs): ModuleList(
      (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
      (1): Conv1d(256, 1, kernel_size=(1,), stride=(1,))
    )
  )
)

### OPTIMIZER ###
 AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    initial_lr: 0.0003
    lr: 0.0003
    weight_decay: 0.0001
)

### SCHEDULER ###
 <torch.optim.lr_scheduler.StepLR object at 0x7fd1325de4f0>

### LOSSES ###

 * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fd1325de6a0>

 * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fd1325de6a0>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fd1325de6d0>}

### Beginning Training...


n_params: 138506129
Training on 10000 samples
Testing on [2000] samples         on resolutions [128].
/home/leeshu/anaconda3/envs/FNO1/lib/python3.9/site-packages/torch/nn/modules/module.py:1747: UserWarning: FNO.forward() received unexpected keyword arguments: ['y']. These arguments will be ignored.
  return forward_call(*args, **kwargs)
Raw outputs of shape torch.Size([24, 1, 128, 128])
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:536: UserWarning: H1Loss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  loss += training_loss(out, **sample)
[0] time=120.02, avg_loss=0.3519, train_err=2.8151
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
[220] time=295.26, avg_loss=0.0045, train_err=0.1074
Eval: 421_h1=0.1894, 421_l2=0.1026
Eval: 128_h1=0.0066, 128_l2=0.0050
[0] time=147.82, avg_loss=0.1155, train_err=2.7689
/home/leeshu/wmm/neuraloperator-main/neuralop/training/trainer.py:581: UserWarning: LpLoss.__call__() received unexpected keyword arguments: ['x']. These arguments will be ignored.
  val_loss = loss(out, **sample)
Eval: 128_h1=0.0776, 128_l2=0.0542
[1] time=117.20, avg_loss=0.1641, train_err=1.3128
Eval: 421_h1=0.1422, 421_l2=0.0831
[1] time=146.42, avg_loss=0.0681, train_err=1.6337
[2] time=117.20, avg_loss=0.1381, train_err=1.1047
Eval: 128_h1=0.0625, 128_l2=0.0530
[221] time=258.46, avg_loss=0.0045, train_err=0.1069
Eval: 421_h1=0.1283, 421_l2=0.0761
Eval: 128_h1=0.0066, 128_l2=0.0049
[118] time=509.80, avg_loss=0.0070, train_err=0.1677
Eval: 128_h1=0.0099, 128_l2=0.0054
[3] time=117.22, avg_loss=0.1228, train_err=0.9826
Eval: 421_h1=0.1436, 421_l2=0.0839
[2] time=146.42, avg_loss=0.0566, train_err=1.3566
Eval: 128_h1=0.0545, 128_l2=0.0527
[4] time=117.20, avg_loss=0.1121, train_err=0.8969
[222] time=251.69, avg_loss=0.0045, train_err=0.1070
Eval: 421_h1=0.1103, 421_l2=0.0706
Eval: 128_h1=0.0066, 128_l2=0.0048
[3] time=146.46, avg_loss=0.0502, train_err=1.2034
Eval: 128_h1=0.0498, 128_l2=0.0514
[5] time=117.21, avg_loss=0.1017, train_err=0.8140
Eval: 421_h1=0.1026, 421_l2=0.0631
[4] time=146.43, avg_loss=0.0457, train_err=1.0950
Eval: 128_h1=0.0465, 128_l2=0.0498
[6] time=117.19, avg_loss=0.0987, train_err=0.7900
[223] time=247.32, avg_loss=0.0045, train_err=0.1082
Eval: 421_h1=0.0950, 421_l2=0.0535
Eval: 128_h1=0.0068, 128_l2=0.0049
[119] time=460.63, avg_loss=0.0070, train_err=0.1672
Eval: 128_h1=0.0099, 128_l2=0.0054
[5] time=146.45, avg_loss=0.0422, train_err=1.0124
Eval: 128_h1=0.0438, 128_l2=0.0479
[7] time=117.19, avg_loss=0.0913, train_err=0.7303
Eval: 421_h1=0.1002, 421_l2=0.0572
[6] time=146.43, avg_loss=0.0395, train_err=0.9466
Eval: 128_h1=0.0416, 128_l2=0.0460
[8] time=117.19, avg_loss=0.0844, train_err=0.6754
Eval: 421_h1=0.0886, 421_l2=0.0494
[224] time=259.00, avg_loss=0.0045, train_err=0.1069
Eval: 128_h1=0.0067, 128_l2=0.0047
[9] time=117.19, avg_loss=0.0820, train_err=0.6557
[7] time=146.44, avg_loss=0.0372, train_err=0.8918
Eval: 128_h1=0.0397, 128_l2=0.0440
Eval: 421_h1=0.0916, 421_l2=0.0466
